[
  {
    "objectID": "About/cv.html",
    "href": "About/cv.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "quarto-website-template-main/404.html",
    "href": "quarto-website-template-main/404.html",
    "title": "404",
    "section": "",
    "text": "Page not found, sorry! Try the search or navigate back to the base website."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Github\n  \n  \n    \n     Email\n  \n  \n    \n     \n  \n\n  \n  \n\nto your Remote Sensing Data Analysis online course, or RESEDA for short!\nThis course helps you improve your analysis of remote sensing image data for the next data science project or thesis. We will have a look at basic and advanced concepts needed for a complete project implementation using remote sensing imagery – with a special focus on automation of individual operations and big data processing. This course will provide a great deal of knowledge and valuable expertise for all related fields of environmental earth sciences!\nFor that purpose we will mainly use the statistical programming language R in a Linux environment – but please don’t panic, it’s not as bad as it sounds: You will be led through a complete analysis process, from data acquisition, to import, exploration and finally the export of your results, guided by a lot of reproduceable examples, exercises and pretty pictures!\nKeep in mind that this course content complements and extends the material covered in the classes “Fernerkundung und Digitale Bildverarbeitung” and “Geographische Informationssysteme”, both being taught at the Freie Universität Berlin.\nPlease feel free to contact us if you have any questions or would like to discuss potential projects.\n\n\n\n\n\nBest regards, your FU Berlin Remote Sensing and Geoinformatics staff"
  },
  {
    "objectID": "R-Crash-Course/contents/RExerciseI.html",
    "href": "R-Crash-Course/contents/RExerciseI.html",
    "title": "R – Exercise I",
    "section": "",
    "text": "Welcome to your first training session!\nNo need to be nervous: this page contains not only tasks, but also their solutions as folded code elements. You can unfold these code blocks by simply clicking on them. Give it a try:\n\n\nClick here to see more\n\n  # Well done!\n\n  # Spoiler! -  you will find your answer here\n\n\nCreate a variable called a and assign the number 2017 to it!\n\n\n\nClick here to see the answer\n\n  # use the \"&lt;-\" operator for variable assignments:\n  a &lt;- 2017\n\n\nCalculate the square root of 1089 and save the result in variable b!\n\n\n\nClick here to see the answer\n\n  # use the built- in function \"sqrt()\" and the number 1089 as an argument:\n  b &lt;- sqrt(1089)\n\n\nCalculate the sum of a and b!\n\n\n\nClick here to see the answer\n\n  # done via standard operators:\n  a + b\n  ## [1] 2050\n\n\nOverwrite variable a by assigning the value 2018 to it!\n\n\n\nClick here to see the answer\n\n  # simply assign a new value to an existing variable in order to overwrite it\n  a &lt;- 2018\n\n\nMake a copy of variable b and name it c!\n\n\n\nClick here to see the answer\n\n  # variable assignment works from right (existing variable) to left (new variable):\n  c &lt;- b\n\n\nCreate your own function called my.fun(), which requires three variables as input. The function should generate the square root of the product of all three variables and return one numeric value!\n\n\n\nClick here to see the answer\n\n  my.fun &lt;- function( var1, var2, var3 ) {\n    result &lt;- sqrt( var1 * var2 * var3 )\n    return(result)\n  }\n\n\nUse a, b and c (from the previous tasks) as input into my.fun() and save the output to variable d! Check the resulting value!\n\n\n\nClick here to see the answer\n\n  d &lt;- my.fun(a, b, c)\n  d\n  ## [1] 1482.431",
    "crumbs": [
      "R Crash Course",
      "Part I",
      "Excersice I"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/PartI.html",
    "href": "R-Crash-Course/contents/PartI.html",
    "title": "Part I",
    "section": "",
    "text": "Part I\n\n\nPackage Management\nPackages are collections of functions and compiled code written by the R community or the R Development Core Team. R comes with a standard set of packages, e.g., base, stats, and graphics. However, R is enormously expandable in its (geo-)statistical functionality via the official network CRAN (The Comprehensive R Archive Network). There are over 20,000 different extensions, or packages, available free of charge. The directory, where installed packages are stored, is called library.\nYour can install new packages to your library by clicking on the Packages-tab and then Install in the Files Pane of R-Studio. Anyway, download and installation of packages can also be done script-based via the function install.packages() (illustrated using the ggplot2 package, which is useful for creating elegant data visualizations):\ninstall.packages(\"ggplot2\")\n\nlibrary()\nThe function library() in line 3 lists all currently installed packages on your system. This list is also visible in the Files Pane of R-Studio (Packages tab). Keep in mind that packages must be installed only once and remain permanently installed, even after a restart of R-Studio.\nOnce installed, you have to load the package into your current R-session before you can use its functionalities by using library() together with the package name:\nlibrary(ggplot2)    \nYou can list all packages, which are loaded in your current R session by using search(). Activated packages also have a tick symbol next to their names in the Files pane (Packages tab) of R-Studio.\n\n\nCalculate With R\nOf course, R can be used as a simple calculator. Required operators can be entered directly into the Console pane of R-Studio or as a whole script in the Source Pane, from which you can send the commands to the Console pane with Ctrl+Enter. Results are then immediately printed to the console. In this online course, corresponding outputs are also shown with two hash tags at the beginning of the line for better transparency and readability:\n# hash tags allow you to make valuable notes and reminders \n\n19 + 23      \n## [1] 42\n\n34 - 22     \n## [1] 12\n\n27 / 9       \n## [1] 3\n\n6 * 8        \n## [1] 48\n\n(2 + 3) * 4  \n## [1] 20\nYou will see a number in square brackets [1] at the beginning of your output prompts. This number refers to the length of your output, i.e., the number of elements, which is 1 for all examples above. More on that in chapter Vectors. In addition to these standard operators, there were plenty of other operators commonly encountered in R:\n\n\n\n\n\n\n\n  ?  \nhelp function\n\n\n  +   –   /   *   ^  \naddition, subtraction, division, multiplication, potentiation\n\n\n  !  \nnegation sign\n\n\n  &lt;   &gt;   &lt;=   &gt;=   ==   !=  \nlesser, greater, lesser or equal, greater or equal, equal, not equal\n\n\n  &   |  \nboolean AND, boolean OR\n\n\n  &lt;-  \nvariable assignment\n\n\n  ~  \nseparate left- and right-hand sides in a model formula\n\n\n  :  \ngenerate regular sequences\n\n\n  %%  \nmodulo\n\n\n[  [[]]  $  @\nindexing in vectors, matrices, and data frames\n\n\n\nThe help operator in R provides access to the documentation pages for R functions, data sets, and other objects, both for packages in the standard R distribution and for contributed packages. In order to access documentation for the sequence operator, for example, enter the command ?\":\" or help(\":\").\n\n\n\n\nVariables\nIn most cases, however, you will want to cache results of commands in order re-access them later on. Then, variables come into play. In R variables are defined using the  \\&lt;-  operator. Although the output will not be printed to the Console pane directly, we store the variable in our temporary workspace. Thus, the variable should be visible under Values ​​in the Environment pane in R-Studio.\nx &lt;- 8 + 7         # assignment to variable x\n\ny &lt;- 4 * 2         # assignment to variable y\nWe only get the value of the variable when we call its name as a command or look into the Environment pane. Further calculations with the variables are also possible:\nx            \n## [1] 15\n\ny  \n## [1] 8\n\nx + y        \n## [1] 23\n\nmy.variable &lt;- x + y\nmy.variable \n## [1] 23\nA convention in R is to include points in variable names, e.g., my.variable. This is for the sake of clarity only, especially when many variables exist, and has no deeper meaning beyond that. Strictly avoid any other special symbols in variable names.\n\n\n\n\nFunctions\nA function is a piece of code written to carry out a specific task. It can accept arguments and returns one or more values.\nR standard packages offer several arithmetic built-in functions and constants, which make statistical analysis quite efficient. A function generally consists of a function name and two parentheses  () , in which arguments are given as input. Of course, previously created variables can serve as arguments for functions, too:\nsqrt(64)           # square root \n## [1] 8\n\nexp(3)             # exponential\n## [1] 20.08554\n\ncos(13)            # cosinus\n## [1] 0.9074468\n\npi                 # constant number pi\n## [1] 3.141593\n\nround(pi)          # round values\n## [1] 3\n\na &lt;- 6\nb &lt;- 9.2\nlog10(a + b)       # logarithm (base 10)\n## [1] 1.181844\nSo, the best way to learn about the internal workings of a function is to write your own one. R allows to create user defined functions, whereby the basic construct of each function is the following:\nname.of.fun( arguments ) { body }\nThe code in between the curly braces is the body of the function. This is where you define all the commands your functions will perform. Let us write a function that calculates a normalized ratio of two numeric values! It is best to copy the following code into the script window, select everything and then execute the code. The function should then appear in the Environment window and can be called hereinafter.\nmy.fun &lt;- function(var1, var2) {\n  x &lt;- (var1 - var2) / (var1 + var2)\n  return(x)\n}\nExplanation: Use function() to create a new function and assign it to any variable, e.g.,my.fun. The two arguments var1 and var2 are placeholders for variables that are assigned when this function is called. Operations of the function are defined between the curly braces {}. Intermediate results, i.e., the x in our example, exist locally within the function – they do not appear in the Environment window. Only variables given to the [return` function can be saved as a variable. The function call is done via:\nresult &lt;- my.fun(42, 13)\nresult \n## [1] 0.5272727\nIf you feel ready click the button below and check your understanding up to here!",
    "crumbs": [
      "R Crash Course",
      "Part I",
      "Package management, variables and functions"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/PartIII.html#indexing-in-matrices",
    "href": "R-Crash-Course/contents/PartIII.html#indexing-in-matrices",
    "title": "Part III",
    "section": "Indexing in matrices",
    "text": "Indexing in matrices\nIndexing in matrices behaves adequately to indexing in vectors, except that we now put two index numbers in the square brackets [] to address rows and columns. Both numbers must always be separated by a comma [line, column]. If we want all the entries from one dimension, we simply leave the corresponding slot for the index numbers empty:\nm &lt;- matrix(1:15, nrow = 5, ncol = 3) \nm\n##      [,1] [,2] [,3]\n## [1,]    1    6   11\n## [2,]    2    7   12\n## [3,]    3    8   13\n## [4,]    4    9   14\n## [5,]    5   10   15\n\nm[ , 2]                     # extract second column\n## [1]  6  7  8  9 10\n\nm[3,  ]                     # extract third row\n## [1]  3  8 13\n\nm[1, c(2, 3)]               # elements of first row in 2nd and 3rd column\n## [1]  6 11",
    "crumbs": [
      "R Crash Course",
      "Part III",
      "Matrices"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/PartIII.html#calculate-with-matrices",
    "href": "R-Crash-Course/contents/PartIII.html#calculate-with-matrices",
    "title": "Part III",
    "section": "Calculate with matrices",
    "text": "Calculate with matrices\nR is an equally powerful tool in terms of linear algebra. Appropriate to the vectors, whole matrices can be multiplied by a single value (scalar multiplication) or element by element. For the latter, however, the matrices necessarily need the same dimensionality dim().\nm1 &lt;- matrix(1:8, nrow = 2)\nm1\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    3    5    7\n## [2,]    2    4    6    8\n\nm1 * 5                         # scalar multiplication\n##      [,1] [,2] [,3] [,4]\n## [1,]    5   15   25   35\n## [2,]   10   20   30   40\n\nm1 * m1                        # multiplication element-wise\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    9   25   49\n## [2,]    4   16   36   64\nSome useful and commonly used functions:\nm2 &lt;- matrix(1:6, nrow = 2)\nm2\n##      [,1] [,2] [,3]\n## [1,]    1    3    5\n## [2,]    2    4    6\n\ncolMeans(m2)               # mean of all columns\n## [1] 1.5 3.5 5.5\n\ncolSums(m2)                # sum of all columns\n## [1]  3  7 11\n\nrowMeans(m2)               # mean of all rows\n## [1] 3 4\n\nrowSums(m2)                # sum of all rows\n## [1]  9 12\n\nt(m2)                      # transpose a matrix\n##      [,1] [,2]\n## [1,]    1    2\n## [2,]    3    4\n## [3,]    5    6\n\nm3 &lt;- matrix(1:6, ncol = 2)\nm3 %*% m2                  # matrix multiplication\n##      [,1] [,2] [,3]\n## [1,]    9   19   29\n## [2,]   12   26   40\n## [3,]   15   33   51\nMatrix multiplications assume that the inner dimensions of the two matrices are the same length (here you will find further information).\nNext training session incoming:",
    "crumbs": [
      "R Crash Course",
      "Part III",
      "Matrices"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/RExerciseV.html",
    "href": "R-Crash-Course/contents/RExerciseV.html",
    "title": "R – Exercise V",
    "section": "",
    "text": "Use ifelse() and your data frame df from exercise IV: If the person is less than or equal to 175 cm, it should have the attribute “small”, otherwise “tall”. Save the result in your df as the new column size.category.\n\n\n\nClick here to see the answer\n\nx &lt;- ifelse(df$size &lt;= 175, \"small\", \"tall\")\n\nx\n## [1] \"small\" \"small\" \"tall\" \"tall\" \"small\"\n\ndf$size.categorie &lt;- x\n\ndf\n##    name age size    city weight size.categorie\n## 1  Anna  66  170 Hamburg  115.0          small\n## 2  Otto  53  174  Berlin  110.2          small\n## 3 Natan  22  182  Berlin   95.0          tall\n## 4   Ede  36  180 Cologne   87.0          tall\n## 5  Anna  32  174 Hamburg   63.0          small\n\n\nWrite a loop that outputs all integers from 5 to 15!\n\n\n\nClick here to see the answer\n\nvektor &lt;- 5:15\nvektor\n##  [1]  5  6  7  8  9 10 11 12 13 14 15\n\nfor (i in vektor) {\n  print(i)\n}\n## [1] 5\n## [1] 6\n## [1] 7\n## [1] 8\n## [1] 9\n## [1] 10\n## [1] 11\n## [1] 12\n## [1] 13\n## [1] 14\n## [1] 15\n\n\nAdvanced: Create a for loop that outputs the arithmetic mean for each variable (column) of your data frame df – provided that the variable is numeric!\n\n\n\nClick here to see the answer\n\nfor (i in 1:ncol(df)) {\n  \n  if (class(df[[i]]) == \"numeric\") {\n    print(names(df)[i])\n    result &lt;- mean(df[[i]], na.rm=TRUE)\n    print(result)\n  }\n  \n}\n## [1] \"age\"\n## [1] 41.8\n## [1] \"size\"\n## [1] 176\n## [1] \"weight\"\n## [1] 94.04\n\n# Even if it looks complicated, take your time and go through it line by line. Everything should be known by now!",
    "crumbs": [
      "R Crash Course",
      "Part V",
      "Excersice V"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/RExerciseIII.html",
    "href": "R-Crash-Course/contents/RExerciseIII.html",
    "title": "R – Exercise III",
    "section": "",
    "text": "Create a matrix named m1 with three rows and five columns and all the numeric (integer) values from 6 to 20!\n\n\n\nClick here to see the answer\n\nm1 &lt;- matrix(6:20, nrow = 3, ncol = 5)\n\nm1\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    6    9   12   15   18\n## [2,]    7   10   13   16   19\n## [3,]    8   11   14   17   20\n\n\nMultiply all elements in m1 by 0.5! Overwrite the matrix m1 with the result!\n\n\n\nClick here to see the answer\n\nm1 &lt;- m1 * 0.5\n\n\nCreate another matrix m2 with one row and five columns and all the numeric (integer) values from 1 to 5!\n\n\n\nClick here to see the answer\n\nm2 &lt;- matrix(1:5, nrow = 1, ncol = 5)\n\nm2\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n\n\nCalculate the sum of all elements in m2!\n\n\n\nClick here to see the answer\n\nsum(m2)\n## [1] 15\n\n\nCombine m1 and m2 with rbing(). Save the result as m3 and check the dimension of the new matrix!\n\n\n\nClick here to see the answer\n\nm3 &lt;- rbind(m1, m2)\n\nm3\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]  3.0  4.5  6.0  7.5  9.0\n## [2,]  3.5  5.0  6.5  8.0  9.5\n## [3,]  4.0  5.5  7.0  8.5 10.0\n## [4,]  1.0  2.0  3.0  4.0  5.0\n\ndim(m3)\n## [1] 4 5\n\n\nIndex the 5th column of m3!\n\n\n\nClick here to see the answer\n\nm3[ , 5]\n## [1]  9.0  9.5 10.0  5.0\n\n\nIndex the 2nd and 4th lines of m3!\n\n\n\nClick here to see the answer\n\nm3[ c(2, 4), ]\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]  3.5    5  6.5    8  9.5\n## [2,]  1.0    2  3.0    4  5.0\n\n\nCalculate the sums for all columns in m3!\n\n\n\nClick here to see the answer\n\ncolSums(m3)\n## [1] 11.5 17.0 22.5 28.0 33.5\n\n\nCalculate the standard deviation for the 3rd column in m3!\n\n\n\nClick here to see the answer\n\nsd( m3[ , 3] )\n## [1] 1.796988\n\n\nFrom m3, index the element in the 2nd column and 2nd line and all eight adjacent elements! Save the result as m4 and examine its object class!\n\n\n\nClick here to see the answer\n\nm4 &lt;- m3[2:4, 2:4]\n\nm4\n##      [,1] [,2] [,3]\n## [1,]  5.0  6.5  8.0\n## [2,]  5.5  7.0  8.5\n## [3,]  2.0  3.0  4.0\n\nclass(m4)\n## [1] \"matrix\"",
    "crumbs": [
      "R Crash Course",
      "Part III",
      "Excersice III"
    ]
  },
  {
    "objectID": "RESEDA/contents/Download.html",
    "href": "RESEDA/contents/Download.html",
    "title": "Download",
    "section": "",
    "text": "We show you two platforms which allow users to view and download remote sensing data such as satellite images, aerial photographs, and cartographic products. You will learn how to use them in order to query and download Landsat 8, Sentinel 2 and Sentinel 1 data from the archives.\n\n\nLandsat Data via USGS EarthExplorer\n\n\n\nSentinel Data via ESA Dataspace",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Download"
    ]
  },
  {
    "objectID": "RESEDA/contents/Download.html#EarthExplorer",
    "href": "RESEDA/contents/Download.html#EarthExplorer",
    "title": "Download",
    "section": "1 USGS EarthExplorer User Interface",
    "text": "1 USGS EarthExplorer User Interface\nThe EarthExplorer supports online search in comprehensive databases, quicklook visualizations, metadata export, and data download for earth science data from the archives of the U.S.Geological Survey (USGS). You get to the USGS EarthExplorer via the following URL using any web browser (Firefox in our RESEDA VM):\nhttps://earthexplorer.usgs.gov/\nOnce clicked, the main EarthExplorer graphical user interface (GUI) should be loaded, which is composed of three key elements:\n\n\nUSGS EarthExplorer user interface\n\n\n1 Header Menu Bar: Buttons for login and registration services, as well as help, RSS and feedback functionalities. After login you can save and load queries here\n2 Data Search Side Bar: Search components are divided among four tabs and allow you to enter search criteria, select datasets to query, enter additional criteria, and review results in a tabular window\n3 Image View with Navigation Elements: embedded Google Maps components to visualize search results, with standard Google Maps navigation tools, i.e., zoom in/ zoom out, street view (lower right corner), and coordinate information of current cursor position (upper right corner). You can toggle between satellite imagery view and GIS data view by selecting the adequate button in the top left corner",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Download"
    ]
  },
  {
    "objectID": "RESEDA/contents/Download.html#registration",
    "href": "RESEDA/contents/Download.html#registration",
    "title": "Download",
    "section": "2 Registration",
    "text": "2 Registration\nFirst of all, to fully use the services of the EarthExplorer, you need to register by clicking on the register button in the header menu bar and proceed through the user registration. Only registered users can download data. You definitely need a working email address for this. The information gathered from the registration process is not distributed to other organizations and is only used to determine trends in data usage. You have to work through a user affiliation/data usage and address page:\n\n\n\nOnce you have completed the registration, you should get an email on your given email address to confirm your account. After confirmation you will be redirected to the login page where you have to fill in your username and password (or click on “login” in the header menu bar).\n\n\nUSGS EarthExplorer login formular\n\nAfter login you will notice your username and your “shopping”/ item basked in the header menu bar:\n\n\nSuccessful login",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Download"
    ]
  },
  {
    "objectID": "RESEDA/contents/Download.html#Performe-search",
    "href": "RESEDA/contents/Download.html#Performe-search",
    "title": "Download",
    "section": "3 Perform a Search",
    "text": "3 Perform a Search\nWe want to use an example to explore the search function of the EarthExplorer: We are looking for all Landsat 8 scenes that depict Berlin in summer 2017 and have a cloud cover of less than 10 %!\nEarthExplorer provides four tabs in the search procedure to guide you through your search request:\n\n\nEarthExplorer search tabs\n\n1. Enter Search Criteria: This one helps defining an area of interest (AOI) and a time span in which data will be found. The most straightforward way to define the AOI is to use the integrated Google Maps by typing in an address or place name, e.g., “Berlin, Alexanderplatz”, click  and choose one of the prompted suggestions. Designations for geographical longitude and latitude of the desired position are also possible, e.g., “52.5223, 13.4132”.\n\n\nDefining area of interest (AOI)\n\nFurthermore there is a global notation used for cataloging Landsat data, called Worldwide Reference System (WRS), whereby Landsat 8 follows the WRS-2. This system divides the Earth’s surface into the recording geometries of the Landsat acquisitions. There are a WRS-2 overview map and a WRS-2 Path/Row to Latitude/Longitude converter provided by the USGS. Especially the converter helps to find all possible Path-Row combinations for your AOI, which is 192/024 and 193/23 in our case for Berlin. Entering one of those Path/Row pairs and click  to add the adequate center of the Landsat acquisition as a coordinate to your Google Map interface:\n\n\nUse Path and Row specifications for queries\n\nIn order to delete any given coordinate, press the red cross  next to it.\nAnother easy way to define your AOI is to just left click within the map, which automatically adds a coordinate for a single coordinate search. By defining two points on the map, you will do a line search, which results in all data products which intersects the line. By defining three or more coordinates, a polygon is automatically displayed, forming your AOI:\n\n\nPolygon defined by four coordinates via left clicking in the Google Map interface\n\nThere are several other ways to define the subject area more precisely, e.g., by shapefiles, features, predefined areas, or kmls, but the methods described are effective and usually sufficient. For more detailed descriptions please have a look at the online help.\nTo make your query more concrete, you can define the time span within which you want to get data at the bottom of the Search Criteria tab. Simply set the start date and end date as well as all desired months, in our case june, july and august:\n\n\nDefining the time span and months of interest\n\nCloud Cover: After selecting the date range, you can further define your search by restricting to a percentage for cloud cover, e.g., 10%:\n\n2. Select Your Data Set(s): The Data Set tab categorizes datasets into similar data collections. There is a dynamic tree structure, which allows you to expand/ collapse products by pressing on the plus and minus signs next to it. As you can see, there is a huge amount of data to choose from. Landsat 8-9 Level-1 data can be found at Landsat &gt; Landsat Collection 2 Level-1 &gt; Landsat 8-9 OLI/TIRS C2 Level-1:\n\n\nLandsat Collection 2 Data Level-1 products\n\nLandsat 8 Level-2 data is also available on demand under Landsat &gt; Landsat Collection 2 Level-2 (On-Demand) &gt; Landsat 8-9 OLI/TIRS C2Level-2 (see chapter Landsat 8-9 for more information).\n3. Additional Criteria (Optional): This tab helps to further narrow the results of your search query by defining additional search criteria, e.g., the allowed proportion of cloud cover over land, absolute cloud cover, day or night as well as Path/Row restrictions. In addition, you can use the unique product ID (e.g., LC09_L1TP_193024_20240629_20240702_02_T1) to find specific individual scenes.\n\n\nAdditional Criteria tab: cloud cover restrictions\n\n4. Search Results: When set, click on  on the bottom of the Data Search Side Bar or the Results tab on the top to execute your research. You will most likely get four data products as a result (depending on the shape of your AOI):\n\n\nSearch results\n\nThree scenes are not much, are they? A cloud cover under 10% is already pretty strict, when you allow more cloud coverage, the number of scenes will increases. Each product is given a unique ID, as well as a acquisition time and the WGS-2 Path and Row. Furthermore there is a number of overlay and download controls you can choose from for each scene:\n\n\nOverlay and download controls\n\n\n1 Show Footprint: display the contour of a scene on Google Map in true color\n2 Show Browse Overlay: display a preview image of the scene\n3 Compare Browse: activate this button on multiple scenes, then overlay those here\n4 Show Metadata and Browse: display the browse image and full metadata for the selected scene\n5 Download Options: allows registered users to download the selected data\n6 Add to Bulk Download: allows registered users to bulk download the selected data\n7 Order Scene: allows registered users to order or request specialized processing of products\n8 Exclude Scene from Results: delete the particular scene from current result window",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Download"
    ]
  },
  {
    "objectID": "RESEDA/contents/Download.html#Down-sec",
    "href": "RESEDA/contents/Download.html#Down-sec",
    "title": "Download",
    "section": "4 Download a Dataset",
    "text": "4 Download a Dataset\nIn order to download the bundle Level-1 scene (see chapter Landsat 8 and Landsat 9), click on Download Options  and choose the Product Option dropdown option:\n\n\nDownload options for a Landsat 9 Level-1 scene\n\nAfter clicking on the dropdown option, the Product Download Options tab would appear, where the top most option is selected to download the whole bundle:\n\n\nProduct Download options for a Landsat 9 Level-1 scene\n\nYou will receive a zipped file, which contains all spectral bands as georeferenced geotiff-files.\nAnyway, Level-2 products need a preprocessing and are NOT available for immediate download. In order to acquire Level-2 data, pick Landsat Collection 1 Level-2 in the Data Sets tab (as shown in 2. Select Your Data Set(s)). In the Result tab you have to use Order Scene  to put all the wanted scenes in your Item Basket. A number next to your Item Basket shows how many scenes you have chosen:\n\n\nAdd scenes to Item Basket\n\nWhen you are done, click on Item Basket in the Header Menu Bar to submit your order. You will see a list of all selected scenes. Confirm your selection by pressing Proceed To Checkout:\n\n\nOrder overview when clicked on Item Basked\n\nOn the next screen, press Submit Order:\n\n\nSubmit your order\n\nDONE! You will be given a unqiue order ID and a confirmation email will be sent to your email address – check in your email inbox!\n\n\nOrder confirmation\n\nAll Level-2 orders submitted through ESPA are processed within 2-5 days, depending on the size of the order and the backlog already in the system.\nBe patient.\nA second email confirmation will be send when the products are ready for download. From this moment on, all scenes will remain available for 10 days.\nClick on the Order status url in your second confirmation email, which redirects you to the following website:\n\n\nOrder overview given by the link of the second email\n\nClick on your Order ID, which brings you to the download site. Simply click on the download link in order to get your data:\n\n\nFinally get your Level-2 data!",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Download"
    ]
  },
  {
    "objectID": "RESEDA/contents/Download.html#copernicus-data-space-ecosystem",
    "href": "RESEDA/contents/Download.html#copernicus-data-space-ecosystem",
    "title": "Download",
    "section": "1 Copernicus Data Space Ecosystem",
    "text": "1 Copernicus Data Space Ecosystem\nCopernicus Data Space Ecosystem provides complete, free and open access to Sentinel-1, Sentinel-2 and Sentinel-3 user products. Sentinel products are available at no cost for anybody, provided you accept the Legal Notice on the use of Copernicus Sentinel Data and Service Information. You get to the ESA SciHUB via the following URL using any web browser (Firefox in our RESEDA VM):\nhttps://dataspace.copernicus.eu/\nOnce loaded, the main SciHUB graphical user interface (GUI) should be visible, which is composed of several key elements:\n\n\nCopernicus Data Space Ecosytem Browser",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Download"
    ]
  },
  {
    "objectID": "RESEDA/contents/Download.html#registration-1",
    "href": "RESEDA/contents/Download.html#registration-1",
    "title": "Download",
    "section": "2 Registration",
    "text": "2 Registration\nFirst of all, same as with USGS EarthExplorer, you need to register in order to fully use the services by clicking on the login button in the Header Menu Bar and proceed through the user registration. The registration grants access rights for searching and downloading Sentinels products:\n\n\nCreate an account using the register option\n\n\n\n\nOnce you have completed the registration, you should get an email on your given email address to confirm your account. After confirmation you may login with your credentials.",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Download"
    ]
  },
  {
    "objectID": "RESEDA/contents/Download.html#perform-a-search",
    "href": "RESEDA/contents/Download.html#perform-a-search",
    "title": "Download",
    "section": "3 Perform a Search",
    "text": "3 Perform a Search\nLet us use an example to explore the search function of the Copernicus Data Space Ecosystem Browser: We are looking for all Sentinel 2 scenes (Level2A L2A) that depict Berlin in the last summer months (June, July & August) and have a maximum cloud cover of 10 %!\nYou can write the name of the area you are interested in in the field or zoom in.\n\n\nSearching and/or zooming to your area of interest using the navigation tools\n\nThen draw a polygon covering your area of interest.\n\n\nCreate a polygon for your area of interest\n\nIt makes sense to filter the results using the advanced search settings which can be found in the left area of the Search Panel.\n\n\nOverview of the main filter options\n\n\nData sources: Filter data by data source\nCloud cover : filter data by cloud cover\nTime range: returns all the products whose sensing dates are included in the defined period\nIngestion Period: returns all the products whose publication dates on the Data Hub are included in the defined period\n\nPress the search button to start the query. The search results list provides all the products matching the submitted search query. Each result consists of thumbnail, the product name, a download-link, the sensing date, the instrument name, the imaging mode, the satellite name and the size of the file in MB. In the upper part of the result window, the current query can be reached by pressing the button “Go to search”.\n\n\nQuery results\n\nThere are the several options for each product when hovering over it with the mouse cursor:\n\n\nQuery results\n\n\n1 Product Info: select items in order to add them to the cart\n2 Zoom to Product: zoom to product in map view\n3 Add Product to Workpace: adding products to workspace in order to download products\n4 Download Product: directly download product\n5 Visualize: Furthermore you might visualize the data within the browser",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Download"
    ]
  },
  {
    "objectID": "RESEDA/contents/Download.html#download-one-or-several-datasets",
    "href": "RESEDA/contents/Download.html#download-one-or-several-datasets",
    "title": "Download",
    "section": "4 Download one or several Datasets",
    "text": "4 Download one or several Datasets\nIn order to download a single Sentinel scene simply click on the download icon and pick a location on your hard drive.\nIf you want to download multiple datasets, choose Add Product to workspace. The workspace can be viewed in your user profile:\n\n\nOpen workspace\n\nWhen you examine your shopping cart, it is possible to select just some of them or to bulk download all of the products in the workspace.\n\n\nworkspace\n\nIf you want to know more about the Copernicus Data Space Ecosystem Browser, please, have a look here: videos\n\n\nVideo tutorials",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Download"
    ]
  },
  {
    "objectID": "RESEDA/contents/Analyse.html",
    "href": "RESEDA/contents/Analyse.html",
    "title": "Analyse",
    "section": "",
    "text": "Analyse\nWe will use two possible scenarios in order to show you the possibilities of R when it comes to remote sensing issues:\n\nClassification: a common land cover classification of a multispectral Landsat 8 scene\nRegression: generation of sub-pixel information based on Landsat 8 data and high resolution reference data\n\nA complete workflow utilizing various R packages is given in the next sections.\n\n\nChapter in a Box\nIn this chapter, the following content awaits you:\nMachine Learning Basics\n- familiarize yourself with the basic terms of the ML, e.g., unsupervised vs. supervised, linear vs. nonlinear, parametric vs. non-parametric, over-fitting vs. under-fitting\nClassification\n- learn how to sample polygons in QGIS\n- classify several land cover classes based on a multispectral Landsat 8 scene using a RF and a SVM\n- test the performance of your classifier via a learning curve\nRegression\n- learn how to prepare reference polygons for regressor training\n- perform a Support Vector Regression",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Chapter in a box"
    ]
  },
  {
    "objectID": "RESEDA/contents/EarthExplorer-Exercise.html",
    "href": "RESEDA/contents/EarthExplorer-Exercise.html",
    "title": "EarthExplorer Exercise",
    "section": "",
    "text": "EarthExplorer Exercise\n####needs to be completely reworked\n\nDownload all Landsat 9 Level-2 products which have sensed Berlin (location: Alexanderplatz) in the months of June through August 2017 and have a maximum cloud coverage of 10%! Save the downloaded files to a folder “landsatdata”\n\n\n\nAt least the following scenes should be included:\n####LC08_L1TP_192023_20170830_20170914_01_T1 LC08_L1TP_192024_20170830_20170914_01_T1\n\n####2) Download all Landsat 9 Level-2 products with the unique scene ID “LC08_L1TP_192023_20160827_20170321_01_T1”! Save the downloaded files to “/media/sf_exchange/landsatdata/” in the VM!\n\n\n(Hint: use additional criteria for ID searches) As you search for a unique ID, exactly one scene should be found:\n####LC08_L1TP_192023_20160827_20170321_01_T1\n\n####3) Find the latest Landsat 8 ascending (nighttime) imaging Berlin!\n\n\n(Hint: use additional criteria) Night scenes are not stored regularly on the data server. The latest night scenes are from the September of 2014:\nLC08_L1GT_050221_20140904_20170419_01_T2\n\n\nSearch all Landsat 5 Level-1 scenes that have shown Berlin 2017:\n\n\n\nYou will not find any scenes, because Landsat 5 was officially decommissioned on June 5, 2013."
  },
  {
    "objectID": "RESEDA/contents/Classification-in-R.html",
    "href": "RESEDA/contents/Classification-in-R.html",
    "title": "Classification in R",
    "section": "",
    "text": "We want to utilize a Random Forest (RF) and a Support Vector Classifier (SVM) algorithm in order to classify the Berlin land cover in six elementary categories: bare soil, water, grassland, forest, urban low density, and urban high density. Therefore, we need an image dataset and a shapefile containing points or polygons to which the respective class is attributed.\nThe workflow will be exemplified by a L9 scene (ID: LC09_L2SP_193023_20250328_20250329_02_T1), which you may already have acquired during the L8/L9 Download Exercise. You need to preprocess the scene as shown in chapter Preprocess. In addition, we narrowed our research area to Berlin to keep the data small (as shown in chapter Visualize in R).\nYou can download both the preprocessed image and the shapefile for testing purposes here.\n\nThis section guides you through a complete classification process for satellite imagery. The resulting classification maps will be validated in the next chapter.\nSample in QGIS – some basic considerations and tips for sampling\n– collect training polygons in QGIS for supervised classification\nPrepare Samples in R – import training polygons into R\n– use training polygons to extract raster information\n– put everything together in a data frame\nRF Classification – train a RF model with “randomForest” package\n– classify image data and export a classification image\nSVM Classification – train a SVM (C-Classification method) with “e1071” package\n– classify image data and export a classification image",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Classification in R"
    ]
  },
  {
    "objectID": "RESEDA/contents/Classification-in-R.html#preliminary-thoughts-about-sampling",
    "href": "RESEDA/contents/Classification-in-R.html#preliminary-thoughts-about-sampling",
    "title": "Classification in R",
    "section": "Preliminary thoughts about sampling",
    "text": "Preliminary thoughts about sampling\nProbably the most frequently asked questions are how many polygons should be created by class and and how big should they be?\n– Good questions!\nUnfortunately, those just can not be answered directly. The amount of training data you need, i.e., polygon count and size, depends both on the\n\ncomplexity of your classification problem (number and similarity of target classes, …) &\ncomplexity of your classification algorithm (number of parameters or weights, RF, SVM, ANN, ML, …).\n\nExamine links! Sampling data in machine learning is a science in itself, which is why there is a wealth of scientific publications about it (Curran & Williamson 1986, Figueroa et al. 2012) and even entire books (Marchetti et al. 2006, Hastie et al. 2017).\nFine, so far that is not much of a help…\nTo keep it very simple: You need a sample of your data that represents the problem you want to solve. Keep in mind, a classifier learns a mathematical function, which maps input data (e.g., spectral bands) to output data (e.g., class labels). In order to achieve this, you should provide enough training data to capture the relationships between input and output. Training data will optimally meet the following requirements:\n\nindependence of test data:\nA training dataset must be independent of the test dataset used for a validation, but can follow the same probability distribution. No training sample may be used to test (validate) the performance of the classifier! In the context of remote sensing data, it is also important that train and test data are spatially maximally distant avoid spatial autocorrelation (Morans I).\nmostly identical distributed:\nEach target class should be equally represented in the training data set. Most datasets do not have an exactly equal number of instances in each class. Small differences often do not matter. However, if there is a strong imbalance, e.g., 90% of all training data represent class 1 and only 10% class 2, most algorithms very quickly “overclassify” the more-prevalent classes. Some simple options to avoid this effect: Collect more samples of the low-represented classes, use data augmentation to synthetically create new samples for under-represented classes, or use a under sampling method. The simplest under sampling method is to delete samples from the over-represented classes during classifier training. We will use this latter method for the RF and SVM implementations later on.\nrepresentative for target classes:\nTraining data should cover as many intra-class variations as possible, e.g., all spectral classes of a thematic target class, such as deciduous trees and conifers for the target class “forest”. Especially with more complex, non-linear classifiers, such as RF and SVM, it is important to include near-border training samples to map the class transitions more accurately. For example, water bodies should also be sampled in the shore area rather than just creating polygons in deep water areas.\navailable in sufficient quantity:\nThere are statistical heuristic methods available to calculate a suitable sample size. Often a factor of the number of classes, the number of input features or the model parameters are used (e.g., 5 features – 25 training samples per class, Theodoridis et al. 2008) or the minimum number of samples necessary to perform the power calculation is searched (Dell et al. 2002). However, these rules are not universally applicable! Anyway, if you have many features, e.g., hundreds of spectral channels in hyperspectral images, it is important to collect even more samples to avoid the curse of dimensionality, i.e., Hughes phenomenon (Hughes 1968). This curse occurs when the samples can not reflect the possible parameter combinations in such a high dimensional feature space. As a result, the classification accuracy decreases as more features are included in the algorithm. The best way to find out if the training samples are sufficiently set is to plot a learning curve. A learning curve plots the model performance on the y-axis versus the size of the training dataset on the x-axis as a line. On this way, you may be able to evaluate the amount of data that is required for a solid model performance, or perhaps how little data you actually need before before the learning curve stagnates or even drops again. This plot can be generated during training, as shown in the next sections.\n\nBefore you start sampling the training data in QGIS, here are some general tips for digitizing your polygons, if you want to perform a monotemporal classification based on spectral features:\n\nevenly distribute the polygons for each class over the entire scene to best cover any atmospheric variations that may exist within the image\nfor each class, try to digitize an area of approximately the same size (sum of all polygons)\nkeep in mind: each raster pixel under your polygons is a training sample!\navoid huge polygons(!), e.g., creating a huge polygon over a homogeneous lake does not add much value in terms of characterization of the spectral properties of a lake. – create several small polygons covering different lakes instead\ntake your time! Sampling is an essential processing step and will largely determine your further analysis\n\nEnough theory, time to collect training data.",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Classification in R"
    ]
  },
  {
    "objectID": "RESEDA/contents/Classification-in-R.html#import-a-raster-dataset",
    "href": "RESEDA/contents/Classification-in-R.html#import-a-raster-dataset",
    "title": "Classification in R",
    "section": "Import a Raster Dataset",
    "text": "Import a Raster Dataset\nThe training polygons should define relevant areas for the differentiation of the desired target classes (bare soil, water, grassland, forest, urban low density, and urban high density in our example). To know where these surfaces are located, we need corresponding image data as a basis. So let us import an image dataset!\nFirst of all, open QGIS.\nThere are several ways to open a raster dataset here: Either navigate via the main menu to Layer &gt; Add Layer &gt; Add Raster Layer…, or press the corresponding icon  in the toolbar or press the shortcut Ctrl + Shift + R to open a file explorer window.\n\n\n\nLocation of Add Raster Layer-function in QGIS\n\n\nIn the file explorer window, navigate to the data folder which holds your L9 data and import a raster dataset. We will use a L9 scene showing Berlin (ID: LC09_L2SP_193023_20250328_20250329_02_T1) from the L9 Download Exercise. The spatial subset can be downloaded here directly:\n\n\nLandsat 9 spatial subset imported into QGIS\n\nIf you have started a new QGIS project (or just opened QGIS), the projection of the entire project will be based on the first dataset you load – in this case the raster file. You can see the current projection of the project in the lower right corner of QGIS. If you use our example data set, you should now see  there. Click on this entry to get more detailed information about coordinate system of our raster dataset (“WGS84 / UTM ZONE 33 N”). Alternatively, you can double-click the dataset in the Layer Panel and view the Coordinate Reference System (CRS) in the General-tab. We want to generate a new shapefile, which shares exactly this georeference system. This is the best way to ensure that the polygons are geographically correctly located in the end.",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Classification in R"
    ]
  },
  {
    "objectID": "RESEDA/contents/Classification-in-R.html#create-a-new-polygon-shapefile",
    "href": "RESEDA/contents/Classification-in-R.html#create-a-new-polygon-shapefile",
    "title": "Classification in R",
    "section": "Create a New Polygon Shapefile",
    "text": "Create a New Polygon Shapefile\nFirst, navigate to the area of interest (AOI) in your image data. Then click on the New Shapfile Layer  icon in the toolbar. If you can not find this icon, right-click in the toolbar area and make sure there is a check mark next to “Manage Layer Toolbar”, which should reveal this icon among others. Once clicked, the “New Shapefile Layer” dialog will be displayed. Choose “polygon” as the Type in the top row of the window. Click on the Coordinate System icon. A new window will pup up, allowing you the choose the CRS of your new shapefile. Choose the same CRS as your raster data (you can use the filter function at the top). On the Fields list, select “id”, and click the button  at the bottom of the list. Under “New field”, type “classes” in the Name box, click on . Finally, this should look like this:\n\n\n\nNew Shapefile dialog settings\n\n\nIf everything is set up, click OK. You will be prompted to the “Save layer as…” dialog. Type the file name (“training_data.shp”), choose a file path and click Save. You will be able to see the new shapefile in the Layers Panel of QGIS. Select it and press the Toggle Editing  icon in order to activate editing functionalities. Note that a little pencil symbol will show up on top of the layer, indicating that the layer is now editable. Now click on the Add Feature  icon. The mouse cursor will now look like a crosshair. Left-click on the map in the Map View to create the first point of your new feature. Keep on left-clicking for each additional point you wish to include in your polygon. When you have finished adding your points, right-click anywhere on the map area to confirm your polygon geometry. An attribute window will appear immediately, asking for your class label. Input the appropriate class label for your polygon and click OK. Click on the Toggle Editing  icon again in order to end editing and to save your changes by choosing Save.\nYou can edit the shape of a polygon with the Node tool . Delete any unwanted polygons by clicking on the tool called “Select Features by Area or Single Click” . Once activated you can left-click on polygons you want to delete, causing them to turn yellow. Then, press the delete key on your keyboard to remove the polygons (only in editing mode). Choose “Categorized” in the uppermost drop-down menu.\nAfter some time you should have collected some training areas:\n\n\n\nCollection of training polygons in one shapefile\n\n\nYou can also color the polygons during editing based on the “classes” attribute, which makes it easier for you to estimate the class distribution. Double-click the shapefile in the Layers Panel and navigate to the Style tab. Ensure that your attribute “classes” is selected in the drop-down menu below. Click Classify once to apply an individual color to each class (click on the colored boxes in order to change the colors) and confirm everything by pressing OK:\n\n\n\nStyle settings for coloring classes\n\n\nIf you think you have collected enough samples, save everything by clicking on the Toggle Editing  icon again and choose to Save.\nWe do not need QGIS anymore, so close it.",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Classification in R"
    ]
  },
  {
    "objectID": "RESEDA/contents/Classification-in-R.html#in-depth-guide",
    "href": "RESEDA/contents/Classification-in-R.html#in-depth-guide",
    "title": "Classification in R",
    "section": "In-depth Guide",
    "text": "In-depth Guide\nIn order to use functionalities of the terra package, load it into your current session via library(). If you did not yet install terra, please do so using install.packages():\n#install.packages(\"terra\")\nlibrary(terra)\nNext: set your working directory, in which all your image and shapefile data is stored by giving a character (do not forget the quotation marks \" \") variable to setwd(). Check your path with getwd() and the stored files in it via dir():\n\ndata folders\nsetwd(\"./data\")\n\ngetwd()\n## [1] \"/reseda/data/landsatdata\"\n\ndir()        \n## [1] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.tif\"                                                              \n## [2] \"polygons_training.dbf\"                                              \n## [3] \"polygons_training.prj\"                                              \n## [4] \"polygons_training.qpj\"                                              \n## [5] \"polygons_training.shp\"                                              \n## [6] \"polygons_training.shx\"                                              \nIf you do not get your files listed, you have made a mistake in your work path – check again! Everything ready to go? Fine, then import your raster file as img and your shapefile as shp and have a look at them:\nimg &lt;- rast(\"LC09_L2SP_193023_20250328_20250329_02_T1_subset.tif\")\nimg\n#### \n\n\nshp &lt;- shapefile(\"training_data.shp\")\nshp\n\n## class       : SpatVector \n## geometry    : polygons \n## dimensions  : 72, 1  (geometries, attributes)\n## extent      : 369802.9, 400028.9, 5812457, 5827504  (xmin, xmax, ymin, ymax)\n## source      : training_data.shp\n## coord. ref. : WGS 84 / UTM zone 33N (EPSG:32633) \n## names       : classes\n## type        :   &lt;chr&gt;\n## values      :   water\n##                 water\n##                 water\n                 \n####\nsame.crs(shp, img)\n## [1] TRUE\nBoth the rast()andvect()functions are provided by the terra package. As shown above, they create objects of the class SpatRaster and SpatVector respectively. The L9 raster provides 7 bands, and our example shapefile 72 features, i.e., polygons. You can check whether the projections of the two datasets are identical or not by executingsame.crs(shp, img). If this is not the case (output equals *FALSE*), the terra package will automatically re-project your data on the fly later on. However, we recommend to adjust the projections manually in advance to prevent any future inaccuracies usingproject().\\ Plot your data to make sure everything is imported properly (check [Visualize in R](./Visualization.qmd) for an intro to plotting). With the argumentadd = TRUE` in line 2 several data layers can be displayed one above the other:\nplotRGB(img, r = 4, g = 3, b = 2, stretch = \"lin\")\nplot(shp, col=\"red\", add=TRUE)\n\nIf you followed this course in the previous section, your shapefile should provide an attribute called “classes”, which includes your target classes as strings, e.g., “water” or “urban”. We will later turn this column into the factor data type because classifiers can only work with integer values instead of words like “water” or “urban”. When converting to factors, strings are sorted alphabetically and numbered consecutively. In order to be able to read the classification image at the end, you should make a note of your classification key:\nlevels(as.factor(shp$classes))\n## [1] \"baresoil\"  \"forest\"    \"grassland\" \"urban_hd\"  \"urban_ld\"  \"water\"\n\nfor (i in 1:length(unique(shp$classes))) {cat(paste0(i, \" \", levels(as.factor(shp$classes))[i]), sep=\"\\n\")}\n## 1 baresoil\n## 2 forest\n## 3 grassland\n## 4 urban_hd\n## 5 urban_ld\n## 6 water\nThe levels() function combines all occurrences in a factor-formatted vector. In the example shown above, value 1 in our classification image will correspond to the baresoil class, value 2 to forest, value 3 to grassland, etc.\nOptional: Let us take a look at the naming of the raster bands via the names() function. Those names can be quite bulky and cause problems in some illustrations when used as axis labels. You can easily rename it to something more concise by overriding the names with any string vector of the same length:\nnames(img)\n## [1] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.1\"\n## [2] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.2\"\n## [3] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.3\"\n## [4] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.4\"\n## [5] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.5\"\n## [6] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.6\"\n## [7] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.7\"\n\nnames(img) &lt;- c(\"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\", \"b7\")\n\nnames(img)\n## [1] \"b1\" \"b2\" \"b3\" \"b4\" \"b5\" \"b6\" \"b7\"\nAppropriate names for the input features are very helpful for orientation and readability. Of course you need to change the vector in line 10 according to your own input features.\nOur goal is to extract the raster values (x), i.e., all input feature values, and the class values (y) of every single pixel within our training polygons and put all together in a data frame. This data frame can then be read by our classifier. We extract the raster values using the command extract() from the raster package. The argument df = TRUE guarantees that the output is a data frame:\nsmp &lt;- extract(img, shp, df = TRUE)\nIt may take some time for this function to complete depending on the spatial resolution of your raster data and the spatial area covered by your polygons. If you have your data stored on an SSD, the process is completed much faster. It may be advisable to save the resulting object to the hard drive save(smp , file = \"smp .rda\") and load it from the hard disk if necessary load(file = \"smp.rda\"). On this way, the extract function does not have to be repeated again and again…\nThe data frame has as many rows as pixels are to be extracted and as many columns as input features are given (in this example the spectral channels). In addition, smp also provides a column named “ID”, which holds the IDs of the former polygon for each pixel (each polygon is automatically assigned an ID). Furthermore, we also know which polygon, i.e., each ID, belongs to which class. Because of this, we can establish a relationship between the deposited ID of each pixel and the class given in the shapefile. We use this to add another column to our data query describing each class. Then we delete the ID column because we do not need it anymore:\nsmp$cl &lt;- as.factor(shp$classes[smp$ID])\nsmp &lt;- smp[-1]\n\nsummary(smp$cl)\n##  baresoil    forest grassland  urban_hd  urban_ld     water \n##       719      2074      1226      1284       969       763\n\nstr(smp)\n## 'data.frame':    7035 obs. of  8 variables:\n##  $ b1: num  192 179 189 159 171 164 173 184 144 150 ...\n##  $ b2: num  229 203 221 179 194 188 192 208 166 165 ...\n##  $ b3: num  321 233 272 188 203 196 208 254 178 177 ...\n##  $ b4: num  204 130 164 97 116 108 119 150 83 80 ...\n##  $ b5: num  161 156 173 125 146 138 146 166 107 104 ...\n##  $ b6: num  100 93 109 63 82 71 82 104 51 46 ...\n##  $ b7: num  72 68 81 40 56 51 59 74 32 30 ...\n##  $ cl: Factor w/ 6 levels \"baresoil\",\"forest\",..: 6 6 6 6 6 6 6 6 6 6 ...\nNow you are ready to start training your classifier as described in the next sections!\nOptional: If you only include spectral information in your classifier, as in our example, it is often helpful to plot the so-called spectral profiles, or z-profiles. Those represent the mean values of each class for the individual spectral bands. You can also represent other features, e.g., terrain height or precipitation, however, you must then pay attention to the value range in the presentation and possibly normalize the data at first. The magic here happens in the aggregate() command, which combines all the rows of the same class . \\~ cl and calculates the arithmetic mean of those groups FUN = mean. This happens for all classes, in the cl column, where NA values are to be ignored via na.rm = TRUE. The rest of the functions in the following script are for visualization purposes only and include standard functions such as plot(), lines(), grid() and legend(). Use the help function for a detailed description of the arguments!\nsp &lt;- aggregate( . ~ cl, data = smp, FUN = mean, na.rm = TRUE )\n\n# plot empty plot of a defined size\nplot(0,\n     ylim = c(min(sp[2:ncol(sp)]), max(sp[2:ncol(sp)])), \n     xlim = c(1, ncol(smp)-1), \n     type = 'n', \n     xlab = \"L8 bands\", \n     ylab = \"reflectance [% * 100]\"\n     )\n\n# define colors for class representation - one color per class necessary!\nmycolors &lt;- c(\"#fbf793\", \"#006601\", \"#bfe578\", \"#d00000\", \"#fa6700\", \"#6569ff\")\n\n# draw one line for each class\nfor (i in 1:nrow(sp)){\n  lines(as.numeric(sp[i, -1]), \n        lwd = 4, \n        col = mycolors[i]\n        )\n  }\n\n# add a grid\ngrid()\n\n# add a legend\nlegend(as.character(sp$cl),\n       x = \"topleft\",\n       col = mycolors,\n       lwd = 5,\n       bty = \"n\"\n       )\n\nNote that the values represent only the arithmetic mean of the classes and do not allow any statement about the underlying distribution. However, such a z-profile plot helps to visually assess the separability of classes at the beginning.\n\n\nab hier weiter",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Classification in R"
    ]
  },
  {
    "objectID": "RESEDA/contents/Classification-in-R.html#in-depth-guide-1",
    "href": "RESEDA/contents/Classification-in-R.html#in-depth-guide-1",
    "title": "Classification in R",
    "section": "In-depth Guide",
    "text": "In-depth Guide\nIn order to be able to use the functions of the randomForest package, we must additionally load the library into the current session via library(). If you do not use our VM, you must first download and install the packages with install.packages():\n#install.packages(\"terra\")\n#install.packages(\"randomForest\")\nlibrary(terra)\nlibrary(randomForest)\nFirst, it is necessary to process the training samples in the form of a data frame. The necessary steps are described in detail in the previous section.\nnames(img) &lt;- c(\"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\", \"b7\")\nsmp &lt;- extract(img, shp, df = TRUE)\nsmp$cl &lt;- as.factor(shp$classes[match(smp$ID, seq_len(nrow(shp)))])\nsmp &lt;- smp[-1]\nAfter that, you can identify the number of available training samples per class with summary(). There is often an imbalance in the number of those training pixels, i.e., one class is represented by a large number of pixels, while another class has very few samples:\nsummary(smp$cl)\n##  baresoil    forest grassland  urban_hd  urban_ld     water \n##       719      2074      1226      1284       969       763\nThis often leads to the problem that classifiers favor and overclass strongly-represented classes in the classification. However, the Random Forest Algorithm, as an ensemble classifier, provides an ideal solution to compensate for this imbalance. For each decision tree, we draw a bootstrap sample from the minority class (class with the fewest samples). Then, we randomly draw the same number of cases, with replacement, from all other classes. This technique is called down-sampling.\nIn our example this is the class baresoil with 719 samples. With the rep() function we form a vector where the length corresponds to the number of target classes. We will use this vector to tell the classifier how many samples it should randomly draw per class for each decision tree during training:\ntable(smp$cl)  # Get class frequencies\n##  baresoil    forest grassland  urban_hd  urban_ld     water \n##       719      2074      1226      1284       969       763\n\nsmp_freq &lt;- as.data.frame(table(smp$cl))  # ✅ Convert freq to a usable format\nmin_count &lt;- min(smp_freq$Freq)  # ✅ Use column 'Freq' for the frequency count\nnum_levels &lt;- nrow(smp_freq)\n# Number of unique classes\n\nsmp_size &lt;- rep(min_count, num_levels)\n\nsmp_size\n## [1] 719 719 719 719 719 719\nThe complete training takes place via just one function call of tuneRF()! This function automatically searches for the best parameter setting for mtry – the number of variables available for each tree node. So we just have to worry about ntree, i.e., the number of trees to grow. 250-1000 trees are usually sufficient. Basically, the more the better, but many trees will increase the calculation time. When tuneRF() is called, we need to specify the training samples as x, i.e., all columns of our smp dataframe except the last one, and the corresponding class labels as y, i.e. the last column of our smp dataframe called “cl”:\nsmp_df &lt;- as.data.frame(smp, xy = FALSE)  # Extract values without coordinates\n\nsmp_df$cl &lt;- as.factor(smp_df$cl)\nsmp_freq &lt;- as.data.frame(table(smp_df$cl))\nmin_count &lt;- min(smp_freq$Freq)\n\nsmp_size &lt;- rep(min_count, nrow(smp_freq))\n\nrfmodel &lt;- tuneRF(\n  x = smp_df[, -ncol(smp_df)],\n  y = smp_df$cl,\n  sampsize = smp_size,\n  strata = smp_df$cl,  \n  ntree = 250,\n  importance = TRUE,\n  doBest = TRUE\n)\n\n## mtry = 2  OOB error = 2.5% \n## Searching left ...\n## mtry = 1     OOB error = 2.54% \n## -0.01704545 0.05 \n## Searching right ...\n## mtry = 4     OOB error = 2.7% \n## -0.07954545 0.05                                       \nIn line 3, we pass our smp.size to define how many samples it should draw per class, and strata = at line 4 defines the column which should use for this stratified sampling. The argument importance = in line 6 allows the subsequent assessment of the variable importance when set to TRUE. By setting the argument doBest to TRUE, the RF with the optimal mtry is output directly from the function.\n\nIf you use tuneRF, you will automatically get a plot that will tell you the OOB errors in the dependency of different mtry settings. As mentioned, the feature automatically identifies the best mtry setting and uses this to generate the optimal RF.\nWhen the model is created, we get some really useful information by executing the object name. First we get the command call with which we trained the model and the final number of variables tried at each split, i.e. the mtry parameter. Furthermore, we get an averaged out of bag (OOB) estimate, as well as a complete confusion matrix based on the training data! The column headers contain the classes of training pixels and the rows describe the corresponding classification. For a more detailed description of a confusion matrix, please refer to the chapter Validate Classifiers.\nrfmodel\n## \n## Call:\n##  randomForest(x = smp[-ncol(smp)], y = smp$cl, ntree = 300, mtry = mtry.opt,      \n##               strata = smp$cl, sampsize = smpsize, importance = TRUE) \n##                Type of random forest: classification\n##                      Number of trees: 300\n## No. of variables tried at each split: 2\n## \n##         OOB estimate of  error rate: 2.52%\n## Confusion matrix:\n##           baresoil forest grassland urban_hd urban_ld water class.error\n## baresoil       708      0         2        3        6     0 0.015299026\n## forest           0   2060         2        0       11     1 0.006750241\n## grassland        4      0      1220        0        2     0 0.004893964\n## urban_hd        18      0         0     1204       62     0 0.062305296\n## urban_ld         7     10         9       39      904     0 0.067079463\n## water            0      0         0        1        0   762 0.001310616\nBefore we started the training procedure, we set the argument importance = true, which now allow us to have a look at the importance variable using the varImpPlot command:\nvarImpPlot(rfmodel)\n\nThe variable importance shows the most significant or important features for the classification, which are band 5 and 6 in this case. For details of those metrics, please refer to the Random Forest Section. However, it is interesting to note that the most important bands also provide the largest spectral differences between the classes in the previous section.\nAdditionally, you can plot the RF model itself, which shows you the relationship between OOB Error and the number of trees used. We can color the lines by passing a vector of hex-colors whose length equals the number of classes + 1 – the first color is the average OOB line, which is also plotted automatically:\nplot(rfmodel, col = c(\"#000000\", \"#fbf793\", \"#006601\", \"#bfe578\", \"#d00000\", \"#fa6700\", \"#6569ff\"))\n\nWe can see a decrease in the error with increasing number of trees, where the urban classes have the highest OOB error values.\nSave the model by using the save() function. This function saves the model object rfmodel to your working directory, so that you have it permanently stored on your hard drive. If needed, you can load it any time with load().\nsave(rfmodel, file = \"rfmodel.RData\")\n#load(\"rfmodel.RData\")\nSince your model is now fully trained, you can use it to predict all the pixels in your image. The command method predict() takes a lot of work from you: It is recognized that there is an image which then passes through your Random Forest pixel by pixel. As with the training pixels, each image pixel is now individually classified and finally reassembled into your final classification image. Use the argument filename = to specify the name of your output map:\nresult &lt;- predict(\n  img, \n  rfmodel, \n  filename = \"classification.tif\", \n  overwrite = TRUE\n)\n\nprint(result)\nRandom Forest classification completed!\nIf you store the result of the predict() function in an object, e.g., result, you can plot the map using the standard plot command and passing this object:\nresult &lt;- classify(result, data.frame(unique(values(result)), 1:length(unique(values(result)))))  # Ensure proper factor levels\n\ncoltab(result) &lt;- c(\"#fbf793\", # baresoil\n                    \"#006601\", # forest\n                    \"#bfe578\", # grassland\n                    \"#d00000\", # urban_hd\n                    \"#fa6700\", # urban_ld\n                    \"#6569ff\"  # water\n)\n\nplot(result, axes = FALSE, box = FALSE)",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Classification in R"
    ]
  },
  {
    "objectID": "RESEDA/contents/Classification-in-R.html#in-depth-guide-2",
    "href": "RESEDA/contents/Classification-in-R.html#in-depth-guide-2",
    "title": "Classification in R",
    "section": "In-depth Guide",
    "text": "In-depth Guide\nIn order to be able to use the functions of the e1071 package, we must additionally load the library into the current session via [library(). If you do not use our VM, you must first download and install the packages withinstall.packages()`:\n#install.packages(\"terra\")\n#install.packages(\"e1071\")\nlibrary(terra)\nlibrary(e1071)\nFirst, it is necessary to process the training samples in the form of a data frame. The necessary steps are described in detail in the previous section.\nnames(img) &lt;- c(\"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\", \"b7\")\nsmp &lt;- extract(img, shp, df = TRUE)\nsmp$cl &lt;- as.factor(shp$classes[match(smp$ID, 1:nrow(shp))])\nsmp &lt;- smp[-1]\nAfter that, you can identify the number of available training samples per class with summary(). There is often an imbalance in the number of those training pixels, i.e., one class is represented by a large number of pixels, while another class has very few samples:\nsummary(smp$cl)\n##  baresoil    forest grassland  urban_hd  urban_ld     water \n##       719      2074      1226      1284       969       763\nThis often leads to the problem that classifiers favor and overclass strongly-represented classes in the classification. Unfortunately, a SVM can not handle this problem as elegantly as Random Forest methods, why you might need to prepare your training [data:\\](data:){.uri} Only if your data is imbalanced, you should consider to do the following undersampling: We shuffle our entire training dataset and pick out the same number of samples for each class. The purpose of shuffle is to prevent sampling of all pixels that are next to each other (spatial autocorrelation). The training data is mixed using the sample() method. We sample all rows of our dataset nrow(smp) in a random manner in line 10:\nhead(smp)\n##    b1  b2  b3  b4  b5  b6 b7    cl\n## 1 192 229 321 204 161 100 72 water\n## 2 179 203 233 130 156  93 68 water\n## 3 189 221 272 164 173 109 81 water\n## 4 159 179 188  97 125  63 40 water\n## 5 171 194 203 116 146  82 56 water\n## 6 164 188 196 108 138  71 51 water\n\nsmp &lt;- smp[sample(nrow(smp)), ]\n\nhead(smp)\n##       b1  b2  b3  b4   b5   b6   b7       cl\n## 2024 176 195 324 234 2146 1022  479   forest\n## 5545 409 517 844 988 3038 3005 1818 baresoil\n## 2877 195 212 363 293 2500 1435  690   forest\n## 6210 303 322 482 411 2465 1410  872 urban_hd\n## 6613 367 418 672 600 2855 1876 1210 urban_ld\n## 321  163 178 201 136  124   76   56    water\nCompare the first six entries of the smp dataset before and after the command in line 10 using the head() function. You will notice that the order of the rows is shuffled!\nWith the min() function, we can select the minimum value from the class column’s summary and use the ave() function to make a selection of the same size for each class:\nsummary(smp$cl)\n##  baresoil    forest grassland  urban_hd  urban_ld     water \n##       719      2074      1226      1284       969       763\n\nsmp.maxsamplesize &lt;- min(summary(smp$cl))\nsmp.maxsamplesize\n## [1] 719\n\nsmp &lt;- smp[ave(1:(nrow(smp)), smp$cl, FUN = seq) &lt;= smp.maxsamplesize, ]\n\nsummary(smp$cl)\n##  baresoil    forest grassland  urban_hd  urban_ld     water \n##       719       719       719       719       719       719\nLet the training begin!\nClassifications using an SVM require two parameters: one gamma \\(\\gamma\\) and one cost \\(C\\) value (for more details refer to the SVM section). These hyperparameters significantly determine the performance of the model. Finding the best hyparameters is not trivial and the best combination can not be determined in advance. Thus, we try to find the best combination by trial and error. Therefore, we create two vectors comprising all values that should be tried out:\ngammas = 2^(-8:5)\ngammas\n##  [1]  0.00390625  0.00781250  0.01562500  0.03125000  0.06250000\n##  [6]  0.12500000  0.25000000  0.50000000  1.00000000  2.00000000\n## [11]  4.00000000  8.00000000 16.00000000 32.00000000\n\ncosts = 2^(-5:8)\ncosts\n##  [1]   0.03125   0.06250   0.12500   0.25000   0.50000   1.00000   2.00000\n##  [8]   4.00000   8.00000  16.00000  32.00000  64.00000 128.00000 256.00000\nSo we have 14 different values for \\(\\gamma\\) and 14 different values for \\(C\\). Thus, the whole training process is done for 196 (14 * 14) models, often referred to as gridsearch. Conversely, this means that the more parameters we check, the longer the training process takes.\nWe start the training with the tune() function. We need to specify the training samples as train.x, i.e., all columns of our smp dataframe except the last one, and the corresponding class labels as train.y, i.e. the last column of our smp dataframe:\nsvmgs &lt;- tune(svm,\n              train.x = smp[-ncol(smp)],\n              train.y = smp[ ncol(smp)],\n              type = \"C-classification\",\n              kernel = \"radial\", \n              scale = TRUE,\n              ranges = list(gamma = gammas, cost = costs),\n              tunecontrol = tune.control(cross = 5)\n)\nWe have to set the type of the SVM to \"C-classification\" in line 4 in order to perform a classification task. Furthermore, we set the kernel used in training and predicting to a RBF kernel via \"radial\". We set the argument scale to TRUE in order to initiate the z-transformation of our data. The argument ranges in line 7 takes a named list of parameter vectors spanning the sampling range. We put our gammas and costs vectors in this list. By using the tunecontrol argument in line 8, you can set k for the k-fold cross validation on the training data, which is necessary to assess the model performance.\nDepending on the complexity of the data, this step may take some time. Once completed, you can check the output by calling the resultant object name:\nsvmgs\n## \n## Parameter tuning of 'svm':\n## \n## - sampling method: 5-fold cross validation \n## \n## - best parameters:\n##  gamma cost\n##      2    8\n## \n## - best performance: 0.02132796\nIn the course of the cross-validation, the overall accuracies were compared and the best parameters were determined: In our example, those are 2 and 8 for \\(\\gamma\\) and \\(C\\), respectively. Furthermore, the error of the best model is displayed: 2.1% error rate (which is quite good!).\nWe can plot the errors of all 196 different hyperparameter combinations in the so called gridsearch using the standard plot() function:\nplot(svmgs)\n\nThe plot appears very homogeneous, as the performance varies only very slightly and is already at a very high level. It can happen that the highest accuracies are found on the edge of the grid search. Then the training with other ranges for \\(\\gamma\\) and \\(C\\) should be done again.\nWe can extract the best model out of our svmgs to use for image prediction:\nsvmmodel &lt;- svmgs$best.model\nsvmmodel\n## \n## Call:\n## best.tune(method = svm, train.x = smp[-ncol(smp)], train.y = smp$cl, \n##     ranges = list(gamma = gammas, cost = costs), tunecontrol = tune.control(cross = 5), \n##     scale = TRUE, kernel = \"radial\", type = \"C-classification\")\n## \n## \n## Parameters:\n##    SVM-Type:  C-classification \n##  SVM-Kernel:  radial \n##        cost:  8 \n##       gamma:  2 \n## \n## Number of Support Vectors:  602\nSave the best model by using the save() function. This function saves the model object svmmodel to your working directory, so that you have it permanently stored on your hard drive. If needed, you can load it any time with load().\nsave(svmmodel, file = \"svmmodel.RData\")\n#load(\"svmmodel.RData\")\nSince your SVM model is now completely trained, you can use it to predict all the pixels in your image. The command method predict() takes a lot of work from you: It is recognized that there is an image which will be processed pixel by pixel. As with the training pixels, each image pixel is now individually classified and finally reassembled into your final classification image. Use the argument filename = to specify the name of your output map:\nresult &lt;- predict(img, svmmodel, filename = \"classification_svm.tif\", overwrite = TRUE)\n\nplot(result, \n     type = \"classes\",  # Ensures categorical visualization\n     col = c(\"#fbf793\", # baresoil\n             \"#006601\", # forest\n             \"#bfe578\", # grassland\n             \"#d00000\", # urban_hd\n             \"#fa6700\", # urban_ld\n             \"#6569ff\"  # water\n     ),\n     axes = FALSE, \n     box = FALSE\n)\nSupport Vector Machine classification completed!\nIf you store the result of the predict() function in an object, e.g., result, you can plot the map using the standard plot command and passing this object:",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Classification in R"
    ]
  },
  {
    "objectID": "RESEDA/contents/PrepareYourself.html",
    "href": "RESEDA/contents/PrepareYourself.html",
    "title": "Prepare Yourself",
    "section": "",
    "text": "Prepare Yourself\nThis chapter guides you on how to get all the necessary software you will need for this online course and introduce you to the fundamental layout of several useful programs. Read on to find out what it is all about!\n\n\nChapter in a Box\nIn this chapter, the following content awaits you:\nQGIS\n– introduction to QGIS user interface\nR-Studio\n– introduction to R-Studio user interface\n– instructions how to execute R commands in R-Studio\nSNAP\n– introduction to SNAP user interface",
    "crumbs": [
      "RESEDA",
      "Preparations",
      "Chapter in a box"
    ]
  },
  {
    "objectID": "RESEDA/contents/Preprocess.html",
    "href": "RESEDA/contents/Preprocess.html",
    "title": "Preprocess",
    "section": "",
    "text": "A preprocessing of remote sensing data is a crucial step in every analytical workflow, and can possibly be the most time consuming one.\nAnyway, we want to focus on converting the just-downloaded Landsat 8 and Sentinel 2 products into GeoTIFF files and the visualization in R and QGIS.\n\nChapter in a Box\nIn this chapter, the following content awaits you:\nPreprocess Landsat 8\n– unzip downloaded image data products\n– stack all bands of interest into a rasterstack and save it\n– create pyramid layers for rasterstacks\n– automate everything for BIG DATA analysis using R\nPreprocess Sentinel 2\n– convert image data from SAFE to tif format\n– resample image data to uniform geometrical resolution\n– automate everything for BIG DATA analysis using SNAP and GPT/ R\nVisualization\n– use R and QGIS for visualization of Landsat 8 data\n\nThe basic understanding of the various operators and data types in R is required. If you want to refresh this knowledge, you can now go to the R Crash Course!",
    "crumbs": [
      "RESEDA",
      "Preprocess",
      "Chapter in a box"
    ]
  },
  {
    "objectID": "RESEDA/contents/SNAP.html",
    "href": "RESEDA/contents/SNAP.html",
    "title": "SNAP",
    "section": "",
    "text": "SNAP\nThe SeNtinel Application Platform (SNAP) is an open source architecture for European Space Agency (ESA) toolboxes designed for exploitation of earth observation data under the the Scientific Exploitation of Operational Missions (SEOM) programme. SNAP is the common architecture for the Sentinel 1, 2 and 3 Toolboxes, which support the scientific exploitation for the ERS-ENVISAT missions, the Sentinel 1/2/3 missions and a range of national and third party missions. Those toolboxes contain the functionalities of previous toolboxes such as BEAM, NEST and Orfeo Toolbox. SNAP not only enables simple functionalities, such as opening and exploring data products, but also creating and computing complex, user-defined processing chains.\n\n\nDefault layout in SNAP\n\n\n1 Toolbar: Main menu and standard tools for saving/opening data products , navigation over image data , as well as point, line and polygon feature drawing tools . The main functionality is listed in the main menu under the menu items Raster, Optical and Radar. Those are also accessible via the graph builder , which we will use to automate and chain operations later on\n2 Product Explorer: lists all loaded data products along with their metadata and band information. The Pixel Info tab allows you to get all the information about the coordinates and raster values of where the mouse pointer is pointing. Double click a band in order to visualize it in the Image View panel, or right click a file and choose “open RGB image window” for a RGB composite\n3 Navigation Tool Window: The Navigation and World View tabs allow you to spatially locate the current image view in the overall picture and on a 3d globe. Furthermore the Color Manipulation tab allows visual adjustments via histogram stretching\n4 Product Library: browse and view metadata of your locally stored Sentinel data products or search for new data sets on the ESA Copernicus Data Space Ecosytem. Have a look at chapter ESA Data Space for more information.\n5 Image View: data you have loaded in your current session can be visualized here. If multiple data sets are open, you can switch back and forth between them by clicking on the tabs at the top of the Image View or tile them vertically or horizontal by clicking on  in the toolbar (1).\n\nThere is an official science toolbox exploitation platform, STEP for short. On this website you can access useful video tutorials showing most of the basic functionalities of SNAP. Furthermore there is an official STEP-forum, in which you can communicate with the SNAP developers or ask the sience community.\nSNAP not only supports the Sentinel missions, but also a wide range of third party products, including optical data (Sentinel 2 Toolbox), e.g., RapidEye, SPOT, MODIS (Aqua and Terra), Landsat (TM), and SAR data (Sentinel 1 Toolbox), e.g., ERS-1/2, ENVISAT, ALOS PALSAR, TerraSAR-X, COSMO-SkyMed and RADARSAT-2.",
    "crumbs": [
      "RESEDA",
      "Preparations",
      "SNAP"
    ]
  },
  {
    "objectID": "RESEDA/contents/Preprocess-Optical-Data.html",
    "href": "RESEDA/contents/Preprocess-Optical-Data.html",
    "title": "Preprocess Optical Data",
    "section": "",
    "text": "Preprocessing optical data in remote sensing (RS) research usually describes the correction of sensor- and platform-specific radiometric and geometric distortions of RS data. Depending on the available data, this can include the following processing steps (not ordered):\n\nradiometric correction due to variations in scene illumination and viewing geometry\ngeometric correcting /orthorectification to improve the positional accuracy of image data\natmospheric correction to minimoze atmohspheric effects and to improve the spectral comparability between two scenes\nconversion of data into various units (e.g., TOA radiation to reflectance) or between different data types (e.g., .img to .tiff)\ncompressing or subsetting data to save disc space\n\nFortunately, both Landsat 8/9 and Sentinel 2 Level-1 products are already radiometrically and geometrically corrected. If you use Level-2 scenes, the atmosphere correction is already done on both products by the data provider, which is a prerequisite especially for a time series analysis. So there is no need for you to do either of this corrections any more!\nWe want to focus on converting the just-downloaded products into GeoTIFF files. A GeoTIFF file is a standard used in many GIS and RS software solutions and allows georeferencing information to be embedded within the TIFF-file itself. Such information includes the map projection, the coordinate system, the ellipsoid, the geodetic datum, the geospatial extent, and many more.\n\n\nLandsat 8 as well as Landsat 9 ship as a tar-archived file with the spectral bands as individual georeferenced tif images. We want to stack these images into a single geotiff-file, i.e., into a so-called raster stack. Afterwards, it is much easier to work with the darta. While this could also be done in QGIS, we will use R for this preprocessing, because it is easier to automate things this way. This results in the following intermediate steps we have to check off:\n\nunzip your downloaded L9 files\nput together the spectral band files of your choice into a rasterstack\nsave this rasterstack to your hard drive\n(optional) delete all redundant data\n(optional) create pyramid layers for a better visualization in QGIS\n\nWe will practice everything exemplary on the basis of a single L9 Level-2 data set. There will be an exhaustive explanation for each line of code. Based on that we will develop a script that will automatically do everything for you in the future.\n\n\nPrerequisite\nThe following content requires that you have either successfully downloaded some Landsat 8 or 9 scenes as part of the Download Section Exercise, or that you have acquired some datasets from the USGS EarthExplorer by your own. If this is not the case, look into the chapter Download - Earthexplorer!\nDone? – Then start R-Studio now!\n\n\nPreprocess a single dataset\nWe recommend writing the following code in the script window of R-Studio and executing it from there (see chapter R-Studio).The example below assumes that you have one or more Landsat 8 or 9 scenes in a “landsatdata”-folder.\nWe will use the terra library to write a raster file later. Additional libraries should always be loaded first, using the function library():\nlibrary(terra)\n## Loading required package: sp\nA few libraries make use of other libraries. So does the terra library with the sp-package, which will be loaded automatically.\nSet the path to your working directory. Please, be aware that the delimiters between subfolders should be either “/” or “\\”. Especially, if you working on Windows machines, take care of this. Use the function getwd() to see which directory is set as your working directory.\nsetwd(\"E:P/ath/to/your/folder\")\n\ngetwd()\nThen define the Landsat 9 product of your choice with its entire file path and save it as the string variable product:\nproduct &lt;- \"LC09_L2SP_193023_20250328_20250329_02_T1.tar\"\n\nfile.exists(product)\n## [1] TRUE\nNOTE: this is just an example – you have to change the file and its path according to your own settings!\nYou can use the function file.exists() to check whether the file can be found on your system or not. If it returns FALSE, make sure you did not mess up the file name.\nWe want to unpack the file into a folder with the same name as the file. The character variable product already contains the full name of the Landsat scene. We just have to get rid of the suffix “.tar”. Therefore we can use the function substr() to delete the last four characters (=“.tar.gz”) of the string:\nproductname &lt;- substr(product, 1, nchar(product) - 4)\n\nproductname\n## [1] \"LC09_L2SP_193023_20250328_20250329_02_T1\"\nThe substring function substr() accepts three arguments here: a string (the entire file path with the suffix), and two integer values – one for a start (“1” = start with the first character) and one for a stop position within the given string. In order to define the stop position, we need to count the number of all characters of the file path via nchar(product) (which is 44 in our case) and substract 4.\nUnzip the Landsat product using the untar() method and define the directory to which all data will be extracted by setting the argument exdir = productname:\nuntar(product, exdir = productname)\nYou should notice a new folder being created next to your initial data product. This could take a short moment. During processing, a red exclamation point can be seen in the upper right corner of the console window. NOTE: If this step fails, your data is corrupt. In this case, you will need to download the file again because something went wrong during data transfer.\nAfter unpacking is complete, we want to have a look in the newly extracted folder and save the file names of all files in this folder into a vector productfiles using the function list.files. By providing the argument full.names = TRUE, the full paths are returned for each file:\nproductfiles &lt;- list.files(productname, full.names = TRUE)\n\nproductfiles\n## [1] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ANG.txt\"          \n## [2] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_MTL.txt\"          \n## [3] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_MTL.xml\"          \n## [4] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_QA_PIXEL.TIF\"     \n## [5] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_QA_RADSAT.TIF\"    \n## [6] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B1.TIF\"        \n## [7] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B2.TIF\"        \n## [8] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B3.TIF\"        \n## [9] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B4.TIF\"        \n##[10] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B5.TIF\"        \n##[11] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B6.TIF\"        \n##[12] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B7.TIF\"        \n##[13] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_QA_AEROSOL.TIF\"\n##[14] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_ATRAN.TIF\"     \n##[15] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_B10.TIF\"       \n##[16] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_CDIST.TIF\"     \n##[17] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_DRAD.TIF\"      \n##[18] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_EMIS.TIF\"      \n##[19] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_EMSD.TIF\"      \n##[20] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_QA.TIF\"        \n##[21] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_TRAD.TIF\"      \n##[22] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_URAD.TIF\"\nThese are all 22 files that come with a Level-2 product including bands regarding thermal observations (see Landsat 8 and Landsat-9 for more information). As a double check, you can have a look at the identical files in your file explorer:\n\nNow we select all the spectral bands that we want to put into our new data stack. Have a deeper look at the files in productfiles. The files are named after the corresponding spectral channels at the end of the file name, e.g., “SR_B1”, “SR_B2”, and so on. We use these terms to extract the bands by utilizing the grep() function. The following code looks a little bloated. Maybe it is. But it gives you the freedom to exclude individual bands that you may not need. NOTE: This is an example of level 2 data. Landsat Level-1 scenes potentially have 11 channels. In the case of Level 1 data, you could easily enter the remaining lines here.\nbands &lt;- c(grep('SR_B1', productfiles, value=TRUE),\n           grep('SR_B2', productfiles, value=TRUE),\n           grep('SR_B3', productfiles, value=TRUE),\n           grep('SR_B4', productfiles, value=TRUE),\n           grep('SR_B5', productfiles, value=TRUE),\n           grep('SR_B6', productfiles, value=TRUE),\n           grep('SR_B7', productfiles, value=TRUE)\n           ) \n\nbands\n## [1] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B1.TIF\"\n## [2] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B2.TIF\"\n## [3] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B3.TIF\"\n## [4] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B4.TIF\"\n## [5] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B5.TIF\"\n## [6] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B6.TIF\"\n## [7] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B7.TIF\"\nThe grep() function searches for the string, which is given as the first argument, e.g., ‘SR_B1’, in the vector of strings (2nd argument). Normally, when the function finds a match, it only returns the position of this find in the vector productfiles. By setting the argument value=TRUE we can write out the content of the vector at the respective position instead. By doing so, we create an vector bands containing all file paths via the standard combine-function c().\nNow we can create a raster stack based on the vector of bands in the variable bands. In order to do so, we use the function rast(), which is part of the terra-library we loaded in the beginning! This raster stack function creates a raster stack object based on a list of filenames. You will learn more about the beauty of raster stacks in the chapter Visualization.\nrasterstack &lt;- rast(bands)\nWe can save this raster stack and store it on your hard drive via writeRaster:\nwriteRaster(rasterstack, \n            paste0(productname, \".tif\"), \n            filetype = \"GTiff\",\n            overwrite = TRUE\n            )\nThe writerRaster fuction is a powerful tool, which is also provided by the terra package. In line 1 we set our raster stack object as the first argument. In line 2 we define the output name of the new file we will create. To do this, just add the suffix “.tif” to our filename using the handy function paste0, which just put all the strings together to one string. Line 3 explicitly defines the output format “GTiff”. How should I know this string “GTiff”, you ask? These strings are fixed by the function and are also listed for other data formats in the documentations. The fourth argument in line 4 only gives the authority to overwrite existing files.\nDONE! A new file containing all spectral bands is now written directly next to the initial packed file you downloaded!\nThere are still two useful additional things left to do:\nFirst, we can now delete the folder we created by uncompressing your Landsat data because it is no longer needed. So, if you want to save disk space, use the command unlink() to simply delete the folder. By setting the argument recursive=TRUE, all files within the folder will be deleted:\nunlink(productname, recursive=TRUE)\nSecond, it is advisable to create so-called pyramid layers for each Landsat dataset. Pyramid layers are used to improve performance. They are a downsampled version of the original raster and speed up the display of raster data by retrieving only the data at a specified resolution that is required for the display. We can automatically create them by using the Geospatial Data Abstraction Library (GDAL). Initially, GDAL has nothing to do with R, but it can be operated via R using eg. sf and terra sf. Here, we use the function gdal_addo.If you want to use this function, remember to install and load the library sf.\nlibrary(sf)\ngdal_addo(paste0(productname, \".tif\"),  overviews = c(2, 4, 8, 16))\nRun the code and you will create a .ovr-file next to your .tif-file. This ovr-file holds the pyramid information, which will prove useful later in QGIS.\nNote: In my case, after using the command gdal_addo() the object rasterstack is not usable anymore. In that case just reopen the save rasterstack file.\nPhew! This was A LOT of (explanatory) text by now. Fortunately, the code is actually much shorter! Here is the complete code for preprocessing one exemplary L8 Level-2 collection 2 file:\n# Load the 'terra' package for raster data manipulation\nlibrary(terra)\n\n# Load the 'sf' package for handling spatial data\nlibrary(sf)\n\n# Set the working directory to the specified folder path\nsetwd(\"E:/Path/to/your/folder\")\n\n# Define the product filename\nproduct &lt;- \"LC09_L2SP_193023_20250328_20250329_02_T1.tar\"\n\n# Create the product name by removing the file extension (.tar) from the product filename\nproductname &lt;- substr(product, 1, nchar(product) - 4)\n\n# Extract the contents of the tar file into a directory named after the product name\nuntar(product, exdir = productname)\n\n# List all files in the product directory with their full paths\nproductfiles &lt;- list.files(productname, full.names = TRUE)\n\n# Select the specific band files (SR_B1 to SR_B7) from the product files\nbands &lt;- c(grep('SR_B1', productfiles, value=TRUE),\n           grep('SR_B2', productfiles, value=TRUE),\n           grep('SR_B3', productfiles, value=TRUE),\n           grep('SR_B4', productfiles, value=TRUE),\n           grep('SR_B5', productfiles, value=TRUE),\n           grep('SR_B6', productfiles, value=TRUE),\n           grep('SR_B7', productfiles, value=TRUE)\n           ) \n\n# Create a raster stack from the selected band files\nrasterstack &lt;- stack(bands)\n\n# Write the raster stack to a GeoTIFF file, specifying the file type and allowing overwrite\nwriteRaster(rasterstack, \n            paste0(productname, \".tif\"), \n            filetype = \"GTiff\",\n            overwrite = TRUE\n            )\n\n# Remove the product directory and its contents\nunlink(productname, recursive=TRUE)\n\n# Generate overviews (pyramids) for the GeoTIFF file to optimize its performance\ngdal_addo(paste0(productname, \".tif\"),  overviews = c(2, 4, 8, 16))\nLearn how to automate things for many datasets in the next section!\n\n\n\n\n\nImagine you have 50 Landsat 8 records. Of course, it would be very tedious to modify and start the script 50 times. That’s why there is now an automated solution for any number of data products!\nAgain, the prerequisite is that you have the L8 Level-2 datasets downloaded to a folder, in which all of your Landsat 8 scenes are stored – please, no other files! Of course you should replace the file path according to your storage location and create a character variable:\npathToFiles &lt;- \"/media/sf_exchange/landsatdata\"\n\ndir(pathToFiles)\n## [1] \"LC09_L2SP_193023_20250328_20250329_02_T1.tar.gz\"\n## [2] \"LC09_L2SP_193023_20250328_20250329_02_T1.tar.gz\"\nYou can check the files inside pathToFiles with the dir() function. It should list all your Landsat 8 files. If that is not the case, make sure you did not mess up the file path name.\nWe can write all the products that exist in the folder in a vector products using the list.files() function:\nproducts &lt;- list.files(pathToFiles, full.names = TRUE)\n\nproducts\n## [1] \"/media/sf_exchange/landsatdata/LC09_L2SP_193023_20250328_20250329_02_T1.tar.gz\"\n## [2] \"/media/sf_exchange/landsatdata/LC09_L2SP_193023_20250328_20250329_02_T1.tar.gz\"\nTo go through all the steps we saw in the previous section above for each of the scenes in products, we use a for-loop. Conceptually, the for-loop does the following:\nfor (i in products) {\n  print(i)\n  print(\"do all the preprocessing stuff\")\n}\n## [1] \"/media/sf_exchange/landsatdata/LC09_L2SP_193023_20250328_20250329_02_T1.tar.gz\"\n## [1] \"do all the preprocessing stuff\"\n## [1] \"/media/sf_exchange/landsatdata/LC09_L2SP_193023_20250328_20250329_02_T1.tar.gz\"\n## [1] \"do all the preprocessing stuff\"\nFor all landsat scenes i in products, it will do all the preprocessing stuff. The variable, or iterator, i is just a placeholder, which is sequentially occupied with the file names of the Landsat products.\nThus, you can just imagine any filename (as a string) every time you see the iterator i.\nThe remaining steps are then identical to those described above. Here is the complete code. Just adjust your pathToFiles and run it in R-Studio to preprocess all of your Landsat 8 Level-2 files!\nlibrary(terra)\n\npathToFiles &lt;- \"./landsatdata\"\n\nproducts &lt;- list.files(pathToFiles, full.names = TRUE)\n\nfor (i in products) {\n  print( paste0(\"processing: \", i) )\n  \n  productname &lt;- substr(i, 1, nchar(i) - 7) \n\n  untar(i, exdir = productname)\n  \n  productfiles &lt;- list.files(productname, full.names = TRUE)\n  \n  bands &lt;- c(grep('band1', productfiles, value=TRUE),\n             grep('band2', productfiles, value=TRUE),\n             grep('band3', productfiles, value=TRUE),\n             grep('band4', productfiles, value=TRUE),\n             grep('band5', productfiles, value=TRUE),\n             grep('band6', productfiles, value=TRUE),\n             grep('band7', productfiles, value=TRUE)\n            ) \n  \n  writeRaster(stack(bands), \n              paste0(productname, \".tif\"), \n              format = \"GTiff\"\n              )\n  \n  unlink(productname, recursive=TRUE)\n\n  makeOVR &lt;- paste0(\"gdaladdo -ro \", productname, \".tif 2 4 8 16\")\n  system( makeOVR )\n  }\n\n\n\n\nSentinel 2 data is delivered as zip-compressed files in Sentinel’s own SAFE format. The spectral bands are stored as jpg-files in this SAFE container in three different geometric resolutions (10 m, 20 m & 60 m as shown in Section Sentinel 2). We want to stack these jpg-files into a single geotiff-file of an uniform pixelsize of 10 m, i.e., into a so-called raster stack (because it is much easier to work with such a raster stack).\nDue to the size of the data, we need to subset the important data from the SAFE container during preprocessing in order to minimize the computational time and the data volume. Otherwise, a single Sentinel 2 image can quickly grow to 8 GB in size! For this, we will use only desktop app SNAP and its commando-line based counterpart, the Graph Processing Tool (GPT). This results in the following intermediate steps:\n\nresample all bands to 10 m\nspatial and bands subset\nsave image as geotiff to hard drive\n\nWe want to perform the preprocessing step by step on the basis of an Sentinel 2 Level-1 scene in SNAP. Based on that we will develop a graph file that will process an arbitrary number of scenes for you!\n \nPrerequisite\nThe following content requires that you have either successfully downloaded some Sentinel 2 scenes as part of the Download Section Exercise, or that you have acquired some datasets from the ESA SciHUB by your own. If this is not the case, look into the chapter Sentinel / SciHUB!\nHere you can download Sentinel-2 level 2 data (from 31th of July 2018) for execute this exercise.\nDone? – Then start SNAP now!\nPreprocess a single dataset\nIf SNAP is started, you can open the zip file of an image directly by File &gt; Open Product.\n\n\nOpen File SNAP\n\n \nUse SNAP’s toolbar to navigate to Raster &gt; Geometric and open the Resample operation:\n\n\nNavigation to Resample\n\nA window will pop up with two tabs. In the first tab, define a downloaded, zipped Sentinel 2 file as the source product. You do NOT need to unzip it in advance! In the example shown below, the file is located in the exchange folder of our VM.\n\n\nResampling settings tab 1\n\nClick on the second tab “Resampling Parameters”. Select spectral band 2 here to define the geometric resolution of the final product (band 2 has a resolution of 10 m) and press :\n\n\nResampling settings tab 2\n\nThis should create a “virtual file” with the suffix “_resampled”, which is not stored physically on your hard drive. The advantage is that no computationally intensive processes have taken place here and we can continue to calculate with the intermediate product, which should be listed in the Product Explorer in SNAP. You will also get a notification about this. Confirm this with OK:\n\n\nVirtual product notification\n\nClose the Resampling tool.\nWith a right click on the processed image (product) &gt; Open RGB Image Window you can open differnet band combinations:\n\n\nOpen as RGB Image\n\n \nNavigate to the Subset function:\n\n\nSubset function in Toolbar\n\nThe subset function allows you to perform both spatial and spectral resampling. By excluding irrelevant data, you can reduce the volume of data by several orders of magnitude. In the example shown below, we do a spatial subset based on geographic coordinates and only select specific bands:\n\n\n\nSubset function settings\n\nBy pressing OK, another data product with the prefix “subset” is generated within a second and should be visible in the Product Explorer in SNAP. Select the newly created data product in the Product Explorer by a simple left-click on it and navigate through the toolbar to File &gt; Export &gt; GeoTIFF, as shown in the next screenshot. If you notice that your file is larger than 4 GB, you can also choose BIGTIFF as the target file format. BigTIFF describes a GeoTIFF file that is over 4 GB in size. However, saving a BiTIFF file in SNAP is very slow, especially for machines with only 8 GB of RAM. Therefore try to use the GeoTIFF file format first:\n\n\nExport function\n\nIn the window of the export function you can then define the file path and the name of the exported file. Then press Export Product to start the processing:\n\n\nExport function settings\n\nThe processing will take some time from now on, based on the spatial subset and the number of bands you selected.\nHere you can watch all presented pre-processing steps of Sentinel-2 data:\n\n\n\nWatch this video on YouTube\n\n\nPre-Processing steps in RStudio:\nWe will now use RStudio to pre-process our Sentinel 2 data.The processing steps will be as follows:\n\nResample bands to a spatial resolution of 10m.\nStack the resampled bands.\nCreate a subset of choice.\n\nOpen R-Studio and install, open the required packages and set your working directory:\ninstall.packages(terra)\n\nlibrary(terra)\n\nsetwd(\"E:\\\\Work\\\\S2_15_05_2024\")\nDefine the folder containing the Sentinel 2 data and list the .jp2 file (Raster Data).\n#Folder containing .jp2 files\nimage_dir &lt;- \"E:\\\\Work\\\\S2_15_05_2024\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\GRANULE\\\\L1C_T32UQD_A037557_20240515T101556\\\\IMG_DATA\"\n\n# list all .jp2 files\njp2_files &lt;- list.files(image_dir, pattern = \"\\\\.jp2$\", full.names = TRUE)\nWe now use band 2 (Blue band), to serve as a reference band to resample all other bands to a spatial resolution of 10m.\nref_band_file &lt;- \"E:\\\\Work\\\\S2_15_05_2024\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\GRANULE\\\\L1C_T32UQD_A037557_20240515T101556\\\\IMG_DATA\\\\T32UQD_20240515T101559_B02.jp2\"\nref_band &lt;- rast(ref_band_file)\nGenerally any band with a 10m resolution in Sentinel 2, such as Blue(B02), Green(B03), Red(B04), NIR|(B08) can be used, however in this case Blue band is used due to its shorter wavelength and a high sensitivity to atmospheric conditions.\nCreate an output folder for the resampled images.\noutput_dir &lt;- \"E:\\\\Work\\\\S2_15_05_2024\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\GRANULE\\\\L1C_T32UQD_A037557_20240515T101556\\\\Resampled\"\ndir.create(output_dir, showWarnings = FALSE)\nInsert the function for Bilinear Interpolation : is a resampling method that uses the distance-weighted average of the four nearest pixel values to estimate a new pixel value. This applies to each raster file.\nresample_raster &lt;- function(file, ref) {\n  r &lt;- rast(file)\n  r_resampled &lt;- resample(r, ref, method = \"bilinear\")  # Bilinear for continuous data\n  return(r_resampled)\n}\nApplying resampling to all bands.\n#Apply resampling to all bands\nfor (file in jp2_files) {\n  band_name &lt;- basename(file)  # Extract filename\n  output_file &lt;- file.path(output_dir, gsub(\".jp2$\", \"_resampled.tif\", band_name))  # Rename output file\n  \n  #Resample\n  r_resampled &lt;- resample_raster(file, ref_band)\n  \n  #Save the resampled raster\n  writeRaster(r_resampled, output_file, overwrite = TRUE)\n  \n  \n  \n  cat(\"Resampled:\", band_name, \"-&gt;\", output_file, \"\\n\")\n}\n\ncat(\"\\n✅ All Sentinel-2 bands have been resampled to 10m and saved in:\", output_dir, \"\\n\")\nStacking the resampled data and plot an image in True Color Composite (RGB). This displayed the whole stacked image.\n#Define the directory containing the .tif files\nimage_dir &lt;- \"E:\\\\Work\\\\S2_15_05_2024\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\GRANULE\\\\L1C_T32UQD_A037557_20240515T101556\\\\Resampled\"  # Replace with your folder path\n\n# List all .tif files in the directory\ntif_files &lt;- list.files(image_dir, pattern = \"\\\\.tif$\", full.names = TRUE)\n\n#stack the .tif files\nstacked_raster &lt;- rast(tif_files)\n\n#Save .tif file\noutput_tif &lt;- \"E:\\\\Work\\\\S2_15_05_2024\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\GRANULE\\\\L1C_T32UQD_A037557_20240515T101556\\\\Stacked_Resampled.tif\"\nwriteRaster(stacked_raster, output_tif, overwrite = TRUE)\n\nplotRGB(stacked_raster, r = 3, g = 2, b = 1, stretch = \"lin\")  ##To display full extent of the stacked image\nCreate a subset of your choice by defining the extent with which you want to work with, and save the image.\n# Load stacked raster\nstacked_raster &lt;- rast(\"E:\\\\Work\\\\S2_15_05_2024\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\GRANULE\\\\L1C_T32UQD_A037557_20240515T101556\\\\Stacked_Resampled.tif\")\n\n#Define the boundary (xmin, xmax, ymin, ymax)\nsubset_extent &lt;- ext(c(500000, 510000, 4500000, 4510000))  # Modify with actual coordinates\n\nsubset_raster &lt;- crop(stacked_raster, subset_extent)\n\noutput_subset_tif &lt;- \"E:\\\\Work\\\\S2_15_05_2024\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\GRANULE\\\\L1C_T32UQD_A037557_20240515T101556\\\\Subset_Stacked_Resampled.tif\"\nwriteRaster(subset_raster, output_subset_tif, overwrite = TRUE)\n\nplotRGB(subset_raster, r = 3, g = 2, b = 1, stretch = \"lin\")  ##To display cropped/selected area\n\n\nIf there are clouds or cloud shadows on your sentinel-2 scene, they can be mask out using the quality scene classification band of your scene.\n\n\n\nSentinel-2 scene (true color) with clouds and cloud shadows\n\nImage quality band and the classes we want to delete for our mask (Values): 3 (cloud shadows), 7 (unclassified), 8 (cloud medium probability), 9 (cloud high probability), 10 (thin cirrus) and 11 (snow or ice). For other scenes, you have to adjust the classes if necessary.\n\n\nSentinel-2 scene with quality scene classification\n\nHere you can download a Sentinel-2 image for executing this pre-processing step. For the scene which we used in the pre-processing before is no cloud correction necessary.\nProcessing steps in RStudio:\nOpen R-Studio and install, open the required packages and set your working directory:\ninstall.packages(\"terra\")\n\nlibrary(terra)\n\nsetwd(\"D:\\\\elearning\\\\exchange\\\\R\")\nOpen and plot the image:\nsen2 &lt;- rast(\"E:\\\\subset_0_of_S2A_MSIL2A_20190626T102031_N0212_R065_T32UQD_20190626T125319_resampled.tif\")\nplot(sen2)\nplotRGB(sen2, r = 4, g = 3, b = 2, stretch = \"lin\")\nSeparate spectral bands and classification (band 1 to 12 is the mulispectral Sentinel- 2 scene, band 13 is the quality classification band):\nsen2_bands &lt;- sen2[[-13]]\nsen2_mask &lt;- sen2[[13]]\nplot(sen2_mask)\nWhich pixels do we want to mask?\nplot(sen2_mask)\nsen2_mask_combi &lt;- sen2_mask\nsen2_mask_combi[sen2_mask == 3 | sen2_mask == 7 | sen2_mask == 8 | \n                  sen2_mask == 9 | sen2_mask == 10 | sen2_mask == 11] &lt;- NA\n\nplot(sen2_mask_combi)\nwriteRaster(sen2_mask_combi, \"sen2_mask.tif\", overwrite = TRUE)\n\n\nMask without the classes: 3,7,8,9,10,11\n\n\nApply mask:\nsen2_bands_masked &lt;- mask(sen2_bands, sen2_mask_combi)\nplotRGB(sen2_bands_masked, r = 4, g = 3, b = 2, stretch = \"lin\")\nwriteRaster(sen2_bands_masked, \"sen2_masked.tif\", overwrite = TRUE)\n\n\nApplied mask\n\n\n…or the whole part in short:\nsen2_bands_masked_a &lt;- sen2_bands\nsen2_bands_masked_a[sen2_mask == 3 | sen2_mask == 7 | sen2_mask == 8 | \n                      sen2_mask == 9 | sen2_mask == 10 | sen2_mask == 11] &lt;- NA\nwriteRaster(sen2_bands_masked_a, \"sen2_masked_alternativ.tif\", overwrite = TRUE)",
    "crumbs": [
      "RESEDA",
      "Preprocess",
      "Preprocess Optical Data"
    ]
  },
  {
    "objectID": "RESEDA/contents/Preprocess-Optical-Data.html#landsat-8-preprocessing",
    "href": "RESEDA/contents/Preprocess-Optical-Data.html#landsat-8-preprocessing",
    "title": "Preprocess Optical Data",
    "section": "",
    "text": "Landsat 8 as well as Landsat 9 ship as a tar-archived file with the spectral bands as individual georeferenced tif images. We want to stack these images into a single geotiff-file, i.e., into a so-called raster stack. Afterwards, it is much easier to work with the darta. While this could also be done in QGIS, we will use R for this preprocessing, because it is easier to automate things this way. This results in the following intermediate steps we have to check off:\n\nunzip your downloaded L9 files\nput together the spectral band files of your choice into a rasterstack\nsave this rasterstack to your hard drive\n(optional) delete all redundant data\n(optional) create pyramid layers for a better visualization in QGIS\n\nWe will practice everything exemplary on the basis of a single L9 Level-2 data set. There will be an exhaustive explanation for each line of code. Based on that we will develop a script that will automatically do everything for you in the future.\n\n\nPrerequisite\nThe following content requires that you have either successfully downloaded some Landsat 8 or 9 scenes as part of the Download Section Exercise, or that you have acquired some datasets from the USGS EarthExplorer by your own. If this is not the case, look into the chapter Download - Earthexplorer!\nDone? – Then start R-Studio now!\n\n\nPreprocess a single dataset\nWe recommend writing the following code in the script window of R-Studio and executing it from there (see chapter R-Studio).The example below assumes that you have one or more Landsat 8 or 9 scenes in a “landsatdata”-folder.\nWe will use the terra library to write a raster file later. Additional libraries should always be loaded first, using the function library():\nlibrary(terra)\n## Loading required package: sp\nA few libraries make use of other libraries. So does the terra library with the sp-package, which will be loaded automatically.\nSet the path to your working directory. Please, be aware that the delimiters between subfolders should be either “/” or “\\”. Especially, if you working on Windows machines, take care of this. Use the function getwd() to see which directory is set as your working directory.\nsetwd(\"E:P/ath/to/your/folder\")\n\ngetwd()\nThen define the Landsat 9 product of your choice with its entire file path and save it as the string variable product:\nproduct &lt;- \"LC09_L2SP_193023_20250328_20250329_02_T1.tar\"\n\nfile.exists(product)\n## [1] TRUE\nNOTE: this is just an example – you have to change the file and its path according to your own settings!\nYou can use the function file.exists() to check whether the file can be found on your system or not. If it returns FALSE, make sure you did not mess up the file name.\nWe want to unpack the file into a folder with the same name as the file. The character variable product already contains the full name of the Landsat scene. We just have to get rid of the suffix “.tar”. Therefore we can use the function substr() to delete the last four characters (=“.tar.gz”) of the string:\nproductname &lt;- substr(product, 1, nchar(product) - 4)\n\nproductname\n## [1] \"LC09_L2SP_193023_20250328_20250329_02_T1\"\nThe substring function substr() accepts three arguments here: a string (the entire file path with the suffix), and two integer values – one for a start (“1” = start with the first character) and one for a stop position within the given string. In order to define the stop position, we need to count the number of all characters of the file path via nchar(product) (which is 44 in our case) and substract 4.\nUnzip the Landsat product using the untar() method and define the directory to which all data will be extracted by setting the argument exdir = productname:\nuntar(product, exdir = productname)\nYou should notice a new folder being created next to your initial data product. This could take a short moment. During processing, a red exclamation point can be seen in the upper right corner of the console window. NOTE: If this step fails, your data is corrupt. In this case, you will need to download the file again because something went wrong during data transfer.\nAfter unpacking is complete, we want to have a look in the newly extracted folder and save the file names of all files in this folder into a vector productfiles using the function list.files. By providing the argument full.names = TRUE, the full paths are returned for each file:\nproductfiles &lt;- list.files(productname, full.names = TRUE)\n\nproductfiles\n## [1] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ANG.txt\"          \n## [2] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_MTL.txt\"          \n## [3] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_MTL.xml\"          \n## [4] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_QA_PIXEL.TIF\"     \n## [5] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_QA_RADSAT.TIF\"    \n## [6] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B1.TIF\"        \n## [7] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B2.TIF\"        \n## [8] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B3.TIF\"        \n## [9] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B4.TIF\"        \n##[10] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B5.TIF\"        \n##[11] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B6.TIF\"        \n##[12] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B7.TIF\"        \n##[13] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_QA_AEROSOL.TIF\"\n##[14] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_ATRAN.TIF\"     \n##[15] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_B10.TIF\"       \n##[16] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_CDIST.TIF\"     \n##[17] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_DRAD.TIF\"      \n##[18] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_EMIS.TIF\"      \n##[19] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_EMSD.TIF\"      \n##[20] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_QA.TIF\"        \n##[21] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_TRAD.TIF\"      \n##[22] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_ST_URAD.TIF\"\nThese are all 22 files that come with a Level-2 product including bands regarding thermal observations (see Landsat 8 and Landsat-9 for more information). As a double check, you can have a look at the identical files in your file explorer:\n\nNow we select all the spectral bands that we want to put into our new data stack. Have a deeper look at the files in productfiles. The files are named after the corresponding spectral channels at the end of the file name, e.g., “SR_B1”, “SR_B2”, and so on. We use these terms to extract the bands by utilizing the grep() function. The following code looks a little bloated. Maybe it is. But it gives you the freedom to exclude individual bands that you may not need. NOTE: This is an example of level 2 data. Landsat Level-1 scenes potentially have 11 channels. In the case of Level 1 data, you could easily enter the remaining lines here.\nbands &lt;- c(grep('SR_B1', productfiles, value=TRUE),\n           grep('SR_B2', productfiles, value=TRUE),\n           grep('SR_B3', productfiles, value=TRUE),\n           grep('SR_B4', productfiles, value=TRUE),\n           grep('SR_B5', productfiles, value=TRUE),\n           grep('SR_B6', productfiles, value=TRUE),\n           grep('SR_B7', productfiles, value=TRUE)\n           ) \n\nbands\n## [1] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B1.TIF\"\n## [2] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B2.TIF\"\n## [3] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B3.TIF\"\n## [4] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B4.TIF\"\n## [5] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B5.TIF\"\n## [6] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B6.TIF\"\n## [7] \"LC09_L2SP_193023_20250328_20250329_02_T1/LC09_L2SP_193023_20250328_20250329_02_T1_SR_B7.TIF\"\nThe grep() function searches for the string, which is given as the first argument, e.g., ‘SR_B1’, in the vector of strings (2nd argument). Normally, when the function finds a match, it only returns the position of this find in the vector productfiles. By setting the argument value=TRUE we can write out the content of the vector at the respective position instead. By doing so, we create an vector bands containing all file paths via the standard combine-function c().\nNow we can create a raster stack based on the vector of bands in the variable bands. In order to do so, we use the function rast(), which is part of the terra-library we loaded in the beginning! This raster stack function creates a raster stack object based on a list of filenames. You will learn more about the beauty of raster stacks in the chapter Visualization.\nrasterstack &lt;- rast(bands)\nWe can save this raster stack and store it on your hard drive via writeRaster:\nwriteRaster(rasterstack, \n            paste0(productname, \".tif\"), \n            filetype = \"GTiff\",\n            overwrite = TRUE\n            )\nThe writerRaster fuction is a powerful tool, which is also provided by the terra package. In line 1 we set our raster stack object as the first argument. In line 2 we define the output name of the new file we will create. To do this, just add the suffix “.tif” to our filename using the handy function paste0, which just put all the strings together to one string. Line 3 explicitly defines the output format “GTiff”. How should I know this string “GTiff”, you ask? These strings are fixed by the function and are also listed for other data formats in the documentations. The fourth argument in line 4 only gives the authority to overwrite existing files.\nDONE! A new file containing all spectral bands is now written directly next to the initial packed file you downloaded!\nThere are still two useful additional things left to do:\nFirst, we can now delete the folder we created by uncompressing your Landsat data because it is no longer needed. So, if you want to save disk space, use the command unlink() to simply delete the folder. By setting the argument recursive=TRUE, all files within the folder will be deleted:\nunlink(productname, recursive=TRUE)\nSecond, it is advisable to create so-called pyramid layers for each Landsat dataset. Pyramid layers are used to improve performance. They are a downsampled version of the original raster and speed up the display of raster data by retrieving only the data at a specified resolution that is required for the display. We can automatically create them by using the Geospatial Data Abstraction Library (GDAL). Initially, GDAL has nothing to do with R, but it can be operated via R using eg. sf and terra sf. Here, we use the function gdal_addo.If you want to use this function, remember to install and load the library sf.\nlibrary(sf)\ngdal_addo(paste0(productname, \".tif\"),  overviews = c(2, 4, 8, 16))\nRun the code and you will create a .ovr-file next to your .tif-file. This ovr-file holds the pyramid information, which will prove useful later in QGIS.\nNote: In my case, after using the command gdal_addo() the object rasterstack is not usable anymore. In that case just reopen the save rasterstack file.\nPhew! This was A LOT of (explanatory) text by now. Fortunately, the code is actually much shorter! Here is the complete code for preprocessing one exemplary L8 Level-2 collection 2 file:\n# Load the 'terra' package for raster data manipulation\nlibrary(terra)\n\n# Load the 'sf' package for handling spatial data\nlibrary(sf)\n\n# Set the working directory to the specified folder path\nsetwd(\"E:/Path/to/your/folder\")\n\n# Define the product filename\nproduct &lt;- \"LC09_L2SP_193023_20250328_20250329_02_T1.tar\"\n\n# Create the product name by removing the file extension (.tar) from the product filename\nproductname &lt;- substr(product, 1, nchar(product) - 4)\n\n# Extract the contents of the tar file into a directory named after the product name\nuntar(product, exdir = productname)\n\n# List all files in the product directory with their full paths\nproductfiles &lt;- list.files(productname, full.names = TRUE)\n\n# Select the specific band files (SR_B1 to SR_B7) from the product files\nbands &lt;- c(grep('SR_B1', productfiles, value=TRUE),\n           grep('SR_B2', productfiles, value=TRUE),\n           grep('SR_B3', productfiles, value=TRUE),\n           grep('SR_B4', productfiles, value=TRUE),\n           grep('SR_B5', productfiles, value=TRUE),\n           grep('SR_B6', productfiles, value=TRUE),\n           grep('SR_B7', productfiles, value=TRUE)\n           ) \n\n# Create a raster stack from the selected band files\nrasterstack &lt;- stack(bands)\n\n# Write the raster stack to a GeoTIFF file, specifying the file type and allowing overwrite\nwriteRaster(rasterstack, \n            paste0(productname, \".tif\"), \n            filetype = \"GTiff\",\n            overwrite = TRUE\n            )\n\n# Remove the product directory and its contents\nunlink(productname, recursive=TRUE)\n\n# Generate overviews (pyramids) for the GeoTIFF file to optimize its performance\ngdal_addo(paste0(productname, \".tif\"),  overviews = c(2, 4, 8, 16))\nLearn how to automate things for many datasets in the next section!\n\n\n\n\n\nImagine you have 50 Landsat 8 records. Of course, it would be very tedious to modify and start the script 50 times. That’s why there is now an automated solution for any number of data products!\nAgain, the prerequisite is that you have the L8 Level-2 datasets downloaded to a folder, in which all of your Landsat 8 scenes are stored – please, no other files! Of course you should replace the file path according to your storage location and create a character variable:\npathToFiles &lt;- \"/media/sf_exchange/landsatdata\"\n\ndir(pathToFiles)\n## [1] \"LC09_L2SP_193023_20250328_20250329_02_T1.tar.gz\"\n## [2] \"LC09_L2SP_193023_20250328_20250329_02_T1.tar.gz\"\nYou can check the files inside pathToFiles with the dir() function. It should list all your Landsat 8 files. If that is not the case, make sure you did not mess up the file path name.\nWe can write all the products that exist in the folder in a vector products using the list.files() function:\nproducts &lt;- list.files(pathToFiles, full.names = TRUE)\n\nproducts\n## [1] \"/media/sf_exchange/landsatdata/LC09_L2SP_193023_20250328_20250329_02_T1.tar.gz\"\n## [2] \"/media/sf_exchange/landsatdata/LC09_L2SP_193023_20250328_20250329_02_T1.tar.gz\"\nTo go through all the steps we saw in the previous section above for each of the scenes in products, we use a for-loop. Conceptually, the for-loop does the following:\nfor (i in products) {\n  print(i)\n  print(\"do all the preprocessing stuff\")\n}\n## [1] \"/media/sf_exchange/landsatdata/LC09_L2SP_193023_20250328_20250329_02_T1.tar.gz\"\n## [1] \"do all the preprocessing stuff\"\n## [1] \"/media/sf_exchange/landsatdata/LC09_L2SP_193023_20250328_20250329_02_T1.tar.gz\"\n## [1] \"do all the preprocessing stuff\"\nFor all landsat scenes i in products, it will do all the preprocessing stuff. The variable, or iterator, i is just a placeholder, which is sequentially occupied with the file names of the Landsat products.\nThus, you can just imagine any filename (as a string) every time you see the iterator i.\nThe remaining steps are then identical to those described above. Here is the complete code. Just adjust your pathToFiles and run it in R-Studio to preprocess all of your Landsat 8 Level-2 files!\nlibrary(terra)\n\npathToFiles &lt;- \"./landsatdata\"\n\nproducts &lt;- list.files(pathToFiles, full.names = TRUE)\n\nfor (i in products) {\n  print( paste0(\"processing: \", i) )\n  \n  productname &lt;- substr(i, 1, nchar(i) - 7) \n\n  untar(i, exdir = productname)\n  \n  productfiles &lt;- list.files(productname, full.names = TRUE)\n  \n  bands &lt;- c(grep('band1', productfiles, value=TRUE),\n             grep('band2', productfiles, value=TRUE),\n             grep('band3', productfiles, value=TRUE),\n             grep('band4', productfiles, value=TRUE),\n             grep('band5', productfiles, value=TRUE),\n             grep('band6', productfiles, value=TRUE),\n             grep('band7', productfiles, value=TRUE)\n            ) \n  \n  writeRaster(stack(bands), \n              paste0(productname, \".tif\"), \n              format = \"GTiff\"\n              )\n  \n  unlink(productname, recursive=TRUE)\n\n  makeOVR &lt;- paste0(\"gdaladdo -ro \", productname, \".tif 2 4 8 16\")\n  system( makeOVR )\n  }",
    "crumbs": [
      "RESEDA",
      "Preprocess",
      "Preprocess Optical Data"
    ]
  },
  {
    "objectID": "RESEDA/contents/Preprocess-Optical-Data.html#sentinel-2-preprocessing",
    "href": "RESEDA/contents/Preprocess-Optical-Data.html#sentinel-2-preprocessing",
    "title": "Preprocess Optical Data",
    "section": "",
    "text": "Sentinel 2 data is delivered as zip-compressed files in Sentinel’s own SAFE format. The spectral bands are stored as jpg-files in this SAFE container in three different geometric resolutions (10 m, 20 m & 60 m as shown in Section Sentinel 2). We want to stack these jpg-files into a single geotiff-file of an uniform pixelsize of 10 m, i.e., into a so-called raster stack (because it is much easier to work with such a raster stack).\nDue to the size of the data, we need to subset the important data from the SAFE container during preprocessing in order to minimize the computational time and the data volume. Otherwise, a single Sentinel 2 image can quickly grow to 8 GB in size! For this, we will use only desktop app SNAP and its commando-line based counterpart, the Graph Processing Tool (GPT). This results in the following intermediate steps:\n\nresample all bands to 10 m\nspatial and bands subset\nsave image as geotiff to hard drive\n\nWe want to perform the preprocessing step by step on the basis of an Sentinel 2 Level-1 scene in SNAP. Based on that we will develop a graph file that will process an arbitrary number of scenes for you!\n \nPrerequisite\nThe following content requires that you have either successfully downloaded some Sentinel 2 scenes as part of the Download Section Exercise, or that you have acquired some datasets from the ESA SciHUB by your own. If this is not the case, look into the chapter Sentinel / SciHUB!\nHere you can download Sentinel-2 level 2 data (from 31th of July 2018) for execute this exercise.\nDone? – Then start SNAP now!\nPreprocess a single dataset\nIf SNAP is started, you can open the zip file of an image directly by File &gt; Open Product.\n\n\nOpen File SNAP\n\n \nUse SNAP’s toolbar to navigate to Raster &gt; Geometric and open the Resample operation:\n\n\nNavigation to Resample\n\nA window will pop up with two tabs. In the first tab, define a downloaded, zipped Sentinel 2 file as the source product. You do NOT need to unzip it in advance! In the example shown below, the file is located in the exchange folder of our VM.\n\n\nResampling settings tab 1\n\nClick on the second tab “Resampling Parameters”. Select spectral band 2 here to define the geometric resolution of the final product (band 2 has a resolution of 10 m) and press :\n\n\nResampling settings tab 2\n\nThis should create a “virtual file” with the suffix “_resampled”, which is not stored physically on your hard drive. The advantage is that no computationally intensive processes have taken place here and we can continue to calculate with the intermediate product, which should be listed in the Product Explorer in SNAP. You will also get a notification about this. Confirm this with OK:\n\n\nVirtual product notification\n\nClose the Resampling tool.\nWith a right click on the processed image (product) &gt; Open RGB Image Window you can open differnet band combinations:\n\n\nOpen as RGB Image\n\n \nNavigate to the Subset function:\n\n\nSubset function in Toolbar\n\nThe subset function allows you to perform both spatial and spectral resampling. By excluding irrelevant data, you can reduce the volume of data by several orders of magnitude. In the example shown below, we do a spatial subset based on geographic coordinates and only select specific bands:\n\n\n\nSubset function settings\n\nBy pressing OK, another data product with the prefix “subset” is generated within a second and should be visible in the Product Explorer in SNAP. Select the newly created data product in the Product Explorer by a simple left-click on it and navigate through the toolbar to File &gt; Export &gt; GeoTIFF, as shown in the next screenshot. If you notice that your file is larger than 4 GB, you can also choose BIGTIFF as the target file format. BigTIFF describes a GeoTIFF file that is over 4 GB in size. However, saving a BiTIFF file in SNAP is very slow, especially for machines with only 8 GB of RAM. Therefore try to use the GeoTIFF file format first:\n\n\nExport function\n\nIn the window of the export function you can then define the file path and the name of the exported file. Then press Export Product to start the processing:\n\n\nExport function settings\n\nThe processing will take some time from now on, based on the spatial subset and the number of bands you selected.\nHere you can watch all presented pre-processing steps of Sentinel-2 data:\n\n\n\nWatch this video on YouTube\n\n\nPre-Processing steps in RStudio:\nWe will now use RStudio to pre-process our Sentinel 2 data.The processing steps will be as follows:\n\nResample bands to a spatial resolution of 10m.\nStack the resampled bands.\nCreate a subset of choice.\n\nOpen R-Studio and install, open the required packages and set your working directory:\ninstall.packages(terra)\n\nlibrary(terra)\n\nsetwd(\"E:\\\\Work\\\\S2_15_05_2024\")\nDefine the folder containing the Sentinel 2 data and list the .jp2 file (Raster Data).\n#Folder containing .jp2 files\nimage_dir &lt;- \"E:\\\\Work\\\\S2_15_05_2024\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\GRANULE\\\\L1C_T32UQD_A037557_20240515T101556\\\\IMG_DATA\"\n\n# list all .jp2 files\njp2_files &lt;- list.files(image_dir, pattern = \"\\\\.jp2$\", full.names = TRUE)\nWe now use band 2 (Blue band), to serve as a reference band to resample all other bands to a spatial resolution of 10m.\nref_band_file &lt;- \"E:\\\\Work\\\\S2_15_05_2024\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\GRANULE\\\\L1C_T32UQD_A037557_20240515T101556\\\\IMG_DATA\\\\T32UQD_20240515T101559_B02.jp2\"\nref_band &lt;- rast(ref_band_file)\nGenerally any band with a 10m resolution in Sentinel 2, such as Blue(B02), Green(B03), Red(B04), NIR|(B08) can be used, however in this case Blue band is used due to its shorter wavelength and a high sensitivity to atmospheric conditions.\nCreate an output folder for the resampled images.\noutput_dir &lt;- \"E:\\\\Work\\\\S2_15_05_2024\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\GRANULE\\\\L1C_T32UQD_A037557_20240515T101556\\\\Resampled\"\ndir.create(output_dir, showWarnings = FALSE)\nInsert the function for Bilinear Interpolation : is a resampling method that uses the distance-weighted average of the four nearest pixel values to estimate a new pixel value. This applies to each raster file.\nresample_raster &lt;- function(file, ref) {\n  r &lt;- rast(file)\n  r_resampled &lt;- resample(r, ref, method = \"bilinear\")  # Bilinear for continuous data\n  return(r_resampled)\n}\nApplying resampling to all bands.\n#Apply resampling to all bands\nfor (file in jp2_files) {\n  band_name &lt;- basename(file)  # Extract filename\n  output_file &lt;- file.path(output_dir, gsub(\".jp2$\", \"_resampled.tif\", band_name))  # Rename output file\n  \n  #Resample\n  r_resampled &lt;- resample_raster(file, ref_band)\n  \n  #Save the resampled raster\n  writeRaster(r_resampled, output_file, overwrite = TRUE)\n  \n  \n  \n  cat(\"Resampled:\", band_name, \"-&gt;\", output_file, \"\\n\")\n}\n\ncat(\"\\n✅ All Sentinel-2 bands have been resampled to 10m and saved in:\", output_dir, \"\\n\")\nStacking the resampled data and plot an image in True Color Composite (RGB). This displayed the whole stacked image.\n#Define the directory containing the .tif files\nimage_dir &lt;- \"E:\\\\Work\\\\S2_15_05_2024\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\GRANULE\\\\L1C_T32UQD_A037557_20240515T101556\\\\Resampled\"  # Replace with your folder path\n\n# List all .tif files in the directory\ntif_files &lt;- list.files(image_dir, pattern = \"\\\\.tif$\", full.names = TRUE)\n\n#stack the .tif files\nstacked_raster &lt;- rast(tif_files)\n\n#Save .tif file\noutput_tif &lt;- \"E:\\\\Work\\\\S2_15_05_2024\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\GRANULE\\\\L1C_T32UQD_A037557_20240515T101556\\\\Stacked_Resampled.tif\"\nwriteRaster(stacked_raster, output_tif, overwrite = TRUE)\n\nplotRGB(stacked_raster, r = 3, g = 2, b = 1, stretch = \"lin\")  ##To display full extent of the stacked image\nCreate a subset of your choice by defining the extent with which you want to work with, and save the image.\n# Load stacked raster\nstacked_raster &lt;- rast(\"E:\\\\Work\\\\S2_15_05_2024\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\GRANULE\\\\L1C_T32UQD_A037557_20240515T101556\\\\Stacked_Resampled.tif\")\n\n#Define the boundary (xmin, xmax, ymin, ymax)\nsubset_extent &lt;- ext(c(500000, 510000, 4500000, 4510000))  # Modify with actual coordinates\n\nsubset_raster &lt;- crop(stacked_raster, subset_extent)\n\noutput_subset_tif &lt;- \"E:\\\\Work\\\\S2_15_05_2024\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\S2B_MSIL1C_20240515T101559_N0510_R065_T32UQD_20240515T140423.SAFE\\\\GRANULE\\\\L1C_T32UQD_A037557_20240515T101556\\\\Subset_Stacked_Resampled.tif\"\nwriteRaster(subset_raster, output_subset_tif, overwrite = TRUE)\n\nplotRGB(subset_raster, r = 3, g = 2, b = 1, stretch = \"lin\")  ##To display cropped/selected area\n\n\nIf there are clouds or cloud shadows on your sentinel-2 scene, they can be mask out using the quality scene classification band of your scene.\n\n\n\nSentinel-2 scene (true color) with clouds and cloud shadows\n\nImage quality band and the classes we want to delete for our mask (Values): 3 (cloud shadows), 7 (unclassified), 8 (cloud medium probability), 9 (cloud high probability), 10 (thin cirrus) and 11 (snow or ice). For other scenes, you have to adjust the classes if necessary.\n\n\nSentinel-2 scene with quality scene classification\n\nHere you can download a Sentinel-2 image for executing this pre-processing step. For the scene which we used in the pre-processing before is no cloud correction necessary.\nProcessing steps in RStudio:\nOpen R-Studio and install, open the required packages and set your working directory:\ninstall.packages(\"terra\")\n\nlibrary(terra)\n\nsetwd(\"D:\\\\elearning\\\\exchange\\\\R\")\nOpen and plot the image:\nsen2 &lt;- rast(\"E:\\\\subset_0_of_S2A_MSIL2A_20190626T102031_N0212_R065_T32UQD_20190626T125319_resampled.tif\")\nplot(sen2)\nplotRGB(sen2, r = 4, g = 3, b = 2, stretch = \"lin\")\nSeparate spectral bands and classification (band 1 to 12 is the mulispectral Sentinel- 2 scene, band 13 is the quality classification band):\nsen2_bands &lt;- sen2[[-13]]\nsen2_mask &lt;- sen2[[13]]\nplot(sen2_mask)\nWhich pixels do we want to mask?\nplot(sen2_mask)\nsen2_mask_combi &lt;- sen2_mask\nsen2_mask_combi[sen2_mask == 3 | sen2_mask == 7 | sen2_mask == 8 | \n                  sen2_mask == 9 | sen2_mask == 10 | sen2_mask == 11] &lt;- NA\n\nplot(sen2_mask_combi)\nwriteRaster(sen2_mask_combi, \"sen2_mask.tif\", overwrite = TRUE)\n\n\nMask without the classes: 3,7,8,9,10,11\n\n\nApply mask:\nsen2_bands_masked &lt;- mask(sen2_bands, sen2_mask_combi)\nplotRGB(sen2_bands_masked, r = 4, g = 3, b = 2, stretch = \"lin\")\nwriteRaster(sen2_bands_masked, \"sen2_masked.tif\", overwrite = TRUE)\n\n\nApplied mask\n\n\n…or the whole part in short:\nsen2_bands_masked_a &lt;- sen2_bands\nsen2_bands_masked_a[sen2_mask == 3 | sen2_mask == 7 | sen2_mask == 8 | \n                      sen2_mask == 9 | sen2_mask == 10 | sen2_mask == 11] &lt;- NA\nwriteRaster(sen2_bands_masked_a, \"sen2_masked_alternativ.tif\", overwrite = TRUE)",
    "crumbs": [
      "RESEDA",
      "Preprocess",
      "Preprocess Optical Data"
    ]
  },
  {
    "objectID": "RESEDA/contents/Acquireyourdata.html",
    "href": "RESEDA/contents/Acquireyourdata.html",
    "title": "Acquire your data",
    "section": "",
    "text": "First of all you need two things: On the one hand, a problem that you want to answer with remote sensing tools and, on the other hand, data with which you can accomplish this.\nIn this chapter, you will get to know two optical (Landsat 8 & Landsat 9 & Sentinel 2) and one SAR satellites (Sentinel 1) in more detail. At the end of the chapter, you will know where and how to acquire individual or immense amounts of data.\n\n\nIn this chapter, the following content awaits you:\nIntro\n- get an overview of remote sensing applications and various satellite missions\nSensor Basics\n- repeat basic concepts of remote sensing imaging\n- deepen your knowledge about the four resolutions in RS\nLandsat Earthexplorer & BIG DATA Download\n- use USGS EarthExplorer for browsing and downloading Landsat imagery\n- learn how to download an impressive amount of Landsat data\nSentinel SciHUB & BIG DATA Download\n- use ESA SciHUB for browsing and downloading Sentinel 1 and 2 imagery\n- learn how to download an impressive amount of Sentinel data\n\n\n\nIn general, satellite remote sensing has enabled major advances in understanding global climate systems and its changes. Thus, most of the research questions are aimed at better understanding and quantifying climate change to some extent. The results are regularly compiled in the IPCC reports.\nHowever, within this tutorial we focus on the analysis of land cover and land use changes (LULC), which includes all subsystems of the earth and allows a wide range of questions.\n\nAgriculture\ncrop type classification, crop condition assessment, crop yield estimation & forecasting, precision farming, irrigation management\n\nSoil & Geology\nmapping of soil types, soil moisture, mapping of soil management practices, groundwater discharge, permafrost carbon storage, extracting mineral deposits, lithological and structural mapping, detect oil reserves, sediment transport modelling, dinosaur tracks\n\nForest & Vegetation\nspecies classification, deforestation processes, tree crown delineation, biomass, stress monitoring (infestations), carbon storage assessment, estimating forest supplies\n\nOceans & Water Bodies\nsea surface temperature, sea current monitoring, wave height, mean sea level delineation, water salinity, algae growth/ red tides, coral reefs, surface wind speed & direction, oil slicks, fishing activities, watershed delineation, wetland preserving\n\nUrban\nurbanization/ population growth, urban heat island, local climate zones, urban structure types, automatic road network delineation, solar panel energy optimization, monitor traffic jams, night time activity, locate construction alteration, ancient archaeological sites\n\nFire & Disasters\nactive fires, burned areas, fire severity, coal mine fires, volcanic ash monitoring, post earthquake or floods damage mapping, assessment of droughts, landslides, dust storms, tsunamis\n\nIce & Glacier\nmonitoring ice sheets and glaciers, glacier melting, snow melt runoff, carbon content assessement, ship tracking & routing\n\n\n\nThere is a tremendous amount of free data products (FREE) provided by various remote sensing missions. We will take a closer look at three of them in the upcoming sections (Landsat 8-9, Sentinel 2 and Sentinel 1).\nThen again, companies, as Digital Globe and Planet Labs, offer very high quality commercial data products (COMMercial). Nevertheless, such data is sometimes offered free of charge during Announcement of Opportunity (AO)-events or can be used for scientific purposes when you submit a project proposal (PROPosal) at any time. Additionally, it is worth to visit the mission websites, as there might be some selected sample data sets for free – but pay attention to the user agreements!\nIn the following an overview of the most popular imaging remote sensing programs is given. The corresponding data providers are linked. Each of these satellites has unique specifications and different mission goals and may well complement each other for your research.\n\n\n\nMISSION\nFREE\nCOMM\nPROP\nOPERATIONAL\nCOMMENT\n\n\nOPTICAL\n\n\n\n\n\n\n\nLandsat 5/7/8/9\n✔\nX\nX\n03/1984 – today\nUSGS Earth Explorer\n\n\nSentinel 2\n✔\nX\nX\n06/2015 – today\nESA SciHUB\n\n\nSentinel 3\n✔\nX\nX\n02/2016 – today\nESA SciHUB\n\n\nMODIS Terra & Aqua\n✔\nX\nX\n02/2001 – today\nNASA Earthdata\n\n\nRapidEye\n(✔)\n✔\n✔\n02/2009 - today\nPlanet EyeFind\n\n\nRADAR\n\n\n\n\n\n\n\nSentinel 1\n✔\nX\nX\n04/2014 - today\nESA Copernicus Browser\n\n\nTerraSAR-X\n(✔)\n✔\n✔\n06/2007 - today\nTerraSAR-X ESA archive",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Chapter in a box"
    ]
  },
  {
    "objectID": "RESEDA/contents/Acquireyourdata.html#chapter-in-a-box",
    "href": "RESEDA/contents/Acquireyourdata.html#chapter-in-a-box",
    "title": "Acquire your data",
    "section": "",
    "text": "In this chapter, the following content awaits you:\nIntro\n- get an overview of remote sensing applications and various satellite missions\nSensor Basics\n- repeat basic concepts of remote sensing imaging\n- deepen your knowledge about the four resolutions in RS\nLandsat Earthexplorer & BIG DATA Download\n- use USGS EarthExplorer for browsing and downloading Landsat imagery\n- learn how to download an impressive amount of Landsat data\nSentinel SciHUB & BIG DATA Download\n- use ESA SciHUB for browsing and downloading Sentinel 1 and 2 imagery\n- learn how to download an impressive amount of Sentinel data",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Chapter in a box"
    ]
  },
  {
    "objectID": "RESEDA/contents/Acquireyourdata.html#remotesensingapp",
    "href": "RESEDA/contents/Acquireyourdata.html#remotesensingapp",
    "title": "Acquire your data",
    "section": "",
    "text": "In general, satellite remote sensing has enabled major advances in understanding global climate systems and its changes. Thus, most of the research questions are aimed at better understanding and quantifying climate change to some extent. The results are regularly compiled in the IPCC reports.\nHowever, within this tutorial we focus on the analysis of land cover and land use changes (LULC), which includes all subsystems of the earth and allows a wide range of questions.\n\nAgriculture\ncrop type classification, crop condition assessment, crop yield estimation & forecasting, precision farming, irrigation management\n\nSoil & Geology\nmapping of soil types, soil moisture, mapping of soil management practices, groundwater discharge, permafrost carbon storage, extracting mineral deposits, lithological and structural mapping, detect oil reserves, sediment transport modelling, dinosaur tracks\n\nForest & Vegetation\nspecies classification, deforestation processes, tree crown delineation, biomass, stress monitoring (infestations), carbon storage assessment, estimating forest supplies\n\nOceans & Water Bodies\nsea surface temperature, sea current monitoring, wave height, mean sea level delineation, water salinity, algae growth/ red tides, coral reefs, surface wind speed & direction, oil slicks, fishing activities, watershed delineation, wetland preserving\n\nUrban\nurbanization/ population growth, urban heat island, local climate zones, urban structure types, automatic road network delineation, solar panel energy optimization, monitor traffic jams, night time activity, locate construction alteration, ancient archaeological sites\n\nFire & Disasters\nactive fires, burned areas, fire severity, coal mine fires, volcanic ash monitoring, post earthquake or floods damage mapping, assessment of droughts, landslides, dust storms, tsunamis\n\nIce & Glacier\nmonitoring ice sheets and glaciers, glacier melting, snow melt runoff, carbon content assessement, ship tracking & routing",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Chapter in a box"
    ]
  },
  {
    "objectID": "RESEDA/contents/Acquireyourdata.html#earth-observation-missions",
    "href": "RESEDA/contents/Acquireyourdata.html#earth-observation-missions",
    "title": "Acquire your data",
    "section": "",
    "text": "There is a tremendous amount of free data products (FREE) provided by various remote sensing missions. We will take a closer look at three of them in the upcoming sections (Landsat 8-9, Sentinel 2 and Sentinel 1).\nThen again, companies, as Digital Globe and Planet Labs, offer very high quality commercial data products (COMMercial). Nevertheless, such data is sometimes offered free of charge during Announcement of Opportunity (AO)-events or can be used for scientific purposes when you submit a project proposal (PROPosal) at any time. Additionally, it is worth to visit the mission websites, as there might be some selected sample data sets for free – but pay attention to the user agreements!\nIn the following an overview of the most popular imaging remote sensing programs is given. The corresponding data providers are linked. Each of these satellites has unique specifications and different mission goals and may well complement each other for your research.\n\n\n\nMISSION\nFREE\nCOMM\nPROP\nOPERATIONAL\nCOMMENT\n\n\nOPTICAL\n\n\n\n\n\n\n\nLandsat 5/7/8/9\n✔\nX\nX\n03/1984 – today\nUSGS Earth Explorer\n\n\nSentinel 2\n✔\nX\nX\n06/2015 – today\nESA SciHUB\n\n\nSentinel 3\n✔\nX\nX\n02/2016 – today\nESA SciHUB\n\n\nMODIS Terra & Aqua\n✔\nX\nX\n02/2001 – today\nNASA Earthdata\n\n\nRapidEye\n(✔)\n✔\n✔\n02/2009 - today\nPlanet EyeFind\n\n\nRADAR\n\n\n\n\n\n\n\nSentinel 1\n✔\nX\nX\n04/2014 - today\nESA Copernicus Browser\n\n\nTerraSAR-X\n(✔)\n✔\n✔\n06/2007 - today\nTerraSAR-X ESA archive",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Chapter in a box"
    ]
  },
  {
    "objectID": "projects/posts/post-with-code/index.html",
    "href": "projects/posts/post-with-code/index.html",
    "title": "Future Forest",
    "section": "",
    "text": "The aim of FutureForest is to use artificial intelligence (AI) methods to provide information that can support climate-adapted forest conversion.\nOngoing Projects\n\nFutureForest (future-forest.eu): The aim of FutureForest is to use artificial intelligence (AI) methods to provide information that can support climate-adapted forest conversion. The effects of different climate forecasts and forest conversion scenarios are to be simulated and the possible effects of different management methods are to be assessed. Main tasks of FU Berlin include:\n\nSentinel-2 time series based detection of forest damaged areas in Germany\nOperational \"near real-time\" monitoring using Deep Learning\n\n\n\nEarly detection of damaged areas\nProvision of a Germany-wide map of current forest damage/warnings\n\nUAV based analysis of the deciduous forests under drought stress in the historical garden districts of Berlin and Potsdam in 2020 (in cooperation with „Stiftung Preußische Schlösser & Gärten Berlin-Brandenburg” (SPSG) and „Grünflächenamt Berlin Steglitz-Zehlendorf”\n\nYou can visit the live dashboard of the Installation site here."
  },
  {
    "objectID": "staff/index.html",
    "href": "staff/index.html",
    "title": "Staff",
    "section": "",
    "text": "Fabian Faßnacht\n\n\n\nRESEDA\n\n\n\n\n\n\n\nRemote sensing & geoinformatics group\n\n\nMay 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarion Stellmes\n\n\n\nRESEDA\n\n\n\n\n\n\n\nRemote sensing & geoinformatics group\n\n\nMay 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nAsad Waseem\n\n\n\nRESEDA\n\n\n\n\n\n\n\nRemote sensing & geoinformatics group\n\n\nMay 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nLina María Pérez García\n\n\n\nRESEDA\n\n\n\n\n\n\n\nRemote sensing & geoinformatics group\n\n\nMay 12, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "staff/people/LinaPerez.html",
    "href": "staff/people/LinaPerez.html",
    "title": "Lina María Pérez García",
    "section": "",
    "text": "Institute of Geographical Sciences\n\nRemote Sensing and Geoinformatics\nStudent collaborator\n\nContact\n\n\nAddress: Malteserstraße 74-100 Room H048 12249 Berlin\nTelephone: +49 30 838 70325\nEmail: lina.perez.garcia[at]fu-berlin.de\nOffice hours:"
  },
  {
    "objectID": "staff/people/MarionStellmes.html",
    "href": "staff/people/MarionStellmes.html",
    "title": "Marion Stellmes",
    "section": "",
    "text": "Institute of Geographical Sciences\n\nRemote Sensing and Geoinformatics\nResearcher and lecturer\n\nContactPersonal ProfileTeachingResearchPublications\n\n\nAddress: Malteserstraße 74-100 Room H055 12249 Berlin\nTelephone: +49 3083861978\nEmail: marion.stellmes[at]fu-berlin.de\nOffice hours:\nSprechstunde: wird noch bekanntgegeben\nOffice hour: By appointment via e-mail.\n\n\n\nShort CV\nSince 11/2016: Senior Lecturer and Researcher, FU Berlin, Department of Earth Sciences, Remote Sensing and Geoinformatics\n05/2011 – 10/2016: PostDoc and lecturer; BMBF-Project TFO (The Future Okavango) and SASSCAL (Southern African Science Service Centre for Climate Change and Adaptive Land Management), Trier University, Environmental Remote Sensing and Geoinformatics\n04/2011: PhD (Dr. rer. nat), thesis title: Land use change syndromes as a framework for integrating satellite observations into the assessment of dryland degradation, Trier University\n01/2003 - 04/2011: Scientific assistant and PhD; EU projects LADAMER (Land Degradation Assessment in Mediterranean Europe) and DeSurvey (A Surveillance System for Assessing and Monitoring of Desertification) as well as the Interreg IVa project Regiowood; Trier University, Environmental Remote Sensing and Geoinformatics\n12/2002: Diploma in Applied Environmental Sciences, Thesis: “Ableitung sonneninduzierter Chlorophyll-Fluoreszenz zur Detektion von Pflanzenstress” (Deduction of sun-induced chlorophyll-fluorescence for the detection of plants under stress conditions); Trier University and DLR Oberpfaffenhofen\n10/1996 - 12/2002: Studies of Applied Environmental Sciences; main subjects: remote sensing and geomathematics, subsidiary subjects: soil sciences and climatology; Trier University\n\n\nSS 2021:\n\nPC-S Regionale Themen der Fernerkundung (BSc)\nS und LFP Projektbezogenes Arbeiten: Urbane Fernerkundung (BSc)\n\nWS 2020/2021:\n\nPC-S Spezielle Themen der Fernerkundung: Umweltmonitoring (BSc)\nPC-S Fernerkundung und Geoinformatik für Fortgeschrittene (MSc)\n\nVorherige Kurse FU Berlin:\n\n\nPC-S Geographische Informationssysteme (BSc)\nPC-S Fernerkundung und digitale Bildverarbeitung (BSc)\nV Fernerkundung und digitale Bildverarbeitung\nPC-S Regionale Themen der Fernerkundung (BSc)\nPC-S Spezielle Themen der Fernerkundung (BSc)\nS, PS, LFP Projekt I (u.a. Kyritzer Heide, Rügen)\nS Projekt II\nPC-S Advanced Remote Sensing (MSc)\nV Advanced Remote Sensing (MSc)\n\n\n\nResearch interest and expertise:\nI am specialized in time series analysis of medium and coarse resolution remote sensing time series and I have been working for more than ten years in the field of land cover/use change detection. Moreover, I have attended in many field studies and campaigns to collect validation data with spectrometers and many other instruments.\nI am interested in interdisciplinary environmental research, especially in the evaluation of ecosystem goods and services and I have a strong background in remote sensing based monitoring of land degradation and desertification. I worked in many EU-funded projects, amongst others the LADAMER project and the DeSurvey project, which were dedicated to assessment of land degradation in the Mediterranean. During the last years, I was involved in the international interdisciplinary project SASSCAL, which is funded by the German Federal Ministry of Education and Research is a joint initiative of Angola, Botswana, Namibia, South Africa, Zambia, and Germany, responding to the challenges of global change.\nBackground\nI received my Diploma in Applied Environmental Sciences in 2002, with an emphasis on optical remote sensing and geomathematics and I wrote my diploma thesis in collaboration with the German Aerospace Centre (DLR), Oberpfaffenhofen, Germany. Until October 2016, I was a senior scientist in the Department of Environmental Remote Sensing and Geoinformatics at Trier University, Germany, where I also obtained my PhD in 2012.\n\n\nComplete lists at:\nResearchgate\n\nGoogle Scholar\n\nOrcid"
  },
  {
    "objectID": "staff/people/FabianFassnacht.html",
    "href": "staff/people/FabianFassnacht.html",
    "title": "Fabian Faßnacht",
    "section": "",
    "text": "Institute of Geographical Sciences\n\nRemote Sensing and Geoinformatics\nProfessor\n\nContactPersonal ProfileTeachingResearchPublications\n\n\nAddress: Malteserstraße 74-100 Room H057 12249 Berlin\nTelephone: +49 30 838 51773\nEmail: fabian.fassnacht[at]fu-berlin.de\nOffice hours:\nSprechstunde: wird noch bekanntgegeben\nOffice hour: By appointment via e-mail.\n\n\n\nShort CV\nSince 04/2022: Professor for Remote Sensing and Geoinformatics, Institute of Geographical Sciences, Freie Universität Berlin\n\n2021: Editor-in-Chief Forestry: An International Journal of Forest Research (OUP, Institute of Chartered Foresters)\n\n2020: Senior Scientist, Institute of Geography and Geoecology, Karlsruhe Institute of Technology\n\n2017+2019: Chinese Academy of Sciences President’s International Fellowship Initiative scholarship as visiting researcher, Northwest Institute of Plateau Biology, Xining, China.\n\n2016: Associate Editor Forestry: An International Journal of Forest Research (OUP, Institute of Chartered Foresters)\n\n2015: Fulbright visiting scholar scholarship to visit Colorado State University, USA\n\n2014: Lecturer/PostDoc, Institute of Geography and Geoecology, Karlsruhe Institute of Technology\n\nSince 2013: Guest lecturer at the Universidad de Chile, Chile.\n2012: Research visit Universidad de Chile, Chile.\n2010-2014: PhD University of Freiburg “Assessing the potential of imaging spectroscopy data to map tree species composition and bark beetle-related tree mortality”\n\n\nSS 2022:\n\nVL und PC-S Geographische Informationssysteme (BSc)\nProjekt I WP Modul (MSc)\n\n\n\nResearch interest and expertise:\n\nApplying remote sensing data to understand ecological patterns and processes of vegetation ecosystems\n\nUnderstanding the link between vegetation traits and remotely sensed data\n\nDevelopment of work-flows to quantify and map forest attributes using remote sensing data\n\nWork-flow development in the field of remote sensing to support a range of application fields including for example forest inventories, wildfire risk assessment, prevention and mitigation of grassland degradation\n\nDevelopment of concepts and work-flows to create synthetic remote sensing data enabling an improved understanding of the link between remotely sensed data and the addressed target variable\n\nProject coordination and project proposals\n\nFORZA - Reconstruction of FORest decline processes in the ZAgros forests of Western Iran using remote sensing and dendrochronology [Principal Investigator], 07/2020-06/2022, BMBF.\n\nINSANE – INnovative Spatial information products for forest Applications using NEw satellite technologies [Principal Investigator], 04/2020-12/2021, DAAD.\n\nErweiterung des ökologischen, waldbaulichen und technischen Wissens zu Waldbränden (ErWiN); Teilvorhaben 1: Verbessertes Verständnis der Waldbranddynamik in deutschen Wäldern mittels Deep Learning und Feuerausbreitungssimulationen [Principal Investigator & Project coordinator] 06/2020-09/2023, Fachagentur Natürliche Rohstoffe e.V..\n\nRegional Monitoring and Modeling of the Effects of Vegetation Restoration on Soil Erosion [Principal Investigator], 02/2019-06/2019, Chinesisch-Deutsches * Zentrum für Wissenschaftsförderung (Workshop-Förderung).\n\nSYSSIFOSS - Synthetic structural remote sensing data for improved forest inventory models [Principal Investigator], 05/2019-10/2022, DFG.\n\nAssessing spatio-temporal impacts of global change on water and biomass production processes at catchment scale: a synergistic approach based on remote sensing and coupled hydrological models to improve sustainable management of forest ecosystems [Co-Investigator], 01/2017-03/2021 FONDECYT (Chile).\n\nSaMovar - Satellite-based Monitoring of invasive species in central-Chile [Principal Investigator], 01/2016-06/2017, BMWi/DLR\n\nModellierung der klimatischen Standorteignung forstlich relevanter Baumarten [Co-Investigator], 01/2015 – 12/2016, LUBW / KLIMOPASS 37\n\nWaldBiomasse [actively involved in project proposal], 05/2013-10/2015, BMWi/DLR.\n\nForestHype [actively involved in project proposal]07/2010-06/2013, BMWi/DLR.\n\n\n\nComplete lists at:\nResearchgate\n\nGoogle Scholar\n\nPublons\n\nOrcid"
  },
  {
    "objectID": "staff/people/Asad Waseem.html",
    "href": "staff/people/Asad Waseem.html",
    "title": "Asad Waseem",
    "section": "",
    "text": "Institute of Geographical Sciences\n\nRemote Sensing and Geoinformatics\nStudent Collaborator\n\nContact\n\n\nAddress: Malteserstraße 74-100 Room H051 12249 Berlin\nTelephone: +49 157 8222 3656\nEmail: asad.waseem[at]fu-berlin.de\nOffice hours: By appointment via e-mail."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Research projects",
    "section": "",
    "text": "Future Forest\n\n\n\nOngoing\n\nsentinel-2\n\nforest damage\n\n\n\n\n\n\n\n\n\nMar 9, 2024\n\n\nRemote sensing & geoinformatics group\n\n\n\n\n\n\n\n\n\n\n\n\nMOFA\n\n\n\nCompleted\n\nmultitemporal\n\nmultisensor\n\n\n\n\n\n\n\n\n\nDec 1, 2014\n\n\nBjörn Waske\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/posts/welcome/index.html",
    "href": "projects/posts/welcome/index.html",
    "title": "MOFA",
    "section": "",
    "text": "Monitoring Farmland Abandonment by multitemporal and multisensor remote sensing imagery (MOFA)\n\nThe research project studies an area in the border region of Poland and Ukraine. With the fall of the Iron Curtain the region experienced drastic changes in political and socio- economic structures. Large farmland areas become abandoned and gradual processes of forest succession take place on the abandoned land. The aim of the project is the development of adequate strategies to monitor farmland abandonment, using multitemporal SAR and multispectral remote sensing data.\n\n\n\nFinally enhanced maps should be provided, which enable more detailed analysis of the gradual process of land cover transitions. (PI: B. Waske, funding: German Research Foundation (DFG) WA 2728/2-1)\nThus, remote sensing based mapping offers great opportunities to map these phenomena and ultimately to better understand patterns, processes and underlying causes. Existing studies are based on multispectral imagery, abandonment maps, however, are difficult to obtain due to spectral ambiguities, phenological variability and limited data availability. SAR data can overcome these problems and different remote sensing studies demonstrated the potential of multisensor imagery. The aim of the project is the development of adequate strategies to monitor farmland abandonment, using multitemporal SAR and multispectral remote sensing data. Finally enhanced maps should be provided, which enable more detailed analysis of the gradual process of land cover transitions.\nThe study site within the border region of Poland and Ukraine (© Google, 2012)\n\n\nProject Duration: 01/2012 - 12/2014\nPrincipal Investigator: Björn Waske\nProjects staff: Jan Stefanski (University Bonn)\nProject partners:\nProf. Dr. Tobias Kümmerle, Biogeography and Conservation Biology Group, Humboldt-University of Berlin\nDr. Oleh Chaskovskyy, Ukrainian National Forestry University, Lviv (Ukraine)\nDr. hab. Jacek Kozak, Department of GIS, Cartography and Remote Sensing, Jagiellonian University, Kraków (Poland)\nFunding: DFG - German Research Foundation (WA 2728/2-1)"
  },
  {
    "objectID": "RESEDA/contents/R-Studio.html",
    "href": "RESEDA/contents/R-Studio.html",
    "title": "R-Studio",
    "section": "",
    "text": "R is a programming language and software environment designed for statistical computing and graphic data output, which is very popular among data scientists. While R is operated purely by command line, there are several graphical user interfaces available, such as Rattle, R Commander, Deducer, RKWard, JGR, and R-Studio.\nIn this online course we will use R-Studio as an integrated development environments (IDE). The default window layout of R-Studio is divided into four panes:\n\n\nDefault layout in R-Studio\n\n\n1 Source Pane: for writing, saving and sending R code to the console. This pane does not exist by default when you start R-Studio. It appears only when you open at least one R-script via File &gt; New File &gt; R script\n2 Console Pane: Code you enter here is immediately processed by R. This pane is used for interactively testing code line-by-line before you copy your final code to the Source pane (1) above\n3 Environment Pane: presents a list of information about all variables/ objects in your current R-Studio session. This list contains their class, dimensions and names. There is a history of all processed lines accessible if you click the History-tab\n4 Files Pane: contains several tabs for a file browser (Files), an essential help function (Help), the package manager (Packages), and viewer for plots (Plots) and interactive R output (Viewer)\n\nAlthough initially confusing, you will get used to each pane over time and quickly learn to switch between them to optimize your workflow when coding!\nR-Studio provides a variety of features that make life easier for you, e.g., color highlighting of syntax, code completion, find and replace functionality, import functions for data sets, and many more. Especially the Source pane allows to save sequences of commands for later re-use, enhancing reproducibility.\n \n\n\nThere are multiple ways to execute code in R-Studio. The simplest and most straight-forward way is to type your code directly in the Console pane and press enter key (or return key).\nIn order to execute a line of code from the Source pane, place your cursor in this line and press Ctrl+Enter or use the Run toolbar button (top right corner of the Source pane). R-Studio automatically advances the cursor to the next line, enabling a execution line-by-line.\nYou can execute multiple lines at once by selecting requested lines and pressing Ctrl+Enter. It is even possible to select parts of a line and execute only this selection. Press Ctrl+Shift+Enter or the Source toolbar button (top right corner of the Source pane) to execute the whole code inside your Source pane.\nIf you want to make use of previously executed commands, you can use the keyboard’s up and down arrow keys when in Console pane to access the last entries, modify them if you want, and then press Enter again.\n \n\n\n\nR provides a comprehensive built-in help system. At the command prompt in the Console pane of R-Studio type any of the following in order to view descriptions of any operators, functions and objects:\nhelp.start()       # general help\nhelp(foo)          # help about function foo\n?foo               # help about function foo\napropos(\"foo\")     # list all functions having \"foo\" in their names\nexample(foo)       # show an example of function foo\n \n\n\n\nThere are some time saving shortcuts implemented to manage your R scripts:\n\nCtrl+Shift+n – create a new R document\nCtrl+o – open a existing file\nCtrl+s – save your currently active file in the Source Pane\nCtrl+f – activate search/ replace functionality within Source Pane\nCtrl+Shift+c – comment/ uncomment selected parts of your code (commented code with a # at the beginning of a line will not be executed)\n\nYou can find all of those commands in the top toolbar of R-Studio as well.",
    "crumbs": [
      "RESEDA",
      "Preparations",
      "RStudio"
    ]
  },
  {
    "objectID": "RESEDA/contents/R-Studio.html#execute",
    "href": "RESEDA/contents/R-Studio.html#execute",
    "title": "R-Studio",
    "section": "",
    "text": "There are multiple ways to execute code in R-Studio. The simplest and most straight-forward way is to type your code directly in the Console pane and press enter key (or return key).\nIn order to execute a line of code from the Source pane, place your cursor in this line and press Ctrl+Enter or use the Run toolbar button (top right corner of the Source pane). R-Studio automatically advances the cursor to the next line, enabling a execution line-by-line.\nYou can execute multiple lines at once by selecting requested lines and pressing Ctrl+Enter. It is even possible to select parts of a line and execute only this selection. Press Ctrl+Shift+Enter or the Source toolbar button (top right corner of the Source pane) to execute the whole code inside your Source pane.\nIf you want to make use of previously executed commands, you can use the keyboard’s up and down arrow keys when in Console pane to access the last entries, modify them if you want, and then press Enter again.",
    "crumbs": [
      "RESEDA",
      "Preparations",
      "RStudio"
    ]
  },
  {
    "objectID": "RESEDA/contents/R-Studio.html#get-some-help",
    "href": "RESEDA/contents/R-Studio.html#get-some-help",
    "title": "R-Studio",
    "section": "",
    "text": "R provides a comprehensive built-in help system. At the command prompt in the Console pane of R-Studio type any of the following in order to view descriptions of any operators, functions and objects:\nhelp.start()       # general help\nhelp(foo)          # help about function foo\n?foo               # help about function foo\napropos(\"foo\")     # list all functions having \"foo\" in their names\nexample(foo)       # show an example of function foo",
    "crumbs": [
      "RESEDA",
      "Preparations",
      "RStudio"
    ]
  },
  {
    "objectID": "RESEDA/contents/R-Studio.html#some-useful-shortcuts",
    "href": "RESEDA/contents/R-Studio.html#some-useful-shortcuts",
    "title": "R-Studio",
    "section": "",
    "text": "There are some time saving shortcuts implemented to manage your R scripts:\n\nCtrl+Shift+n – create a new R document\nCtrl+o – open a existing file\nCtrl+s – save your currently active file in the Source Pane\nCtrl+f – activate search/ replace functionality within Source Pane\nCtrl+Shift+c – comment/ uncomment selected parts of your code (commented code with a # at the beginning of a line will not be executed)\n\nYou can find all of those commands in the top toolbar of R-Studio as well.",
    "crumbs": [
      "RESEDA",
      "Preparations",
      "RStudio"
    ]
  },
  {
    "objectID": "RESEDA/contents/Machine-Learning-Basics.html",
    "href": "RESEDA/contents/Machine-Learning-Basics.html",
    "title": "Machine Learning Basics",
    "section": "",
    "text": "This section includes the basics of machine learning (ML) theory as well as the commonly used terms and concepts in order to get comfortable with with the two ML algorithms we will focus on: Random Forest (RF) and Support Vector Machines (SVM).\n\nYou will learn what machine learning means and get insight in two ML algorithms for image classification: Random Forest and Support Vector Machine.\nWhat is Machine Learning?\n– basic introduction and definintion\n– commonly used terms when dealing with ML\nRandom Forest\n– Random Forest method explained\n– concept of Decision Trees (CART) as part of Random Forests\nSupport Vector Machine\n– concept of a Support Vector Machine\n\n\n\n\n\nWe will have a look at two supervised, non-parametric and nonlinear algorithms in the next two sections, the Random Forest and Support Vector Machine. These terms do not tell you anything? Then read the following short intro, which explains the most important terminology.\nMachine Learning (ML) is actually a lot of things – it is a generic term for the artificial generation of knowledge or artificial intelligence. A artificially learning system learns from examples and can generalize those after the learning phase is completed. Examples are not simply memorized, but the underlying pattern is recognized. This becomes handy, when new, unknown, data should be handled (learning transfer).\nML methods optimize their performances iteratively by learning from the training examples or train data. Those methods can be descriptive by distinguishing between several classes of a phenomenon (i.e., classification) or predictive by making predictions of a phenomenon (i.e., regression). Ultimately, ML methods create a function, which tries to map input data \\(x\\) (e.g., Landsat 8 band reflectances) to an desired output response \\(y\\) (e.g., class labels, such as urban):\n\\[\n\\text{Find }f : x \\mapsto y = f(x).\n\\]\nIn the field of remote sensing, descriptive ML algorithms are often used for land cover classifications. The range of applications for this is huge, ranging from land use / land cover (LULC) changes of all kind, e.g., urban sprawl, burned area detection, flood area forecast or land degradation. In order to perform a classification, the algorithm must learn to differentiate between the various types of patterns based on training samples. After the model has learned, it can then be tested for performance using independent testing samples. There are two main ways an algorithm can train: in a unsupervised or a supervised learning manner:\n\n\n\n\n\nThe difference between unsupervised and supervised algorithms is based on whether they include a priori knowledge during the training phase by using labeled training samples or not.\nUnsupervised algorithms have only the training samples \\(x\\) available, e.g., reflectance values of all 9 bands of Landsat 8. Thus, the class label information, e.g., urban, or forest, is missing. The objective of the algorithm is to describe how the data are organized or clustered – it has to find patterns and relationships by itself.\nSupervised algorithms make use of a set of labeled training samples, including the samples \\(x\\) and the appropriate class labels \\(y\\). The objective here is to predict the value \\(y\\) corresponding to a new, unknown, sample \\(x\\). In other words, we teach the algorithm what the individual classes look like in the feature space, e.g., Landsat 8 bands.\nThere are also semisupervised methods, where the two beforementioned approaches are combined. Waske at al. 2009 provided a more complete overview of different classifier categories and examples.\n\n\n\n\n\nThis differentiation is straightforward:\nLinear algorithms assume, that the sample features \\(x\\) and the label output \\(y\\) are linearly related and there is an affine function \\(f(x) = \\langle w, x \\rangle + b\\) describing the underlying relationship.\nNonlinear algorithms assumes a nonlinear relationship between \\(x\\) and \\(y\\). Thus, \\(f(x)\\) can by a function of arbitrary complexity.\n\n\n\n\n\nSome ML methods requires the data to follow a specific distribution in its feature space, for example the form of a multivariate normal Gaussian model. A method is called parametric, when those assumptions are made. The Maximimum Likelihood Classifier is such a parametric algorithm.\nNon-parametric approaches are not constrained to prior assumptions on the data distribution. Such methods allow application in versatile tasks and data types, such as e.g., RADAR data.\n\n\n\n\n\nIn statistics, the term fit refers to how well you approximate the function the ML algorithm uses to map input \\(x\\) to output \\(y\\).\nOverfitting appears, when a model learns to map the training data too well, which negatively impacts the performance of the model on new, unknown, data. Thus, the model lost its ability to generalize.\nA trained ML model is underfitting, if it neither can model the train nor the test data correctly. Underfitting is easy to detect in the training phase by using given performance metrics. If the problem can not be solved, another classifier should be considered.",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Machine Learning Basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/Machine-Learning-Basics.html#WIML",
    "href": "RESEDA/contents/Machine-Learning-Basics.html#WIML",
    "title": "Machine Learning Basics",
    "section": "",
    "text": "We will have a look at two supervised, non-parametric and nonlinear algorithms in the next two sections, the Random Forest and Support Vector Machine. These terms do not tell you anything? Then read the following short intro, which explains the most important terminology.\nMachine Learning (ML) is actually a lot of things – it is a generic term for the artificial generation of knowledge or artificial intelligence. A artificially learning system learns from examples and can generalize those after the learning phase is completed. Examples are not simply memorized, but the underlying pattern is recognized. This becomes handy, when new, unknown, data should be handled (learning transfer).\nML methods optimize their performances iteratively by learning from the training examples or train data. Those methods can be descriptive by distinguishing between several classes of a phenomenon (i.e., classification) or predictive by making predictions of a phenomenon (i.e., regression). Ultimately, ML methods create a function, which tries to map input data \\(x\\) (e.g., Landsat 8 band reflectances) to an desired output response \\(y\\) (e.g., class labels, such as urban):\n\\[\n\\text{Find }f : x \\mapsto y = f(x).\n\\]\nIn the field of remote sensing, descriptive ML algorithms are often used for land cover classifications. The range of applications for this is huge, ranging from land use / land cover (LULC) changes of all kind, e.g., urban sprawl, burned area detection, flood area forecast or land degradation. In order to perform a classification, the algorithm must learn to differentiate between the various types of patterns based on training samples. After the model has learned, it can then be tested for performance using independent testing samples. There are two main ways an algorithm can train: in a unsupervised or a supervised learning manner:",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Machine Learning Basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/Machine-Learning-Basics.html#UvS",
    "href": "RESEDA/contents/Machine-Learning-Basics.html#UvS",
    "title": "Machine Learning Basics",
    "section": "",
    "text": "The difference between unsupervised and supervised algorithms is based on whether they include a priori knowledge during the training phase by using labeled training samples or not.\nUnsupervised algorithms have only the training samples \\(x\\) available, e.g., reflectance values of all 9 bands of Landsat 8. Thus, the class label information, e.g., urban, or forest, is missing. The objective of the algorithm is to describe how the data are organized or clustered – it has to find patterns and relationships by itself.\nSupervised algorithms make use of a set of labeled training samples, including the samples \\(x\\) and the appropriate class labels \\(y\\). The objective here is to predict the value \\(y\\) corresponding to a new, unknown, sample \\(x\\). In other words, we teach the algorithm what the individual classes look like in the feature space, e.g., Landsat 8 bands.\nThere are also semisupervised methods, where the two beforementioned approaches are combined. Waske at al. 2009 provided a more complete overview of different classifier categories and examples.",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Machine Learning Basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/Machine-Learning-Basics.html#LnL",
    "href": "RESEDA/contents/Machine-Learning-Basics.html#LnL",
    "title": "Machine Learning Basics",
    "section": "",
    "text": "This differentiation is straightforward:\nLinear algorithms assume, that the sample features \\(x\\) and the label output \\(y\\) are linearly related and there is an affine function \\(f(x) = \\langle w, x \\rangle + b\\) describing the underlying relationship.\nNonlinear algorithms assumes a nonlinear relationship between \\(x\\) and \\(y\\). Thus, \\(f(x)\\) can by a function of arbitrary complexity.",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Machine Learning Basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/Machine-Learning-Basics.html#parametric-vs.-non-parametric-algorithms",
    "href": "RESEDA/contents/Machine-Learning-Basics.html#parametric-vs.-non-parametric-algorithms",
    "title": "Machine Learning Basics",
    "section": "",
    "text": "Some ML methods requires the data to follow a specific distribution in its feature space, for example the form of a multivariate normal Gaussian model. A method is called parametric, when those assumptions are made. The Maximimum Likelihood Classifier is such a parametric algorithm.\nNon-parametric approaches are not constrained to prior assumptions on the data distribution. Such methods allow application in versatile tasks and data types, such as e.g., RADAR data.",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Machine Learning Basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/Machine-Learning-Basics.html#overfitting-vs.-underfitting",
    "href": "RESEDA/contents/Machine-Learning-Basics.html#overfitting-vs.-underfitting",
    "title": "Machine Learning Basics",
    "section": "",
    "text": "In statistics, the term fit refers to how well you approximate the function the ML algorithm uses to map input \\(x\\) to output \\(y\\).\nOverfitting appears, when a model learns to map the training data too well, which negatively impacts the performance of the model on new, unknown, data. Thus, the model lost its ability to generalize.\nA trained ML model is underfitting, if it neither can model the train nor the test data correctly. Underfitting is easy to detect in the training phase by using given performance metrics. If the problem can not be solved, another classifier should be considered.",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Machine Learning Basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/Machine-Learning-Basics.html#decision-tree-method",
    "href": "RESEDA/contents/Machine-Learning-Basics.html#decision-tree-method",
    "title": "Machine Learning Basics",
    "section": "Decision Tree Method",
    "text": "Decision Tree Method\nA Decision Tree, also known as Classification and Regression Tree (CART), is a supervised learning algorithm first introduced by Breiman et al. in 1984. We want to focus on the implementation used by the R package randomForest, which we will use later in this section.\nA CART trains by splitting all available samples into homogeneous sub-groups of high purity based on a most significant feature.\nBut one thing at a time…\nA decision tree conceptionally consists of three elements:\n\nOne Root Node: That is the starting point, which includes all training samples. A first split is done here.\nMultiple Decision Nodes: Those are nodes where further splits are done.\nMultiple Leaf (or Terminal) Nodes: Those are nodes where the assignments to the classes (e.g., urban, water, etc.) happens for classifications tasks (or the mean response of all observation in this node is calculated for regressions).\n\nThe structure and complexity of each DT can vary, depending on the training samples and the features used. For example, a very simple DT could look like this:\n\nOk, let us make that clearer with a fictitious example: We want to classify a Landsat 8 scene with 11 multispectral bands into four classes Forest, Urban, Bare Soil & Water. As a training dataset we use 500 pixels, 200 pixels belong to the forest class, 120 to urban, 80 to bare soil and to 100 water class, accordingly. We put all 500 samples in the Root Node in the top of the DT:\n\nThe next step is to split the node into two sub-nodes using a single threshold value. The CART algorithm always uses binary splits, i.e., it splits a node exactly in two sub-nodes, to increase the homogeneity of the resultant sub-nodes. In other words, it increases the purity of the node in respect to our target class. For example, the highest purity would be achieved if only one class was present in the first sub-node, and only one second class in the second sub-node. Of course this is not possible in our four class classification problem with only one split:\n\nThe first split could only clearly separate the Water samples, otherwise all classes are present at certain proportions in both subnodes. As you can see in the figure above, the DT decided to use the Landsat band 2 (b2) and the reflectance threshold value of 450 to do the split.\nBut wait a minute, how does the classifier know HOW and WHERE to do a split?!\nThe DT does not consider all features (Landsat bands) as candidates at each split, but only a certain number of features (\\(mtry\\)). By default, the parameter \\(mtry\\) corresponds to the root of the number of features \\(M\\) available, i.e., \\(mtry = \\sqrt{M}\\). Thus, in our example the DT samples 3 out of 11 Landsat bands randomly and with replacement, because \\(\\sqrt{11}\\approx3\\).\nThere are various measures for determining the most significant candidate (e.g., Gini Index, Chi-Square, Variance Reduction or Information Gain). Our CART algorithm uses the Gini Index, so let us have a look at what it does:\nGini Index says, if we select two samples from our node, then they must be of the same class. If a node contains only one class, i.e., a completely pure node, Gini value would be 0. Thus, the lower the Gini value, the purer the node – which is good, because we want to group the samples according to their classes. The Gini Index is calculated by subtracting the sum of the squared probabilities \\(p\\) of each class \\(\\{1,2, …, C\\}\\) from one:\n\\[\n\\text{Gini} = 1 – \\sum_{i=1}^{C} (p^2).\n\\]\nFor the first split in our example shown in the figure above, the math would be:\n\ncalculate Gini for left sub-node: \\((160/310)^2 + (40/310)^2 + (10/310)^2 + (100/310)^2 = 0.39\\)\ncalculate Gini for right sub-node: \\(0.38\\) if the Landsat Band 2 and the reflectance value 450 are chosen as as the feature and the threshold value, respectively. Finding a suitable threshold is an iterative process. So the algorithm tries many different values before picking out the one resulting in the smallest Gini, in this case the 450. As \\(mtry = 3\\) in our example, the DT will test two more bands, e.g., band11 and band 1, the same way and compare the Gini values in the end.\n\nThat is all the magic behind splitting procedure! In the same way, the sub-nodes are split further, making the tree “grow”:\n\nAll samples within a leaf node are assigned to the class most frequently represented. The figure above shows, that the leaf nodes are not perfectly pure, e.g., there are 35 forest and 8 urban samples assigned to the water-node on the far left. These will result in misclassifications in your final classification map.",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Machine Learning Basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/Machine-Learning-Basics.html#working-with-random-forests",
    "href": "RESEDA/contents/Machine-Learning-Basics.html#working-with-random-forests",
    "title": "Machine Learning Basics",
    "section": "Working With Random Forests",
    "text": "Working With Random Forests\nIn the case of a Random Forest approach, the tree shown above would continue growing until almost all nodes contain only one class at a time. While that would most likely result in overfitting in a single DT, this effect is relativized by the majority vote in the end when using a RF. Anyway, there are some more advantages when using a RF:\nOut of Bag (OOB) Error:\nThere is no need for a cross-validation or a separate test set, as the OOB Error is estimated internally in the training phase as an unbiased estimate of the classification error. Remind: Each DT is created based on a small sample of your original training data (the so-called bootstrap sample). About one-third of the samples within this bootstrap sample are left out (out-of-bag) and not used for the construction of the DT. After the construction of the DT, the left out samples are used to test the tree. The Leaf Nodes then provide the class that is most represented by the OOB samples in each node. The proportion of samples that does not correspond to the class in the Leaf Node, averaged over all Leaf Nodes, is the OOB Error (overall accuracy for classifications and mean absolute error for regressions).\nVariable Importance (VI)::\nA RF can identify the most significant/ important features for your classification or regression task. Therefore it can also be considered as a method for dimensionality reduction.\nIf you plot the variable importance, you will get two measures in the randomforest package:\nMeanDecreaseAccuracy: we permute all threshold values of a specific feature (e.g., Landsat band 2) and see how the purity in a Leaf Node changes. In detail: we count the number of correctly classified OOB samples once the tree is grown. After that, we randomly permutate the values of a specific feature for the OOB samples and classify the OOB samples again. The difference of correctly classified samples between permuted and original OOB data, averaged over all trees, is the MeanDecreaseAccuracy.\nMeanDecreaseGini: each time a split is done, both sub-nodes become more pure and the Gini values decrease. The sum of all Gini decreases for each individual feature (e.g., Landsat band) over all trees is the MeanDecreaseAccuracy.",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Machine Learning Basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/Machine-Learning-Basics.html#basic-concept",
    "href": "RESEDA/contents/Machine-Learning-Basics.html#basic-concept",
    "title": "Machine Learning Basics",
    "section": "Basic Concept",
    "text": "Basic Concept\nConsider a set of labelled training samples \\(X\\), e.g., pixels of an RS image. Each sample is a single vector in the feature space, e.g., one grey scale value for each of the 11 spectral bands of a Landsat 8 scene gives a vector with dimensions \\(1\\times8\\). The concept of a SVM is based on an optimal linear separating hyperplane that is fitted to the training samples of two classes within this multidimensional feature space. Thereby, the optimization problem that has to be solved is aiming at a maximization of the margins between this hyperplane and the closest training samples, the so-called and name giving Support Vectors. Only those Support Vectors are needed to describe the hyperplane mathematically exact, vectors farther from the hyperplane remain hidden. The following figure shows a hyperplane separating two classes in two-dimensional space, e.g. Band 3 on the x-axis and Band 4 on the y-axis:\n\n\n\nA linear hyperplane separating two classes (blue and green dots) in a two-dimensional feature space, e.g., Landsat bands 3 and 4\n\n\nThis wide and empty border area surrounding the hyperplane should ensure that new vectors, which are not in our training dataset, i.e., pixels in S imagery, will be classified as reliably as possible in a later prediction process.\nOk, the classification problem in the figure above is very easy to solve linearly. However, the condition of linear separability is generally not met in real-life data. In the case of non-linear separable samples, the SVM needs a so-called kernel. A kernel is actually a function, which maps the data into a higher dimensional space where the data is separable. There are several of these kernels, such as linear, polynomial, radial basis and sigmoid kernels, with the radial basis function (RBF) kernel most commonly used for non-linear problems. In doing so, even the most nested training samples become linear separable and the hyperplane can be determined. After a transformation back into the initial lower-dimensional space, this linear hyperplane can become a non-linear or even un-connected hypersurface:\n\n\n\nNon-linear hyperplane after inverse transformation from higher dimensionality separating two classes\n\n\nIn the context of Remote Sensing, binary classification problems are not common. There are several approaches to solve multi-class problems, the most frequently used approach is the one-against-one rule, which is also implemented in the e1071 package we use in the next section. According to this one-against-one approach, a binary classifier is trained for each pair-wise class combination, and the most frequently assigned class label is assigned in the end.",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Machine Learning Basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/Machine-Learning-Basics.html#common-parameters",
    "href": "RESEDA/contents/Machine-Learning-Basics.html#common-parameters",
    "title": "Machine Learning Basics",
    "section": "Common Parameters",
    "text": "Common Parameters\nRegularisation parameter \\(C\\): As the hyperplane can be of arbitrary dimensionality, it could be fitted perfectly to match the training dataset. However, this would result in extreme overfitting. To regularisation parameter C allows the SVM to misclassify individual samples, but at the same time punish them. Large C values cause the hyperplane margin to shrink, which may help to increase the amount of correctly classified samples. Conversely, a small C value will result in a larger-margin hyperplane, leading to lower punishments and maybe more misclassified samples.\nKernel parameter gamma \\(\\gamma\\): This parameter defines how far the influence of a single training sample reaches, i.e., its radius of influence. Low values mean “far”, and high values mean “close”. The model performance is quite sensitive to this parameter: when gamma is too large, the radii of influence of the SVs are too small and only affect the SVs itself, making the regularisation with C impossible and thus lead to overfitting. When gamma is too small, the model is too constrained and may not be capable to capture the complexity of our training dataset.\nParameter epsilon \\(\\varepsilon\\) (only in regressions): The epsilon parameter is an additional value of tolerance where no penalty is given to errors. The larger epsilon is, the larger errors you allow in your model. When epsilon is zero, all errors will be penalized.",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Machine Learning Basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/SensorBasics.html",
    "href": "RESEDA/contents/SensorBasics.html",
    "title": "Sensor Basics",
    "section": "",
    "text": "In this section you will repeat the meaning of basic remote sensing terminology using the example of three satellites which we will continue to use in RESEDA, i.e., Landsat 8, Sentinel 2 and Sentinel 1. However, the shown methods in this online course are also adaptable to other satellite sensors, airborne imagery or close-ups by considering the respective sensor characteristics.\n\n\n\n\nNon-imaging sensors, such as laser profilers or RADAR altimeter, usually provide point data, which do not offer spatially continuous information about how the input varies within the sensor’s field of view.\nBy contrast, imaging sensors are instruments that build up a spatially continuous digital image within their field of view, whereby they include not only information about the intensity of a given target signal, but also information about its spatial distribution. Examples include aerial photography, visible or near infrared scanner as well as synthetic aperture radars (SAR). All three satellite missions we will work with (Landsat 8, Sentinel 2 and Sentinel 1) are imaging sensors.\n\n\n\n\n\nAnyway, there is a fundamental differentiation of remote sensing systems that we need to be aware of: passive and active sensors. This classification is based on the underlying recording principles, which are contrasted in the following:\n\n\nPassive sensor operating mode\n\nPassive sensors should be the more common of those two. Passive sensors measure solar light reflected or emitted from the Earth surfaces and objects.\nThese instruments primarily rely on short waved electromagnetic solar energy of the sun as the only source of radiation. Objects on the Earth’s surface react to this electromagnetic energy either with reflection, transmission, or absorption, depending on the composition of the object’s atoms. Passive sensors mainly capture the reflected proportion of the solar energy. Thus, they can only do their observation job when the sun is present as a radiation source – that is, only during the day. Anyway, since objects also partially absorb incoming solar light, there is a inherent radiation of these objects, which can be measured, for example, as thermal radiation.\nPassive remote sensing imagery can be very similar to how our human eyes perceive land cover, which makes it easier for us to interpret the image data.\nUnfortunately, there is one big limitation of passive systems: Due to the fact that the reflected electromagnetic radiation has to pass through the Earth’s atmosphere, the signal is strongly influenced by the weather and cloud conditions: Fog, haze and clouds render affected image information partially or completely useless, as electromagnetic energy is scattered by the large particles of dense clouds. This is where active sensors come into play.\n\n\nActive sensor operating mode\n\nActive sensors emit their own electromagnetic radiation to illuminate the object they observe.\nActive sensors send a pulse of energy at the speed of light to the Earth’s surface, that is reflected, refracted or scattered by the objects on the surface and the atmosphere. The recieved backscatter then gives information on land surface characteristics. There are many types of active sensors out there: RADAR (Radio Detection and Ranging), Scatterometer, LiDAR (Light Detection and Ranging), and Laser Altimeter. We will take a closer look at Sentintel 1, a SAR system (Synthetic Aperture RADAR), which is an imaging RADAR type working with microwaves. Images of active sensors are comparatively difficult to interpret: A SAR signal contains amplitude and phase information. Amplitude is the strength of the RADAR response and phase is the fraction of a full sine curve. In order to generate beautiful images out of these information, a more extensive preprocessing is often necessary, which we will do in SNAP.\nSince no natural light source is required, active sensors are capable to emit and capture their signals regardless of daytime. In addition, many systems operate in the electromagnetic domain of microwaves, so their wavelength is large enough to be unaffected by clouds and other atmospheric distortions.\n\n\n\n\n\nWhen working with imaging remote sensing systems, a distinction is made between four different resolution terms: geometric, spectral, radiometric and temporal. It is essential to know those resolutions in order to be able to assess whether the sensor system is suitable for your research question!\n\n\nA digital image consists of at least one matrix of integers values – the picture elements, or pixels. Each pixel contains information about a signal response from a small area on the land surface, e.g., reflectance or backscatter. The geometric Resolution describes the edge length of this area (usually in meters) and is determined by the sensor’s instantaneous field of view (IFOV). It determines which object sizes can still be identified in the image – the degree of detail, so to speak. The effects of geometric resolutions becomes evident when comparing different images, for example an airborne orthophoto (0.1 m), Sentinel 2 (10 m) and Landsat 8 (30 m) images:\n\n\nResidential area in Friedenau, orthophoto (l), Sentinel 2 data (m), and Landsat 8 data (r)\n\nThere are several synonyms commonly used for the term geometric resolution, e.g., spatial resolution, pixel size, pixel edge length, or ground sampling distance.\n\n\n\nWe humans can only perceive the visible light around us, which is just a very small part of the available electromagnetic spectrum. Satellites, on the other hand, sense a much wider range of the electromagnetic spectrum and can provide us with information about processes that would otherwise be neglected.\nSpectral satellite sensors measure the reflection from the earth’s surface in different wavelength areas of the electromagnetic spectrum, via so-called channels or bands. Each band can have a different bandwidth, i.e., the area scanned within the electromagnetic spectrum. The sensors concentrate the signals gathered within a band to one (pixel-) value via a sensor specific filter function.\nHowever, spectral resolution describes the number of bands that the sensor senses. The purpose of multiple bands is to capture the differences in the reflection characteristics of different surfaces and materials within the electromagnetic spectrum.\nPanchromatic systems have only one spectral channel with a large bandwidth usually from 0.4 to 0.7 μm. Due to this large bandwidth there is enough energy available to achieve high geometric resolutions. Various satellites additionally provide such a panchromatic channel, e.g., Landsat program, Quickbird-2 pan, and IKONOS pan.\nMultispectral systems generally refers to 3 to 15 bands, which are usually located within the visible range (VIS, 0.4-0.7 μm), near-infrared (NIR, 0.75–1.4 μm), short-wavelength infrared (SWIR, 1.4–3 μm), mid-wavelength infrared (MWIR, 3–8 μm) and long wavelength/ thermal infrared (LWIR/TIR, 8–15 μm). Examples: Landsat program, Sentinel 2/3, SPOT, Quickbird, and RapidEye.\n\n\n\n\nSentinel 2 image, airport Schönefeld, Berlin; left: true color composite (RGB 4,3,2), right: pseudo-color composite (RGB 8,4,3)\n\nHyperspectral systems (spectrometer) offer hundreds or thousands of bands with narrow bandwidths. A spectral resolution this high gives ability to distinguish minor differences of land cover characteristics, which in turn provides ability to address issues that could not be solved with multispectral data, e.g., mineral or building material classifications. There are some airborne spectrometers (AVIRIS, HySPEX, HyMAP) and to date only one operational satellite: Hyperion. Anyway, more spaceborne systems are already in the starting blocks, e.g., EnMAP and HyspIRI.\n\n\n\nA digital sensor generally recognizes objects as intensity values, whereby it can only distinguish between dark and bright. The radiometric resolution refers to the ability to tell apart objects based on differences in those intensities. Thus, a sensor with a high radiometric resolution records more intensity levels or grey-scale levels. This property is expressed by the number of bits. Most satellite products have a bit depth of 8 to 16 bits, which means that they support 28(=256) or 216(=65536) different gray scale levels, respectively. A 1 bit image would be subject to a huge loss of information in comparison!\nThose effects are easier to understand when looking at the image comparison below: An image with a bit depth of 1 contains only two gray scale levels (21=2), i.e., black and white. Using 2 bits double the number of available colors (22=4), which allows a few more details (as shown in the middle image). When using 8 bits, the image can draw from a whole color ramp ranging from black to white (dark to bright) comprising 256 grey scale values (28=256).\n\n\nBlue band of an orthophoto in 1 bit (l), 2 bit (m), and 8 bit (r) representation\n\nHuman perception is barely sufficient to detect gray-scale differences beyond 8 bits in digital images. Nevertheless, machine learning algorithms often benefit from a finer differentiation of contrasts.\n\n\n\nThe temporal resolution of a sensor is simply the distance of time (usually in days) between two image acquisitions of the same area. A high temporal resolution thus indicates a smaller time window between two images, which allows a better observation of temporally highly dynamic processes on the Earth’s surface, e.g., weather or active fire monitoring.\nMost satellite sensors have a temporal resolution of about 14 days. Anyway, by using a satellite constellation of multiple sensors identical in construction the time between two acquisitions can be shortened. For example Planet Labs operates five RapidEye satellites, which are synchronized so that they overlap in coverage.\nHowever, weather satellites are capable of acquiring images of the same area every 15 minutes. This can be explained by the different orbits of the satellites: geostationary and polar orbiting satellite systems.\n\n\nGeostationary orbiting satellite\n\nGeostationary sensors follow a circular geosynchronous orbit directly above the Earth’s equator. A geosynchronous system provides the same orbital period as the Earth’s rotation period (24 h), so it always looks at the same area on Earth, which it can observate at very high frequencies of several minutes. This rotation pattern is only possible at an altitude very close to 35.786 km, which generally results in a comparatively lower geometric resolution. Geostationary orbits are used by weather, communication and television satellites.\n\n\nPolar orbiting satellite\n\nPolar orbiting satellites pass above or nearly above the poles on each orbit, so the inclination to the Earth’s equator is very close to 90 degrees. They fly at an altitude of approximately 800-900 km. The lower a satellite flies, the faster it is. That’s why an orbit takes only ~90 minutes. While flying from the north to south pole in 45 minutes, sun-synchronous sensors look at the sunlit side of the Earth (descending images). Moving from south to nord pole results in nighttime imagery (ascending images). On an descending flight, all polar orbiting satellites cross the equator between 10:00 am and 10:15 am (local time) to provide maximum illumination and minimum water vapor to prevent haze and cloud build-up. Due to their inclination, they map the entire surface of the earth within several days (~14 d) as the earth continues to rotate beneath them. The temporal resolution of polar orbiting satellites usually describes the repetition rate at the Equator. The coverage gets better at higher latitudes due to the poleward convergence of the satellite orbits.",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Sensor basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/SensorBasics.html#non-imaging-vs-imaging-sensors",
    "href": "RESEDA/contents/SensorBasics.html#non-imaging-vs-imaging-sensors",
    "title": "Sensor Basics",
    "section": "",
    "text": "Non-imaging sensors, such as laser profilers or RADAR altimeter, usually provide point data, which do not offer spatially continuous information about how the input varies within the sensor’s field of view.\nBy contrast, imaging sensors are instruments that build up a spatially continuous digital image within their field of view, whereby they include not only information about the intensity of a given target signal, but also information about its spatial distribution. Examples include aerial photography, visible or near infrared scanner as well as synthetic aperture radars (SAR). All three satellite missions we will work with (Landsat 8, Sentinel 2 and Sentinel 1) are imaging sensors.",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Sensor basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/SensorBasics.html#passive-vs-active-sensors",
    "href": "RESEDA/contents/SensorBasics.html#passive-vs-active-sensors",
    "title": "Sensor Basics",
    "section": "",
    "text": "Anyway, there is a fundamental differentiation of remote sensing systems that we need to be aware of: passive and active sensors. This classification is based on the underlying recording principles, which are contrasted in the following:\n\n\nPassive sensor operating mode\n\nPassive sensors should be the more common of those two. Passive sensors measure solar light reflected or emitted from the Earth surfaces and objects.\nThese instruments primarily rely on short waved electromagnetic solar energy of the sun as the only source of radiation. Objects on the Earth’s surface react to this electromagnetic energy either with reflection, transmission, or absorption, depending on the composition of the object’s atoms. Passive sensors mainly capture the reflected proportion of the solar energy. Thus, they can only do their observation job when the sun is present as a radiation source – that is, only during the day. Anyway, since objects also partially absorb incoming solar light, there is a inherent radiation of these objects, which can be measured, for example, as thermal radiation.\nPassive remote sensing imagery can be very similar to how our human eyes perceive land cover, which makes it easier for us to interpret the image data.\nUnfortunately, there is one big limitation of passive systems: Due to the fact that the reflected electromagnetic radiation has to pass through the Earth’s atmosphere, the signal is strongly influenced by the weather and cloud conditions: Fog, haze and clouds render affected image information partially or completely useless, as electromagnetic energy is scattered by the large particles of dense clouds. This is where active sensors come into play.\n\n\nActive sensor operating mode\n\nActive sensors emit their own electromagnetic radiation to illuminate the object they observe.\nActive sensors send a pulse of energy at the speed of light to the Earth’s surface, that is reflected, refracted or scattered by the objects on the surface and the atmosphere. The recieved backscatter then gives information on land surface characteristics. There are many types of active sensors out there: RADAR (Radio Detection and Ranging), Scatterometer, LiDAR (Light Detection and Ranging), and Laser Altimeter. We will take a closer look at Sentintel 1, a SAR system (Synthetic Aperture RADAR), which is an imaging RADAR type working with microwaves. Images of active sensors are comparatively difficult to interpret: A SAR signal contains amplitude and phase information. Amplitude is the strength of the RADAR response and phase is the fraction of a full sine curve. In order to generate beautiful images out of these information, a more extensive preprocessing is often necessary, which we will do in SNAP.\nSince no natural light source is required, active sensors are capable to emit and capture their signals regardless of daytime. In addition, many systems operate in the electromagnetic domain of microwaves, so their wavelength is large enough to be unaffected by clouds and other atmospheric distortions.",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Sensor basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/SensorBasics.html#imaging-sensor-resolutions",
    "href": "RESEDA/contents/SensorBasics.html#imaging-sensor-resolutions",
    "title": "Sensor Basics",
    "section": "",
    "text": "When working with imaging remote sensing systems, a distinction is made between four different resolution terms: geometric, spectral, radiometric and temporal. It is essential to know those resolutions in order to be able to assess whether the sensor system is suitable for your research question!\n\n\nA digital image consists of at least one matrix of integers values – the picture elements, or pixels. Each pixel contains information about a signal response from a small area on the land surface, e.g., reflectance or backscatter. The geometric Resolution describes the edge length of this area (usually in meters) and is determined by the sensor’s instantaneous field of view (IFOV). It determines which object sizes can still be identified in the image – the degree of detail, so to speak. The effects of geometric resolutions becomes evident when comparing different images, for example an airborne orthophoto (0.1 m), Sentinel 2 (10 m) and Landsat 8 (30 m) images:\n\n\nResidential area in Friedenau, orthophoto (l), Sentinel 2 data (m), and Landsat 8 data (r)\n\nThere are several synonyms commonly used for the term geometric resolution, e.g., spatial resolution, pixel size, pixel edge length, or ground sampling distance.\n\n\n\nWe humans can only perceive the visible light around us, which is just a very small part of the available electromagnetic spectrum. Satellites, on the other hand, sense a much wider range of the electromagnetic spectrum and can provide us with information about processes that would otherwise be neglected.\nSpectral satellite sensors measure the reflection from the earth’s surface in different wavelength areas of the electromagnetic spectrum, via so-called channels or bands. Each band can have a different bandwidth, i.e., the area scanned within the electromagnetic spectrum. The sensors concentrate the signals gathered within a band to one (pixel-) value via a sensor specific filter function.\nHowever, spectral resolution describes the number of bands that the sensor senses. The purpose of multiple bands is to capture the differences in the reflection characteristics of different surfaces and materials within the electromagnetic spectrum.\nPanchromatic systems have only one spectral channel with a large bandwidth usually from 0.4 to 0.7 μm. Due to this large bandwidth there is enough energy available to achieve high geometric resolutions. Various satellites additionally provide such a panchromatic channel, e.g., Landsat program, Quickbird-2 pan, and IKONOS pan.\nMultispectral systems generally refers to 3 to 15 bands, which are usually located within the visible range (VIS, 0.4-0.7 μm), near-infrared (NIR, 0.75–1.4 μm), short-wavelength infrared (SWIR, 1.4–3 μm), mid-wavelength infrared (MWIR, 3–8 μm) and long wavelength/ thermal infrared (LWIR/TIR, 8–15 μm). Examples: Landsat program, Sentinel 2/3, SPOT, Quickbird, and RapidEye.\n\n\n\n\nSentinel 2 image, airport Schönefeld, Berlin; left: true color composite (RGB 4,3,2), right: pseudo-color composite (RGB 8,4,3)\n\nHyperspectral systems (spectrometer) offer hundreds or thousands of bands with narrow bandwidths. A spectral resolution this high gives ability to distinguish minor differences of land cover characteristics, which in turn provides ability to address issues that could not be solved with multispectral data, e.g., mineral or building material classifications. There are some airborne spectrometers (AVIRIS, HySPEX, HyMAP) and to date only one operational satellite: Hyperion. Anyway, more spaceborne systems are already in the starting blocks, e.g., EnMAP and HyspIRI.\n\n\n\nA digital sensor generally recognizes objects as intensity values, whereby it can only distinguish between dark and bright. The radiometric resolution refers to the ability to tell apart objects based on differences in those intensities. Thus, a sensor with a high radiometric resolution records more intensity levels or grey-scale levels. This property is expressed by the number of bits. Most satellite products have a bit depth of 8 to 16 bits, which means that they support 28(=256) or 216(=65536) different gray scale levels, respectively. A 1 bit image would be subject to a huge loss of information in comparison!\nThose effects are easier to understand when looking at the image comparison below: An image with a bit depth of 1 contains only two gray scale levels (21=2), i.e., black and white. Using 2 bits double the number of available colors (22=4), which allows a few more details (as shown in the middle image). When using 8 bits, the image can draw from a whole color ramp ranging from black to white (dark to bright) comprising 256 grey scale values (28=256).\n\n\nBlue band of an orthophoto in 1 bit (l), 2 bit (m), and 8 bit (r) representation\n\nHuman perception is barely sufficient to detect gray-scale differences beyond 8 bits in digital images. Nevertheless, machine learning algorithms often benefit from a finer differentiation of contrasts.\n\n\n\nThe temporal resolution of a sensor is simply the distance of time (usually in days) between two image acquisitions of the same area. A high temporal resolution thus indicates a smaller time window between two images, which allows a better observation of temporally highly dynamic processes on the Earth’s surface, e.g., weather or active fire monitoring.\nMost satellite sensors have a temporal resolution of about 14 days. Anyway, by using a satellite constellation of multiple sensors identical in construction the time between two acquisitions can be shortened. For example Planet Labs operates five RapidEye satellites, which are synchronized so that they overlap in coverage.\nHowever, weather satellites are capable of acquiring images of the same area every 15 minutes. This can be explained by the different orbits of the satellites: geostationary and polar orbiting satellite systems.\n\n\nGeostationary orbiting satellite\n\nGeostationary sensors follow a circular geosynchronous orbit directly above the Earth’s equator. A geosynchronous system provides the same orbital period as the Earth’s rotation period (24 h), so it always looks at the same area on Earth, which it can observate at very high frequencies of several minutes. This rotation pattern is only possible at an altitude very close to 35.786 km, which generally results in a comparatively lower geometric resolution. Geostationary orbits are used by weather, communication and television satellites.\n\n\nPolar orbiting satellite\n\nPolar orbiting satellites pass above or nearly above the poles on each orbit, so the inclination to the Earth’s equator is very close to 90 degrees. They fly at an altitude of approximately 800-900 km. The lower a satellite flies, the faster it is. That’s why an orbit takes only ~90 minutes. While flying from the north to south pole in 45 minutes, sun-synchronous sensors look at the sunlit side of the Earth (descending images). Moving from south to nord pole results in nighttime imagery (ascending images). On an descending flight, all polar orbiting satellites cross the equator between 10:00 am and 10:15 am (local time) to provide maximum illumination and minimum water vapor to prevent haze and cloud build-up. Due to their inclination, they map the entire surface of the earth within several days (~14 d) as the earth continues to rotate beneath them. The temporal resolution of polar orbiting satellites usually describes the repetition rate at the Equator. The coverage gets better at higher latitudes due to the poleward convergence of the satellite orbits.",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Sensor basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/SensorBasics.html#data-products",
    "href": "RESEDA/contents/SensorBasics.html#data-products",
    "title": "Sensor Basics",
    "section": "Data Products",
    "text": "Data Products\nLandsat 8 and 9 each acquire over 500 scenes each day. Most acquired scenes are downlinked to the Landsat Ground Network and made available for download within 24 hours after acquisition. Those scenes are usually uploaded to and stored in the USGS global archive. All Landsat standard data products are processed using the Landsat Product Generation System (LPGS) and come as compressed “.tgz”-files, which can be uncommpressed by using file archiver, such as 7-Zip or WinRAR. Once uncompressed, the image data is in GeoTIFF output format and projected to the Universal Transverse Mercator (UTM) map projection with the World Geodetic System 84 (WGS84) datum.\nThere are two main data products, which differ in the previous level of preprocessing: one without an atmospheric correction (Level-1) and one with (Level-2). Information on the processing level designations can be found in the metadata file (“.MTL.txt”) that is delivered with the Landsat 8 product.\n \n\nLevel-1 products\nStandard Landsat 8 data products consist of quantized and calibrated scaled Digital Numbers (DN) representing the multispectral image acquired by OLI and TIRS. They can be converted to Top Of Atmosphere (TOA) reflectance and radiance values by using radiometric rescaling coefficients as described in this USGS guide.\nLevel-1 Landsat scenes with the highest available data quality are declared as Tier 1 (L1TP) and are considered suitable for time-series analysis. Tier 1 includes data that have well-characterized radiometry and consistent georegistration and that is inter-calibrated across the different Landsat instruments.\n\nL1TP (Tier 1): This product offers radiometrically calibrated and orthorectified pixels by using auxiliary digital elevation models (DEM) and ground control points (GCP) to correct for relief displacement\nL1GT (Tier 2): worse, since GCP were not available\nL1GS (Tier 2): worst, since neither GCPs nor DEMs were available\n\nLevel-1 products contain the following 14 files once uncompressed:\n\nLevel-1 bands (1, 2, 3, 4, 5, 6, 7, 8, 9, 10 , and 11)\nQuality Assessment (QA) Band\nAngle Band Coefficients file\nMetadata text file (MTL.txt)\n\n \n\n\nLevel-2 products\nUSGS offers on-demand Surface Reflectance data products (Level-2). Surface Reflectance products provide an estimate of the surface spectral reflectance as it would be measured at ground level in the absence of atmospheric scattering or absorption. Surface reflectance values are scaled between 0 % and 100 %. This atmospheric correction is done via the Landsat Surface Reflectance Code (LaSRC), which utilizes the Landsat band 1, auxiliary MODIS data and radiative transfer models.\nKeep in mind: This product contain neither Level-1 TOA layers nor thermal bands!\nAlthough these data are also free, it may take several days for the processing to be completed by the servers of the Earth Resources Observation and Science (EROS) Center, before you are able to download them (read on: USGS Earth Explorer).\nLevel-2 products contain the following 13 files once uncompressed:\n\nLevel-2 bands (1, 2, 3, 4, 5, 6, and 7)\nRadiometric Saturation QA band (radsat_qa.tif)\nSurface Reflectance Aerosol QA band (sr_aerosol.tif)\nLevel-2 Pixel Quality Assessment band (pixel_qa.tif)\nSurface Reflectance metadata file (.xml)\nLevel-1 metadata file (MTL.txt)\nLevel-1 Angle Coefficient file (ANG.txt)\n\nIn general, we recommend you to use the Level 2 products, especially if you work multitemporally, since the radiometry of surface reflectances is more comparable between scenes.\nThe official product guide offers more information on the Level-1 and Level-2 Landsat products.",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Sensor basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/SensorBasics.html#data-products-1",
    "href": "RESEDA/contents/SensorBasics.html#data-products-1",
    "title": "Sensor Basics",
    "section": "Data Products",
    "text": "Data Products\nA single Sentinel 2 scene is huge. The swath width of Sentinel 2 is 290 km. So that the data can be handled at all, imagery is geometrically subdivided into rectangular tiles, or so-called granules. The Payload Data Ground Segment (PDGS) is responsible for the processing and archiving those granules. All products are innately projected to the Universal Transverse Mercator (UTM) coordinate system with the World Geodetic System 84 (WGS84) datum.\nAnd again, Sentinel 2 data goes through multiples processing levels:\n\nLevel-0: unprocessed instrument data at full resolution; telemetry analysis, preliminary cloud mask generation, and coarse coregistration\nLevel-1A: radiometric corrections and geometric viewing model refinement\nLevel-1B: resampling, conversion to reflectances and cloud/water/land mask generation\n\nMore information on pre-processing is given in the Sentinel 2 User Handbook.\nAnyway, Level-0, Level-1A and Level-1B products are PDGS-internal products not made available to users! When browsing the ESA SciHUB archive, you will be able to choose from Level-1C and Level-2A products:\nLevel-1C products are radiometric and geometric corrected top of atmosphere (TOA) data. This corrections include orthorectification and spatial registration on the UTM/WGS84 system with sub-pixel accuracy. Level-1C imagary is delivered in granules of 100×100 km of approximately 600-800 MB each. The individual spectral bands are present in their respective resolution (10, 20 and 60m), which is why resampling to a uniform geometric resolution is often necessary for further processing. A product consists of image data, available as JPEG2000 files, and the associated metadata, all capsuled within a “SAFE” file container. You will need the SNAP-software in order to read those SAFE-container (see chapter Visualize for a guide). The Sentinel-SAFE format wraps image data and product metadata in a specific folder structure. Do not alter this folder structure or any file names to ensure that image data and auxiliary information can be imported correctly.\nLevel-2A is the surface reflectance, or Bottom-Of-Atmosphere (BOA), product derived from a associated Level-1C product. This product is corrected by any distortion of atmosphere, terrain and cirrus clouds. Level-2A datasets come with some additional data layers: aerosol optical thickness-, water vapour, and scene classification maps and quality indicators, including cloud and snow probabilities. You can choose a resolution (10 m, 20 m, or 60 m) in which all channels are resampled uniformly.\nAnyway, if you already own a Level-1C scene, the conversion to Level-2A can also be done by yourself via the processor Sen2Cor. A guide on how to do this is given in chapter Classification in R.\nIn general, we recommend you to use the Level-2A products, especially if you work multitemporally, since the radiometry of surface reflectances is more comparable between scenes.",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Sensor basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/SensorBasics.html#data-products-2",
    "href": "RESEDA/contents/SensorBasics.html#data-products-2",
    "title": "Sensor Basics",
    "section": "Data Products",
    "text": "Data Products\nJust like Sentinel 2 data, Sentinel 1 ships its data in a zipped “SAFE” container format wrapping image data and product metadata in a specific folder structure. Do not alter this folder structure or any file names to ensure that image data and auxiliary information can be imported correctly. You do not need to unzip the file! The SNAP-software is able to read the file zipped.\nSentinel-1 data products which are generated by the Payload Data Ground Segment (PDGS) operationally are distributed at three levels of processing:\n\nLevel-0: compressed and unfocused SAR raw data, basis for all other high level products\nLevel-1: baseline product intended for most data users\nLevel-2: geolocated wind, wave and currents products derived from Level-1\n\nFor most applications, you should focus on Level-1 data. Additionally, Level-1 products can be one of two product types:\n\nSingle Look Complex (SLC) products are represented by a complex (I and Q) magnitude value and therefore contains both amplitude and phase information\nGround Range Detected (GRD) products consist of focused SAR data that has been detected, multi-looked and projected to ground range using an Earth ellipsoid model\n\nA more comprehensive explanation of data product is given in the Sentinel 1 user guide.",
    "crumbs": [
      "RESEDA",
      "Acquire",
      "Sensor basics"
    ]
  },
  {
    "objectID": "RESEDA/contents/Visualization.html",
    "href": "RESEDA/contents/Visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "Alright, you have some image data now. The next step is to get an overview of the radiometric and geometric properties of the imagery you own. The comparison of several data sets is especially comfortable in QGIS with its overlay functionalities. However, we also want to take a closer look at the visualization capacities of R in order to achieve an optimal image presentation.\nWe will do some visualization settings on a Landsat 9 Scene data downloaded in the chapter Acquire. So if you have not done the exercises yet, catch it up here.\n\nIn this section you will learn how to best represent image data in QGIS and R using different contrast stretchings and (false-)color composites.\nVisualization Basics\n– singleband vs multiband\n– different color composites and their advantages\n– histograms explained\nVisualize data in QGIS\n– import a L9 scene\n– compare singleband and multiband visualization\n– min/ max contrast stretching\nVisualize data in R/ R-Studio\n– import a L9 scene\n– compare singleband and multiband visualization\n– different contrast stretchings\n\n\n\nIn the following basic concepts and terms are discussed and a understanding for the following section will be developed.\n\n\n\n\nMost optical imaging satellite systems measure radiance in multiple spectral bands, including Landsat 8 and Sentinel 2 (see section Sensor Basics). As shown in the section Preprocess, these spectral bands are delivered as individual jpg- or tif-files within the data products. You can visualize each of those images separately by choosing a gray scale gradient (or any color slice). The following picture shows only the green band of an orthophoto of Berlin. Bright areas indicate high intensity of solar radiation reflected by the targets in the pixel, dark areas indicate low intensity:\n\n\n\nGreen band of an very high resolution orthophoto of Berlin in gray scale visualization\n\n\nSuch gray scale visualizations are useful for panchromatic bands, where only one band is available. However, this representation does not reflect what resembles the perception by the human eye. We are used to colorful pictures through our eyesight. On digital monitors, such as PC monitors, TV, or smartphones, this colorful display is achieved by mixing a number of different light colors, with shades of three primary colors: Red, Green and Blue. This method is called additive color mixing:\n\n\n\nAdditive color mixing: adding all three primary colors together yields white\n\n\nThe same principle is used in the representation of satellite data: In the case of a colored RGB representation we have three “slots”: one for red, one for green and one for blue. If we fill each of these slots with a spectral channel of our satellite product, we will create a RGB composite due to the mixture of three different matrices of pixel values, i.e., digital numbers (DNs):\n\n\n\nThree bands (grey scale singlebands) form a multiband (RGB composite)\n\n\nIn this example, color representation is what we would assume: trees are green, footpaths are gray, and roofs are brown. This representation is called true-color composite and is achieved when the blue slot is occupied by the blue spectral band of the satellite sensor, the green slot with the green band and the red slot with the red band (have a look at the bands of L8 and S2 again!). Since sensors have far more spectral bands, there is a wide range of possible band combinations, so called false-color, or pseudocolor composites. Over time, some of these combinations have proven useful for specific issues:\n\n\n\n\n\n\n\n\n\nBLUE\nGREEN\nRED\nPURPOSE\n\n\nblue\ngreen\nred\ntrue color composite, the “nature color” combination\n\n\nNIR\nred\ngreen\n“false-colour” combination for vegetation, appears in shades of red\n\n\nMIR\nNIR\ngreen\nprovides a “natural-like” rendition, penetrating atm. particles and smoke\n\n\nNIR\nMIR\nred\noffers added definition of land-water boundaries\n\n\nNIR\nMIR\nblue\ngeneral great amount of information and color contrast\n\n\n\n\n\n\n\n\nEach grayscale image consists of a 2-dimensional array, i.e, a matrix, of pixels that describe the intensities of the solar energy radiance measured by the sensor. These pixels are nothing more than integers and can be represented as such:\n\n\n\nDigital numbers within a small subset of the red band of an orthophoto\n\n\nThe occurrence of these integers can be counted and plotted. This representation is called a histogram and describes the frequency distribution of the gray level values in a single band image. A histogram shows the frequencies at the y-axis and the respective digital numbers (pixel values) at the x-axis:\n\n\n\nSchematic histogram\n\n\nHistograms of optical images are typically normally distributed and thus unimodal. The range of digital numbers (min & max in the figure above) depends on the individual radiometric resolution of the data set, e.g., Landsat 8 Level-1 and Sentintel 2 Level-1 data provide 0 – 65,535 DNs (16 bit unsigned) and Landsat 8 Level-2 data provide −32,768 – 32,767 DNs (16 bit signed).\nHowever, the complete range of theoretically possible digital numbers is rarely used by remote sensing imagery. As a consequence, images usually have a very low contrast when viewed in QGIS or R , whereby essential information remains hidden to us. The contrast of an image is a measure of its dynamic range, i.e., the spread of its histogram, and is calculated by the difference between maximum and minimum values of the image. A Minimum-Maximum contrast stretching increases the dynamic range by applying a linear scaling function that maps pixel values between the two extremes:\n\n\n\nGreen band of a orthophoto; schematic contrast stretching – left: original image view, right: enhanced contrast due to contrast stretching\n\n\nThe image now takes on the full dynamic range and subjectively looks better to the human eye. The actual values of the dataset remain unaffected by any contrast stretching – it is only a modification for visual purposes. An underlying color or lookup table translates the original DNs to the stretched DNs. In addition to the Minimum-Maximum method, there are other stretch procedures available, such as Standard Deviations, Percent Clip or Sigmoid.",
    "crumbs": [
      "RESEDA",
      "Preprocess",
      "Visualization"
    ]
  },
  {
    "objectID": "RESEDA/contents/Visualization.html#VisualBasics",
    "href": "RESEDA/contents/Visualization.html#VisualBasics",
    "title": "Visualization",
    "section": "",
    "text": "In the following basic concepts and terms are discussed and a understanding for the following section will be developed.\n\n\n\n\nMost optical imaging satellite systems measure radiance in multiple spectral bands, including Landsat 8 and Sentinel 2 (see section Sensor Basics). As shown in the section Preprocess, these spectral bands are delivered as individual jpg- or tif-files within the data products. You can visualize each of those images separately by choosing a gray scale gradient (or any color slice). The following picture shows only the green band of an orthophoto of Berlin. Bright areas indicate high intensity of solar radiation reflected by the targets in the pixel, dark areas indicate low intensity:\n\n\n\nGreen band of an very high resolution orthophoto of Berlin in gray scale visualization\n\n\nSuch gray scale visualizations are useful for panchromatic bands, where only one band is available. However, this representation does not reflect what resembles the perception by the human eye. We are used to colorful pictures through our eyesight. On digital monitors, such as PC monitors, TV, or smartphones, this colorful display is achieved by mixing a number of different light colors, with shades of three primary colors: Red, Green and Blue. This method is called additive color mixing:\n\n\n\nAdditive color mixing: adding all three primary colors together yields white\n\n\nThe same principle is used in the representation of satellite data: In the case of a colored RGB representation we have three “slots”: one for red, one for green and one for blue. If we fill each of these slots with a spectral channel of our satellite product, we will create a RGB composite due to the mixture of three different matrices of pixel values, i.e., digital numbers (DNs):\n\n\n\nThree bands (grey scale singlebands) form a multiband (RGB composite)\n\n\nIn this example, color representation is what we would assume: trees are green, footpaths are gray, and roofs are brown. This representation is called true-color composite and is achieved when the blue slot is occupied by the blue spectral band of the satellite sensor, the green slot with the green band and the red slot with the red band (have a look at the bands of L8 and S2 again!). Since sensors have far more spectral bands, there is a wide range of possible band combinations, so called false-color, or pseudocolor composites. Over time, some of these combinations have proven useful for specific issues:\n\n\n\n\n\n\n\n\n\nBLUE\nGREEN\nRED\nPURPOSE\n\n\nblue\ngreen\nred\ntrue color composite, the “nature color” combination\n\n\nNIR\nred\ngreen\n“false-colour” combination for vegetation, appears in shades of red\n\n\nMIR\nNIR\ngreen\nprovides a “natural-like” rendition, penetrating atm. particles and smoke\n\n\nNIR\nMIR\nred\noffers added definition of land-water boundaries\n\n\nNIR\nMIR\nblue\ngeneral great amount of information and color contrast\n\n\n\n\n\n\n\n\nEach grayscale image consists of a 2-dimensional array, i.e, a matrix, of pixels that describe the intensities of the solar energy radiance measured by the sensor. These pixels are nothing more than integers and can be represented as such:\n\n\n\nDigital numbers within a small subset of the red band of an orthophoto\n\n\nThe occurrence of these integers can be counted and plotted. This representation is called a histogram and describes the frequency distribution of the gray level values in a single band image. A histogram shows the frequencies at the y-axis and the respective digital numbers (pixel values) at the x-axis:\n\n\n\nSchematic histogram\n\n\nHistograms of optical images are typically normally distributed and thus unimodal. The range of digital numbers (min & max in the figure above) depends on the individual radiometric resolution of the data set, e.g., Landsat 8 Level-1 and Sentintel 2 Level-1 data provide 0 – 65,535 DNs (16 bit unsigned) and Landsat 8 Level-2 data provide −32,768 – 32,767 DNs (16 bit signed).\nHowever, the complete range of theoretically possible digital numbers is rarely used by remote sensing imagery. As a consequence, images usually have a very low contrast when viewed in QGIS or R , whereby essential information remains hidden to us. The contrast of an image is a measure of its dynamic range, i.e., the spread of its histogram, and is calculated by the difference between maximum and minimum values of the image. A Minimum-Maximum contrast stretching increases the dynamic range by applying a linear scaling function that maps pixel values between the two extremes:\n\n\n\nGreen band of a orthophoto; schematic contrast stretching – left: original image view, right: enhanced contrast due to contrast stretching\n\n\nThe image now takes on the full dynamic range and subjectively looks better to the human eye. The actual values of the dataset remain unaffected by any contrast stretching – it is only a modification for visual purposes. An underlying color or lookup table translates the original DNs to the stretched DNs. In addition to the Minimum-Maximum method, there are other stretch procedures available, such as Standard Deviations, Percent Clip or Sigmoid.",
    "crumbs": [
      "RESEDA",
      "Preprocess",
      "Visualization"
    ]
  },
  {
    "objectID": "RESEDA/contents/Visualization.html#import-a-dataset",
    "href": "RESEDA/contents/Visualization.html#import-a-dataset",
    "title": "Visualization",
    "section": "Import a Dataset",
    "text": "Import a Dataset\nFirst of all, open QGIS. QGIS is very similar to ArcGIS/ ArcMap, which you already know from the second semester (GIS course: “Geographische Informationssysteme”). As with all operations, there are several ways to open a raster dataset here: Either navigate via the main menu to Layer &gt; Add Layer &gt; Add Raster Layer…, or press the corresponding icon  in the toolbar or press the shortcut Ctrl + Shift + R to open a file explorer window.\n\n\n\nLocation of Add Raster Layer-function in QGIS\n\n\nIn the file explorer window, navigate to the data folder which holds your L8 data. In the meantime, due to preprocessing, you may already have quite a few files in this folder. Click on a tif-container (extension “.tif”) and then on Open.\n\n\n\nChoose a .tif-file for import in the file explorer\n\n\nIf you have clicked Open, your data shine in full glory for the first time in QGIS! The file name will automatically appear in the Layers panel and the image data will be visible in the Map View:\n\n\n\nImported L8 dataset\n\n\nNavigate through your data with the mouse buttons and the mouse wheel, or with the navigation tools  in the toolbar.",
    "crumbs": [
      "RESEDA",
      "Preprocess",
      "Visualization"
    ]
  },
  {
    "objectID": "RESEDA/contents/Visualization.html#singleband-visualization",
    "href": "RESEDA/contents/Visualization.html#singleband-visualization",
    "title": "Visualization",
    "section": "Singleband Visualization",
    "text": "Singleband Visualization\nBy default, QGIS maps the first three bands of a given rasterstack to the red, green and blue “slots” to create a color image. But we can also look at all the bands in the L8 layer stack individually. Open the Layer Properties dialog for the image layer by right-clicking on it in the Layer Panel and selecting Properties option or simply double-click the image layer. Switch to the Symbology tab and set Singleband gray in the drop down menu for the Render type option. You can choose the spectral channel you want to visualize by setting Gray band directly below. Save by clicking Apply and then OK:\n\n\n\nChoose Singleband gray for the Render type option and press OK\n\n\nWhoops! You will now see a totally gray rectangle which has no use at all. That is because we did not do any contrast stretching yet. We have to tell QGIS to scale the digital numbers to the whole bitspace (16 bit). In a grayscale visualization, this is black, white and all shades of gray in between. So open the Layer Properties dialog again, choose Stretch to MinMax as the contrast enhancement method. Unfold the Load min/max values section directly below. The Cumulative count cut setting helps to eliminate very low and very high digital numbers, e.g., as a result of clouds. The standard data range is set from 2% to 98% of the DNs and can be adapted manually. Now click on the Apply button and you will notice that values for minimum and maximum values are generated in the fields above:\n\nConfirm everything by clicking on OK after that. For the following picture we zoomed closer to the West of Berlin and its surrounding countryside:\n\n\n\nLandsat 8 band 1, West of Berlin, stretched based on global statistics\n\n\nThe city looks a bit oversaturated and offers little contrast. So far, we have used the information of the entire scene for contrast stretching. Since possible cloud fields and other edge areas of the scene are included in the calculation of the min/max values, it is advisable to zoom in on a cloud-free area and recalculate the values based on the currently visible canvas. So search for a cloud-free spot in your image, open the Layer Properties again and select Clip extent to canvas just below the Load button. Then click the Load button and OK:\n\n\n\nLandsat 8 band 1, West of Berlin, stretched based on local statistics\n\n\nOkay, but now it is time to bring some color into play.",
    "crumbs": [
      "RESEDA",
      "Preprocess",
      "Visualization"
    ]
  },
  {
    "objectID": "RESEDA/contents/Visualization.html#multiband-visualization",
    "href": "RESEDA/contents/Visualization.html#multiband-visualization",
    "title": "Visualization",
    "section": "Multiband Visualization",
    "text": "Multiband Visualization\nAnd again, open Layer Properties dialog for the L8 image layer by right-clicking on it in the Layer Panel and selecting Properties option or simply double-click the image layer. Now select Multiband color as the Render Type and select three spectral bands for the three RGB slots. First let us ave a look at the true color composite, which is band 4,3,2 for the RGB slots. Search for a cloud-free spot in your image, and select the remaining settings as described in the previous section as follows:\n\n\n\nLayer Properties settings for Multiband coloring\n\n\nThe resulting image is quite appealing, isn’t it? Feel free to play around with other band combinations!\n\n\n\nLandsat 9 true color composite (RGB: 4,3,2)\n\n\n\n\n\nLandsat 9 pseudocolor composite (RGB: 5,4,3)\n\n\n\n\n\nLandsat 9 pseudocolor composite (RGB: 6,5,3)\n\n\n\n\n\nLandsat 9 pseudocolor composite (RGB: 5,7,2)\n\n\nLook at the forest areas in the southwest of the clipping (Berlin, Grunewald). The trees look so much more differentiated in the NIR and MIR bands – what an information gain!",
    "crumbs": [
      "RESEDA",
      "Preprocess",
      "Visualization"
    ]
  },
  {
    "objectID": "RESEDA/contents/Validate.html",
    "href": "RESEDA/contents/Validate.html",
    "title": "Validate",
    "section": "",
    "text": "By now, you have generated your classification map, great! But can you rely on the map’s information? To underpin the meaningfulness of your results, a validation is needed.\nThe validation of remote sensing data is the last step in our workflow. The purpose of this chapter is to describe standard and advanced methods for validating a classification map.\n\nIn this chapter, the following content awaits you:\nValidation Intro\n– training, testing, validation – use the right terminology\n– best validation practice for remote sensing\nCreate Samples in R\n– stratified random sampling in R\n– generation and export of point coordinates as shapefile for usage in QGIS\nLabel Samples in QGIS\n– import of point shapefile\n– label points according to their class membership\n– use Landsat and very high resolution basemaps as validation basis for labeling\n– save labeled point shapefile\nAccuracy Statistics in R\n– generate a complete accuracy matrix in R\n– calculate confidence intervalls for overall accuracies\n– calculate kappa statistics\nArea Adjusted Accuracies\n– calculate area weighted accuracy statistics according to Olofsson et al. 2014\n\n.\nTraining dataset: A model is initially fit on a training sample dataset. The model iteratively learn from those training samples and tries to map data \\(x\\) to output response \\(y\\).\nTesting dataset: During training, algorithms often use a testing dataset for an unbiased evaluation of a model fit while tuning the model’s hyperparameter, e.g., \\(mtry\\) for RF, or \\(\\gamma\\) and \\(C\\) for SVM. The testing dataset is generated internally, e.g., in the form of OOB samples in RF, or cross validation in SVM.\nValidation dataset: Finally, a validation dataset is completely independent from the other two datasets and provides an unbiased evaluation of a model fit.\nAll right, so what is the best validation practice for remote sensing studies?\n\nautomatically create multiple point coordinates all over your study area or your classification extent\nmanually attribute the corresponding class labels to all of those point coordinates (labeling)\nstatistically examine the deviations and matches between the manually assigned class labels and the labels assigned by the classificator at any given point coordinates\n\nIn the following, we want to present a best practice workflow for a classification in detail.",
    "crumbs": [
      "RESEDA",
      "Validate",
      "Chapter in a box"
    ]
  },
  {
    "objectID": "RESEDA/contents/Validate.html#in-depth-guide",
    "href": "RESEDA/contents/Validate.html#in-depth-guide",
    "title": "Validate",
    "section": "In-depth Guide",
    "text": "In-depth Guide\nAll we need is the [raster` package, so make sure you’ve imported it.\n#install.packages(\"terra\")\nlibrary(terra)\nFurthermore, you should have your classification map ready (Chapter Analysis), lying in your working directory. Import it with the raster() function:\nsetwd(\"E:\\\\Work\")\nimg.classified &lt;- rast(\"classification_RF.tif\")\nThe raster provides a function called sampleStratified(), which does all the work for us:\nsmp.test &lt;- spatSample(x = classification, \n                       size = 50, \n                       method = \"stratified\", \n                       na.rm = TRUE, \n                       as.points = TRUE)\nThis function needs an Raster-Object as x argument and a positive integer value as size argument. Latter is the number of sample points per class. Additionally we can exclude all NA values, by setting na.rm = TRUE. NA Values can arise if your scene lies diagonally in space and points are placed in the border areas. Line 4 ensures, that the returned object is a SpatialPointDataFrame (which is easier to handle).\nWe can now check the class labels of our newly extracted validation points:\nsmp.test$classification_RF\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [71] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3\n## [106] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [141] 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n## [176] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5\n## [211] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n## [246] 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n## [281] 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\nNote: the name of the column within smp.test depends on the file name of your classification map.\nDue to the subsequent manual labeling in the next section, it makes sense to mix the samples so that the order is random. Again, we use the sample() function for this:\nsmp.test &lt;- smp.test[sample(1:nrow(smp.test)), ]\nBy looking at the class labels again, we see that the order is now random:\nsmp.test$classification_RF\n##   [1] 3 1 4 4 6 4 5 3 6 2 1 5 4 2 2 4 4 3 4 5 1 3 4 2 6 3 3 6 1 4 6 6 5 6 2\n##  [36] 3 1 1 3 3 1 2 5 4 3 4 2 4 1 5 1 1 4 1 3 5 2 1 5 4 5 2 3 6 2 1 5 3 5 2\n##  [71] 2 6 4 1 5 1 6 4 6 6 6 6 1 1 1 2 4 5 6 4 5 3 5 4 2 2 1 2 2 3 5 6 6 1 1\n## [106] 2 1 2 1 1 3 4 6 1 6 6 5 4 6 2 1 6 6 5 1 5 2 3 6 4 2 6 6 3 4 6 4 2 6 3\n## [141] 2 2 2 3 2 6 6 5 3 3 4 2 6 1 2 5 1 3 6 1 3 6 1 3 4 2 2 5 4 1 1 6 5 4 4\n## [176] 4 2 5 1 6 1 5 5 6 4 5 2 2 2 5 3 2 6 2 5 3 6 4 1 5 5 3 1 3 5 4 4 4 4 2\n## [211] 3 5 3 4 1 1 3 3 4 2 2 3 3 1 4 4 6 2 1 1 2 6 3 4 5 3 6 1 6 5 2 3 6 3 6\n## [246] 5 1 1 4 1 2 4 3 4 5 5 3 5 2 3 2 2 6 3 6 5 5 3 2 4 3 1 3 6 5 6 5 5 5 6\n## [281] 2 3 5 5 1 1 3 4 4 1 4 5 6 4 2 3 5 3 5 4\nIn addition, we can delete all variables in our dataframe smp.test and append a consecutive ID variable called ID, which will then be displayed to us in QGIS:\nsmp.test &lt;- smp.test[, -(1:2)]\nsmp.test$ID &lt;- 1:nrow(smp.test)\n\nprint(smp.test)\n## class       : SpatialPointsDataFrame \n## features    : 300 \n## extent      : 369870, 400650, 5812410, 5827500  (xmin, xmax, ymin, ymax)\n## coord. ref. : +proj=utm +zone=33 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 \n## variables   : 1\n## names       :  ID \n## min values  :   1 \n## max values  : 300\nTo visualize the distribution of our validation points, we can plot the SpatialPointDataFrame smp.test on top of our classification map in one plot:\nplot(classification, \n     axes = FALSE, \n     box = FALSE,\n     col = c(\"#fbf793\", \"#006601\", \"#bfe578\", \"#d00000\", \"#fa6700\", \"#6569ff\")\n)\n\npoints(smp.test)\n\nLast but not least, it is still necessary to save the SpatialPointDataFrame smp.test as a shapefile to your hard drive. This is also very easy with the shapefile() function from the raster package. Choose an appropriate filename = for the new shapefile created.\nwriteVector(smp.test, \"validation_RF.shp\", overwrite = TRUE)",
    "crumbs": [
      "RESEDA",
      "Validate",
      "Chapter in a box"
    ]
  },
  {
    "objectID": "RESEDA/contents/Validate.html#significance-test",
    "href": "RESEDA/contents/Validate.html#significance-test",
    "title": "Validate",
    "section": "Significance Test",
    "text": "Significance Test\nFurthermore, we can check if the result is purely coincidental, i.e., whether a random classification of the classes could have led to an identical result. We can use a binomial test for this. We only need two values for this test:\ntotal number of correctly classified validation points, and\nthe total number of validation points in our confusion matrix:\nsign &lt;- binom.test(x = sum(diag(accmat)),\n                   n = sum(accmat),\n                   alternative = c(\"two.sided\"),\n                   conf.level = 0.95\n                   )\n\npvalue &lt;- sign$p.value\npvalue\n## [1] 5.30128e-39\n\nCI95 &lt;- sign$conf.int[1:2]\nCI95\n## [1] 0.8187710 0.9006459\nThe p-value is much lower than 0.05, so the classification result is highly significant. If the classification were repeated under the same conditions, it can be assumed that the OA is 95% in the range of 81.2% to 90.0%.",
    "crumbs": [
      "RESEDA",
      "Validate",
      "Chapter in a box"
    ]
  },
  {
    "objectID": "RESEDA/contents/Validate.html#kappa",
    "href": "RESEDA/contents/Validate.html#kappa",
    "title": "Validate",
    "section": "Kappa",
    "text": "Kappa\nThe Kappa Coefficient can be used to evaluate the accuracy of a classification. It evaluates how well the classification performs compared to map, in which all values are just randomly assigned.\nThe Kappa coefficient can range from -1 to 1.\nA value of 0 indicates that the classification is as good as random values.\nA value below 0 indicates the classification is significantly worse than random.\nA value greater than 0 indicates that the classification is significantly better than random.\nWhen you have the accuracy matrix as a table \\(m_{i, j}\\) with \\(c\\) different classes, then Kappa is\n\\[\\kappa = \\frac{N_{o} – N_{e}}{N – N_{e}} , \\text{with} \\\\\n   N = \\sum_{i,j = 1}^{c} m_{i, j}\\\\\n   N_{o} = \\sum_{i=1}^c{m_{i, i}}\\\\\n   N_{e} = \\frac{1}{N}\\cdot\\sum_{l=1}^c\\left(\\sum_{j=1}^c{m_{l, j}} \\cdot \\sum_{i=1}^c{m_{i, l}}\\right)\n\\]\nThat looks pretty complicated. Using R, we can write our own function, which calculates \\(\\kappa\\) for us! The calculation also looks much friendlier:\nkappa &lt;- function(m) {\n  N &lt;- sum(m)\n  No &lt;- sum(diag(m))\n  Ne &lt;- 1 / N * sum(colSums(m) * rowSums(m))\n  return( (No - Ne) / (N - Ne) )\n}\nUse the accuracy matrix as argument for our new function to calculate the Kappa coefficient:\nkappacoefficient &lt;- kappa(accmat)\nkappacoefficient\n## [1] 0.8359646\nHowever, Kappas use has been questioned by many articles and is therefore not recommended (see Pontius Jr and Millones 2011).",
    "crumbs": [
      "RESEDA",
      "Validate",
      "Chapter in a box"
    ]
  },
  {
    "objectID": "RESEDA/contents/Regression-in-R.html",
    "href": "RESEDA/contents/Regression-in-R.html",
    "title": "Regression in R",
    "section": "",
    "text": "Very high-resolution reference data are usually difficult to obtain or only available for small areas of the study area. However, low-resolution data, such as Landsat 8 and Landsat 9 (30 m), are available in a high spatio-temporal resolution. Using a regression method, we can create sub-pixel information by relating the high-resolution information to very low-resolution Landsat 9 pixels.\nWe want to perform a Support Vector Regression in order to regress proportions of imperviousness for each Landsat 9 pixel in Berlin. For this we will use two data sets in this section:\n\na shapefile containing very high-resolution land cover information (including imperviousness), based on a digitized digital orthophoto of 2016 (Berlin Environmental Atlas)\na Landsat 9 acquisition (ID: LC09_L2SP_193023_20250328_20250329_02_T1), which you may already have acquired during the L8 & L9 Download Exercise\n\n\n\nLandsat 8 scene overlaid by the shapefile (colored by classes) in QGIS. The different detail levels become very clear in this visualization\n\n\nPrepare the samples for training – learn how to preprocess your shapefile\n– extract raster features and percentages of your target class (e.g., imperviousness)\n– create your training data set for regression analysis\nSVM Regression – learn how to perform a Support Vector Regression (SVR) in R using the e1071 package\n– predict the whole image data based on your regression model",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Regression in R"
    ]
  },
  {
    "objectID": "RESEDA/contents/Regression-in-R.html#in-depth-guide",
    "href": "RESEDA/contents/Regression-in-R.html#in-depth-guide",
    "title": "Regression in R",
    "section": "In-depth Guide",
    "text": "In-depth Guide\nIn order to use functionalities of the terra package, load it into your current session via library(). If you do not use our VM, you must first download and install the packages with install.packages():\n#install.packages(\"terra\")\nlibrary(terra)\nNext: set your working directory, in which all your image and shapefile data is stored by giving a character (do not forget the quotation marks \" \") variable to setwd(). Check your path with getwd() and the stored files in it via dir():\nsetwd(\"E:\\\\Work\")\n\ngetwd()\n## [1] \"E:\\\\Work\"\n\ndir()  \n##  [1] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.tif\"                                                \n##  [2] \"reg_train_data.dbf\"     \n##  [3] \"reg_train_data.prj\"\n##  [4] \"reg_train_data.qpj\"\n##  [5] \"reg_train_data.shp\" \n##  [6] \"reg_train_data.shx\" \nIf you do not get your files listed, there is an error in your working path – check again! Everything ready to go? Fine, then import your raster file as img and your shapefile as shp and have a look at them:\nimg &lt;- rast(\"E:\\\\Work\\\\LC09_L2SP_193023_20250328_20250329_02_T1_subset.tif\")\nimg\n## class       : SpatRaster \n## dimensions  : 504, 1030, 7  (nrow, ncol, nlyr)\n## resolution  : 30, 30  (x, y)\n## extent      : 369795, 400695, 5812395, 5827515  (xmin, xmax, ymin, ymax)\n## coord. ref. : EPSG:32633  (UTM Zone 33N, WGS84)\n\nshp &lt;- vect(\"E:\\\\Work\\\\reg_train_data\\\\reg_train_data.shp\")\nshp\n## class       : SpatVector \n## geometry    : polygons \n## features    : 286 \n## extent      : 390233.4, 390698.4, 5817147, 5817720  (xmin, xmax, ymin, ymax)\n## coord. ref. : EPSG:32633 \n\nsame.crs &lt;- crs(shp) == crs(img)\nprint(same.crs) # Should return TRUE if both CRS match\nBoth the brick() and shapefile() functions are provided by the raster package. As shown above, they create objects of the class RasterBrick and SpatialPolygonsDataFrame respectively. The L8 raster provides 7 bands, and our example shapefile 286 features, i.e., polygons. You can check whether the projections of the two datasets are identical or not by executing compareCRS(shp, img). If this is not the case (output equals FALSE), the raster package will automatically re-project your data on the fly later on. However, we recommend to adjust the projections manually in advance to prevent any future inaccuracies.\nPlot your data to make sure everything is imported properly (check Visualize in R for an intro to plotting):\nplot(shp)\n\nWith the argument add = TRUE in line 2 several data layers can be displayed one above the other:\nplot(img[[4]], col = gray.colors(100))\nplot(shp, add = TRUE, border = \"red\", lwd = 2)\n\nIn Line 1, initially only the fourth channel of the image img[[4] was displayed with a gray color representation using 100 shades col=gray.colors(100). The black spots in the upper plot represent the polygons of the shapefile. It becomes clear that they cover only a very small part of our study area.\nOptional: Let us take a look at the naming of the raster bands via the names() function. Those names can be quite bulky and cause problems in some illustrations when used as axis labels. You can easily rename it to something more concise by overriding the names with any string vector of the same length:\nnames(img)\n## [1] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.1\"\n## [2] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.2\"\n## [3] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.3\"\n## [4] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.4\"\n## [5] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.5\"\n## [6] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.6\"\n## [7] \"LC09_L2SP_193023_20250328_20250329_02_T1_subset.7\"\n\nnames(img) &lt;- c(\"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\", \"b7\")\n\nnames(img)\n## [1] \"b1\" \"b2\" \"b3\" \"b4\" \"b5\" \"b6\" \"b7\"\nIn order to accelerate all computation processes in the following, we first trim the image to the extent of the shapefile using the crop() function. It is often recommended to use a buffer of zero width wrapped around the polygons to avoid any topological errors in the upcoming process using gbuffer() as shown in line 1:\nshp &lt;- buffer(shp, width = 0)\nimg.subset &lt;- crop(img, shp)\nNow we just want to extract the pixels that are 100% covered by the polygons. Full coverage is important to be able to extract reliable predictions about the ratio of an target class. In order to assess this coverage, we use the rasterize() function with the argument getCover = TRUE to generate a mask. In line 2, we set all pixels which have less than 100% coverage to NA. In the end, we use the mask to subset our image data again using the mask() function:\nimg.mask &lt;- rasterize(shp, img.subset, field = NULL, touches = TRUE, cover = TRUE)\nimg.mask[img.mask &lt; 100] &lt;- NA\nimg.subset &lt;- mask(img.subset, img.mask)\nPlot the image data again with the polygons in order to see our achievements:\nplot(shp, col = NA, border = \"red\", lwd = 2)  \nplot(img.subset[[4]], add = TRUE)\n\nOnly the raster cells within our polygons are left – great!\nIn a next step, we transform those pixels to polygons, forming a grid with the information of the Landsat 8 bands maintained by using the rasterToPolygons() function. In addition, we want to assign each of these cells in the grid with an ID in order to address them (line 2):\ngrid &lt;- as.polygons(img.subset)\ngrid$ID &lt;- 1:nrow(grid)\n\nprint(grid)\n\n## class       : SpatialPolygonsDataFrame \n## features    : 93 \n## extent      : 390255, 390675, 5817165, 5817705  (xmin, xmax, ymin, ymax)\n## coord. ref. : +proj=utm +zone=33 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 \n## variables   : 8\n## names       :   b1,   b2,   b3,   b4,   b5,   b6,   b7, ID \n## min values  :  144,  198,  339,  279,  678,  563,  398,  1 \n## max values  : 1175, 1245, 1452, 1422, 3686, 2382, 2193, 93\n\nplot(grid, border = \"black\", lwd = 0.5)\n\nFinally, we have generated 93 cells (polygons) in this grid for which we now want to make a statement of how much of their areas is impervious, i.e., all polygons that have either building or impervious as Class_name attribute.\nFor this, we generate a new, empty data frame in which we write the training samples one after the other in line 1. We use a for-loop to iterate over each cell of the grid in line 2:\nsmp &lt;- data.frame()\n\nfor (i in 1:nrow(grid)) {\n  \n  cell &lt;- intersect(grid[i, ], shp)\n  \n  cell &lt;- cell[cell$Class_name == \"building\" | cell$Class_name == \"impervious\", ]\n  \n  if (nrow(cell) &gt; 0) {\n    areaPercent &lt;- sum(terra::expanse(cell) / terra::expanse(grid[i, ]))\n  } else {\n    areaPercent &lt;- 0\n  }\n  \n  newsmp &lt;- cbind(as.data.frame(grid[i, 1:nlyr(img)]), areaPercent)\n  \n  smp &lt;- rbind(smp, newsmp)\n}\n\nhead(smp)\n##      b1   b2   b3   b4   b5   b6  b7  areaPercent\n## 2  1175 1245 1452 1422 1742 1321 950 9.765630e-01\n## 3   497  597  736  767 1335 1175 947 7.986431e-01\n## 5   277  330  477  462 1221  960 743 3.669043e-01\n## 4   265  310  444  428 1077  789 545 1.727224e-01\n## 41  230  267  390  343  809  613 404 9.765625e-06\n## 42  168  217  339  312  678  563 398 3.540961e-02\nExplanation:\nLine 3: we select the ith cell of the grid and intersect this cell with the shp, which gives us the geometries and attributes of both the shapefile and the cell.\nLine 4: only the polygons belonging to the target classes were extracted, we discard all other polygons.\nLine 5: if a polygon of a target class exists in the cell, do everything inside the curly brackets. If there is no polygon in the cell, jump to line 7.\nLine 6: calculate the proportion of the area of each polygon and sum the proportions ([area(grid)\\[1\\]equals 900 m² in our example due to the geometric resolution).\\ Line 7: if there is no polygon of class *building* or *impervious*, setareaPercentto zero.\\ Line 10: use the column bind ([cbind()) function to combine the feature values (reflectances of Landsat image) and the areaPercent value to form a new sample newsmp.\nLine 10: use row bind ([rbind()) to add the new samplenewsmpto our training datasetsmp`.\nWe now prepared a training data set smp with all input features (Landsat 9 bands 1 to 7) and the percentage value of our target class (imperviousness).\nTime to train a Support Vector Machine in the next section!",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Regression in R"
    ]
  },
  {
    "objectID": "RESEDA/contents/Regression-in-R.html#in-depth-guide-1",
    "href": "RESEDA/contents/Regression-in-R.html#in-depth-guide-1",
    "title": "Regression in R",
    "section": "In-depth Guide",
    "text": "In-depth Guide\nIn order to be able to use the regression functions of the e1071 package, we must additionally load the library into the current session via library(). If you do not use our VM, you must first download and install the packages with install.packages():\n#install.packages(\"terra\")\n#install.packages(\"e1071\")\nlibrary(terra)\nlibrary(e1071)\nFirst, it is necessary to process the training samples in the form of a data frame. The necessary steps are shown in line 10-31 and described in detail in the previous section.\nAfter the preprocessing, we can train our Support Vector Regression with the training dataset smp. We will utilize an epsilon Support Vector Regressions, which requires three parameters: one gamma \\(\\gamma\\) value, one cost \\(C\\) value as well as a epsilon \\(\\varepsilon\\) value (for more details refer to the SVM section). These hyperparameters significantly determine the performance of the model. Finding the best hyparameters is not trivial and the best combination can not be determined in advance. Thus, we try to find the best combination iteratively by trial and error. Therefore, we create three vectors comprising all values that should be tested:\ngammas &lt;- 2^(-8:3)\ncosts &lt;- 2^(-5:8)\nepsilons &lt;- c(0.1, 0.01, 0.001)\nSo we have 14 different values for \\(\\gamma\\), 14 different values for \\(C\\) and three different values for \\(\\varepsilon\\). Thus, the whole training process is tested for 588 (14 * 14 * 3) models. Conversely, this means that the more parameters we check, the longer the training process takes.\nWe start the training with the tune() function. We need to specify the training samples as train.x, i.e., all columns of our smp dataframe except the last one, and the corresponding class labels as train.y, i.e. the last column of our smp dataframe, which holds the areaPercentageof our target class imperviousness:\nsvmgs &lt;- tune(svm,\n              train.x = smp[-ncol(smp)],\n              train.y = smp[ncol(smp)],\n              type = \"eps-regression\",\n              kernel = \"radial\", \n              scale = TRUE,\n              ranges = list(gamma = gammas, cost = costs, epsilon = epsilons),\n              tunecontrol = tune.control(cross = 5)\n)\nWe have to set the type of the SVM to \"eps-regression\" in line 4 in order to perform a regression task. Furthermore, we set the kernel used in training and predicting to a RBF kernel via \"radial\" (have a look at the SVM section for more details). We set the argument scale to TRUE in order to initiate the z-transformation of our data. The argument ranges in line 7 takes a named list of parameter vectors spanning the sampling range. We put our vectors gammas, costs and epsilons in this list. By using the tunecontrol argument in line 8, you can set k for the k-fold cross validation on the training data, which is necessary to assess the model performance.\nDepending on the complexity of the data, this step may take some time. Once completed, you can check the output by calling the resultant object name:\nsvmgs \n##\n## Parameter tuning of ‘svm’:\n##\n## - sampling method: 5-fold cross validation \n##\n## - best parameters:\n##       gamma cost epsolon\n##  0.00390625    8     0.1\n## \n## - best performance: 0.03157376 \nIn the course of the cross-validation, the overall accuracies were compared and the best parameters were determined: In our example, those are 0.0039, 8 and 0.1 for \\(\\gamma\\), \\(C\\) and \\(\\varepsilon\\), respectively. Furthermore, the Mean Absolute Error (MAE) of the best model is displayed, which is 0.0316, or 3.12%, in our case.\nWe can extract the best model out of our svmgs to use for image prediction:\nsvrmodel &lt;- svmgs$best.model\nsvrmodel \n## \n## Call:\n## best.tune(method = svm, train.x = smp[-ncol(smp)], train.y = smp[ncol(smp)], ranges = list(gamma = gammas, cost = costs, \n##     epsolon = epsilons), tunecontrol = tune.control(cross = 5), type = \"eps-regression\", kernel = \"radial\", scale = TRUE)\n## \n## \n## Parameters:\n##    SVM-Type:  eps-regression \n##  SVM-Kernel:  radial \n##        cost:  8 \n##       gamma:  0.00390625 \n##     epsilon:  0.1 \n## \n## \n## Number of Support Vectors:  78\nSave the best model by using the save() function. This function saves the model object svrmodel to your working directory, so that you have it permanently stored on your hard drive. If needed, you can load it any time with load().\nsave(svrmodel, file = \"svrmodel.RData\")\n#load(\"svrmodel.RData\")\nSince your SVR model is now completely trained, you can use it to predict all the pixels in your image. The command method predict() takes a lot of work from you: It is recognized that there is an image which will be processed pixel by pixel. As with the training pixels, each image pixel is now individually regressed and finally reassembled into your final regression image. Save the output as raster object result and have a look at its minimum and maximum values (line 11):\nresult &lt;- predict(img, svrmodel)\n\nresult\n## class       : RasterLayer \n## dimensions  : 504, 1030, 519120  (nrow, ncol, ncell)\n## resolution  : 30, 30  (x, y)\n## extent      : 369795, 400695, 5812395, 5827515  (xmin, xmax, ymin, ymax)\n## coord. ref. : +proj=utm +zone=33 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 \n## data source : in memory\n## names       : layer \n## values      : -0.7562021, 1.15675  (min, max)\nYou may will notice some “super-positive” (values above 1.0 or 100%) and/ or some negative (values below 0.0 or 0%) values. Such values are not uncommon, since the SVR implementation of the e1071 package is not subject to any non-negative constraints. You could simply fix this issue by overriding all adequate entries with meaningful minimum and maximum values (0 for 0% and 1 for 100%) by adding the following two lines:\nresult[result &gt; 1] &lt;- 1\nresult[result &lt; 0] &lt;- 0\n\nresult &lt;- ifel(result &gt; 1, 1, result)\nresult &lt;- ifel(result &lt; 0, 0, result)\nFinally, save your regression raster output using the writeRaster() function and plot your result in R:\nwriteRaster(result, filename=\"regression.tif\", overwrite=TRUE)\n\nplot(result, col=gray.colors(100))\n\nDone! You now got a map, which indicates the percentage of imperviousness, i.e., subpixel-information, for every single pixel in your image data.",
    "crumbs": [
      "RESEDA",
      "Analyse",
      "Regression in R"
    ]
  },
  {
    "objectID": "RESEDA/contents/QGIS.html",
    "href": "RESEDA/contents/QGIS.html",
    "title": "QGIS",
    "section": "",
    "text": "QGIS\nQGIS is a open-source geographic information system (GIS) for viewing, manipulating, and gathering spatial data and is licensed under the GNU General Public License. Essential features of the application are the broad support of common vector data and raster data, e.g., shapefiles and GeoTIFFs. There is an integration of sophisticated digitizing tools for creating vector data, as well as tools for a easy creation of cartographic maps, which you can use in publications and final papers.\nThe layout and appearance of QGIS is highly modifiable. The panels are arranged as follows:\n\n\nDefault layout in QGIS\n\n\n1 Main Menu and Toolbars: various toolbars available, e.g., project toolbar for QGIS project management, map navigation toolbar, attributes toolbar for feature selection and identification, layer management toolbar for adding new layers, and many more. Right click in the toolbar area in order to select/ deselect the individual toolbars and panels\n2 Layers Panel: shows all data layers in your current QGIS session. The data layers listed at the top are those that are visible in the data view on top. Rearange the order by left-clicking one layer and then dragging and dropping it to the desired location. You can hide individual layers by removing the checkmark next to it\n3 Browser Panel: lets you navigate in your filesystem and manage geodata. You can have access to common vector files, databases and WMS/WFS connections\n4 Map View: visualize all your vector and raster data loaded into your current session (layers panel)\n5 Status Bar: get all the information about the coordinates of where the mouse pointer is pointing, the scale, magnifier, rotation and the current coordinate system of your QGIS session\n\nSo why do we use QGIS over other geographic information systems? At first it does not cost a penny and supports all types of operating systems. This is why the community is big and there are lots of help and tutorials available online, e.g., the Official User Manual or internet forums, such as Stack Exchange. Furthermore it is actively developed by the QGIS Development Team and volunteer developers who regularly release updates and bug fixes.\nWe will mainly use QGIS in this online course to prepare classification maps accordingly in sections Visualization and Visualize in QGIS .",
    "crumbs": [
      "RESEDA",
      "Preparations",
      "QGIS"
    ]
  },
  {
    "objectID": "RESEDA/contents/The End!.html",
    "href": "RESEDA/contents/The End!.html",
    "title": "The End! {the-end}",
    "section": "",
    "text": "The End! {the-end}\nThank you for working through the RESEDA course. 🎉",
    "crumbs": [
      "RESEDA",
      "The End!",
      "The End!"
    ]
  },
  {
    "objectID": "RESEDA/index.html",
    "href": "RESEDA/index.html",
    "title": "Let’s start!",
    "section": "",
    "text": "Caution (June 2024): We are updating the tutorial step by step!",
    "crumbs": [
      "RESEDA",
      "Overview",
      "Let's start!"
    ]
  },
  {
    "objectID": "RESEDA/index.html#learning-objectives",
    "href": "RESEDA/index.html#learning-objectives",
    "title": "Let’s start!",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThis online course is divided into separate sections covering particular topics, which together provide a whole workflow commonly used for remote sensing imagery. Although the sections are built on one another and we recommend to handle them in given order, feel free to skip parts you are already comfortable with and focus on the chapters you are interested in. In almost every section you will come across exercises, e.g., multiple choice questionnaires or coding exercises to proof your comprehension of previous sections.\nLet us have a look at the learning objectives and checkpoints of each individual section:\nPrepare Yourself\ninstall our VirtualBox containing all required software get used to the GUIs of R-Studio, QGIS and SNAP refresh basics of the programming language R (if needed)\nAcquire Data\n\nrepeat basics of optical and radar imagery\nbecome familiar with online data provider and HUBs\nautomatically download many images using bulk downloads\n\nAnalyse Your Data\n\nrepeat basics of classification and regression tasks\nclassify image data in R with Random Forest and SVM\nvisualize results in R and QGIS\n\nValidate Results\n\nrepeat validation basics\nvalidate results in R with state of the art methods\n\nSAR Processing\n\nget deeper insights in SAR processing\nprocess Sentinel imagery in SNAP\nlearn to do InSAR and texture analysis\n\nWe wish you a lot of creative ideas, much findings and great results!\nBest regards, your FU Berlin Remote Sensing and Geoinformatics staff.",
    "crumbs": [
      "RESEDA",
      "Overview",
      "Let's start!"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/PartII.html#indexing-in-vectors",
    "href": "R-Crash-Course/contents/PartII.html#indexing-in-vectors",
    "title": "Part II",
    "section": "Indexing in vectors",
    "text": "Indexing in vectors\nTo address individual elements in our vector, we must “index” it. We index something in R by using square brackets [] . Counting starts at 1 – so we get the first element of the vector with [1]. If the index number is greater than the length of the vector, NA values are obtained.\nSince a vector is one-dimensional, we only need to write one number as an index between the square brackets in order to retrieve the respective entry. A minus sign in front of the index number leads to an exclusion of the respective entry:\nx &lt;- c(4, 2, 7, 9, 3) \nx\n## [1] 4 2 7 9 3\n\nx[1]           # first element\n## [1] 4\n\nx[-3]          # all but the third element\n## [1] 4 2 9 3\n\nx[2:4]         # all elements from second to fourth entry\n## [1] 2 7 9\n\nx[c(2, 5)]     # only the second and fifth element\n## [1] 2 3",
    "crumbs": [
      "R Crash Course",
      "Part II",
      "Vectors and factors"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/PartII.html#calculate-with-vectors",
    "href": "R-Crash-Course/contents/PartII.html#calculate-with-vectors",
    "title": "Part II",
    "section": "Calculate with vectors",
    "text": "Calculate with vectors\nNumerical vectors are calculated element by element. By multiplying a vector with the value 5, we again get a vector of the same length in which each element has been multiplied by 5:\nv1 &lt;- c( 1,  3,  5,  7)\n\nv1 * 5\n## [1]  5 15 25 35\n\nv1 + 100\n## [1] 101 103 105 107\nIf we have two vectors of the same length, we can compute them both element by element, so the first element of one vector is calculated with the first element of the other, and so on.\nv1 &lt;- c( 1,  3,  5,  7)\nv2 &lt;- c(20, 40, 60, 80)\n  \nv1 + v2\n## [1] 21 43 65 87\n\nv2 * v1\n## [1]  20 120 300 560\nHowever, if the two vectors are of different lengths, the shorter will be “recycled” until the length of the other vector is reached (“recycling rule”):\nv3 &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nv4 &lt;- c(10, 1)  \n\nv3 + v4\n##  [1] 11  3 13  5 15  7 17  9 19 11\nThere are many useful functions in R that speed up data analysis. Here are some functions that are already implemented in the base package and thus in every basic installation of R:\nv5 &lt;- c(2, 4, 6, 8, 1, 3)\n\nmean(v5)                    # arithmetisches Mittel des Vektors\n## [1] 4\n\nmedian(v5)                  # median\n## [1] 3.5\n\nmax(v5)                     # maximum value\n## [1] 8\n\nmin(v5)                     # minimum value\n## [1] 1\n\nsum(v5)                     # sum of all elements\n## [1] 24\n\nquantile(v5)                # quantiles\n##   0%  25%  50%  75% 100% \n## 1.00 2.25 3.50 5.50 8.00\n\nvar(v5)                     # variance\n## [1] 6.8\n\nsd(v5)                      # standard deviation\n## [1] 2.607681\n\nsort(v5, decreasing=TRUE)   # sort elements\n## [1] 8 6 4 3 2 1",
    "crumbs": [
      "R Crash Course",
      "Part II",
      "Vectors and factors"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/RExerciseIV.html",
    "href": "R-Crash-Course/contents/RExerciseIV.html",
    "title": "R – Exercise IV",
    "section": "",
    "text": "Create a data frame df that contains the following variables for at least four observations:\n\n\nname: name of at least four friends or acquaintances\nage: the age of those persons\nsize: the height of those persons in cm\ncity: Place of residence of those persons (city)\n\n\n\nClick here to see the answer\n\ndf &lt;- data.frame(\n  name = c(\"Anna\", \"Otto\", \"Natan\", \"Ede\"), \n  age  = c(66, 53, 22, 36),\n  size = c(170, 174, 182, 180),\n  city = c(\"Hamburg\", \"Berlin\", \"Berlin\", \"Cologne\")\n  )\n\ndf\n##    name age size    city\n## 1  Anna  66  170 Hamburg\n## 2  Otto  53  174  Berlin\n## 3 Natan  22  182  Berlin\n## 4   Ede  36  180 Cologne\n\n\nExamine the dimensionality, structure and statistical summary of your data frame df!\n\n\n\nClick here to see the answer\n\ndim(df)\n## [1] 4 4\n\nstr(df)\n## 'data.frame':    4 obs. of  4 variables:\n##  $ name: Factor w/ 4 levels \"Anna\",\"Ede\",\"Natan\",..: 1 4 3 2\n##  $ age : num  66 53 22 36\n##  $ size: num  170 174 182 180\n##  $ city: Factor w/ 3 levels \"Berlin\",\"Cologne\",..: 3 1 1 2\n\nsummary(df)\n##     name        age             size            city  \n##  Anna :1   Min.   :22.00   Min.   :170.0   Berlin :2  \n##  Ede  :1   1st Qu.:32.50   1st Qu.:173.0   Cologne:1  \n##  Natan:1   Median :44.50   Median :177.0   Hamburg:1  \n##  Otto :1   Mean   :44.25   Mean   :176.5              \n##            3rd Qu.:56.25   3rd Qu.:180.5              \n##            Max.   :66.00   Max.   :182.0\n\n\nIndex the second column with simple square brackets [] and save the output as df.subset! Which class does the output belong to?\n\n\n\nClick here to see the answer\n\ndf.subset &lt;- df[2]\n\ndf.subset\n##   age\n## 1  66\n## 2  53\n## 3  22\n## 4  36\n\nclass(df.subset)\n## [1] \"data.frame\"\n\n\nIndex the variable age with double square brackets [] and save the output as age.persons. Which class does the output belong to?\n\n\n\nClick here to see the answer\n\nage.persons &lt;- df[[\"alter\"]]\n\n# oder\n\nage.persons &lt;- df[[2]]\n\nage.persons\n## [1] 66 53 22 36\n\nclass(age.persons)\n## [1] \"numeric\"\n\n\nAdd the variable weight in kg to the data frame df!\n\n\n\nClick here to see the answer\n\ndf$weight &lt;- c(115, 110.2, 95, 87)\n\ndf\n##    name age size    city weight\n## 1  Anna  66  170 Hamburg  115.0\n## 2  Otto  53  174  Berlin  110.2\n## 3 Natan  22  182  Berlin   95.0\n## 4   Ede  36  180 Cologne   87.0\n\n\nAdd another observation (person) to your df!\n\n\n\nClick here to see the answer\n\nnew.person &lt;- data.frame(\n  \"name\"   = \"Anna\",\n  \"age\"    = 32,\n  \"size\"   = 174,\n  \"weight\" = 63,\n  \"city\"   = \"Hamburg\"\n)\n\ndf &lt;- rbind(df, new.person)\n\ndf\n##    name age size    city weight\n## 1  Anna  66  170 Hamburg  115.0\n## 2  Otto  53  174  Berlin  110.2\n## 3 Natan  22  182  Berlin   95.0\n## 4   Ede  36  180 Cologne   87.0\n## 5  Anna  32  174 Hamburg   63.0\n\n\nCalculate the mean value of the variable age and save the result as ages.mean!\n\n\n\nClick here to see the answer\n\nages.mean &lt;- mean(df$age)\n\n\nIndex all observations (persons) that are older than the average ages.mean!\n\n\n\nClick here to see the answer\n\ndf[df$age &gt; ages.mean, ]\n\n\nIndex all persons, which are lighter than 100 kg AND at least 180 cm tall!\n\n\n\nClick here to see the answer\n\ndf[df$weight = 180, ]\n##    name age size    city weight\n## 3 Natan  22  182  Berlin     95\n## 4   Ede  36  180 Cologne     87\n\n# or\n\nsubset(df, df$weight = 180)\n##    name age size    city weight\n## 3 Natan  22  182  Berlin     95\n## 4   Ede  36  180 Cologne     87",
    "crumbs": [
      "R Crash Course",
      "Part IV",
      "Excersice IV"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/PartIV.html#indexing-in-lists",
    "href": "R-Crash-Course/contents/PartIV.html#indexing-in-lists",
    "title": "Part IV",
    "section": "Indexing in lists",
    "text": "Indexing in lists\nUsing the respective index number or the assigned element name (if available), we can use a double square bracket [[]] to access the contents of the list. Using a simple square bracket, we would only get a part of the list here, which would still belong to class list:\nl[1]                        # first part of the list\n## $my.integer\n## [1] 13\nclass(l[1])\n## [1] \"list\"\n\nl[[1]]                      # extract first element (integer value)\n## [1] 13\nclass(l[[1]])\n## [1] \"integer\"\n\nl[[\"my.string\"]]           # extract element by its name\n## [1] \"Hello\"\n\nl[[3]]                      # extract third element (matrix)\n##      [,1] [,2] [,3]\n## [1,]    1    3    5\n## [2,]    2    4    6",
    "crumbs": [
      "R Crash Course",
      "Part IV",
      "Lists"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/PartIV.html#modify-lists",
    "href": "R-Crash-Course/contents/PartIV.html#modify-lists",
    "title": "Part IV",
    "section": "Modify lists",
    "text": "Modify lists\nLists can be expanded (assign a new index number or new element name to a value), and elements can be deleted (assign NULL) or overwrite individual list elements (reassign existing index or name):\nl[\"my.numeric\"] &lt;- 45.7325          # add new element to list\n\nl[1] &lt;- NULL                         # delete first element in list\n\nl[\"meinString\"] &lt;- \"World\"           # overwrite existing element",
    "crumbs": [
      "R Crash Course",
      "Part IV",
      "Lists"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/PartIV.html#indexing-in-data-frames",
    "href": "R-Crash-Course/contents/PartIV.html#indexing-in-data-frames",
    "title": "Part IV",
    "section": "Indexing in data frames",
    "text": "Indexing in data frames\nIn a data frame columns can be addressed either by the double square brackets [[]] by means of index numbers or directly by the name of the column (if available) by means of the dollar sign \\$. In addition, the rows or columns can be addressed adequately to a matrix by means of simple square brackets[]:\ndf\n##     name size weight\n## 1    Ben  185    110\n## 2  Hanna  166     60\n## 3   Paul  175     76\n## 4 Arthur  190     89\n\ndf[2]                                  # output column 2 as data frame\n##   size\n## 1  185\n## 2  166\n## 3  175\n## 4  190\n\ndf[[2]]                                # output as numeric\n## [1] 185 166 175 190\n\ndf$size                                # output as numeric\n## [1] 185 166 175 190\n\ndf[ , 2]                               # column output as numeric\n## [1] 185 166 175 190\n\ndf[1,  ]                               # row output as data frame\n##   name size weight\n## 1  Ben  185    110\n\ndf[1, 2]                               # element in row 1, col 2 as numeric\n## [1] 185\nVarious queries are also possible, for which we use the boolean operators:\ndf\n##     name size weight\n## 1    Ben  185    110\n## 2  Hanna  166     60\n## 3   Paul  175     76\n## 4 Arthur  190     89\n\ndf$size &gt; 170\n## [1]  TRUE FALSE  TRUE  TRUE\n\ndf[df$size &gt; 170, ]                     \n##     name size weight\n## 1    Ben  185    110\n## 3   Paul  175     76\n## 4 Arthur  190     89\n\ndf[df$size &gt; 180 & df$weight &lt; 100, ]        # AND condition\n##     name size weight\n## 4 Arthur  190     89\n\ndf[df$size &gt; 188 | df$weight &lt; 70, ]         # OR condition\n##     name size weight\n## 2  Hanna  166     60\n## 4 Arthur  190     89\n\ndf[df$name == \"Ben\" | df$name == \"Hanna\", ]  # OR condition\n##    name size weight\n## 1   Ben  185    110\n## 2 Hanna  166     60\nExplanation: For queries we use boolean operators. By the query in line 8 we get a boolean Vector, which contains a TRUE if the respective value of the Observation is greater than 170. We use this vector in line 11 to index the corresponding entries in the data frame (outputs all observations with a TRUE). When chaining conditions, either both conditions must be fulfilled at the same time by using AND &, or only one of both by using OR |.",
    "crumbs": [
      "R Crash Course",
      "Part IV",
      "Lists"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/PartIV.html#modify-data-frames",
    "href": "R-Crash-Course/contents/PartIV.html#modify-data-frames",
    "title": "Part IV",
    "section": "Modify data frames",
    "text": "Modify data frames\nOften it is necessary to delete data from a data frame or to implement additional entries later. For both tasks there are several possibilities in R. In the following two simple solutions:\ndf2 &lt;- df[ , -2]                                # delete column by index\ndf2\n##     name weight\n## 1    Ben    110\n## 2  Hanna     60\n## 3   Paul     76\n## 4 Arthur     89\n\ndf3 &lt;- subset(df, select = -c(weight, size))    # delete column by name\ndf3\n##     name\n## 1    Ben\n## 2  Hanna\n## 3   Paul\n## 4 Arthur\n\ndf4 &lt;- df[-3, ]                                 # delet row by index\ndf4\n##     name size weight\n## 1    Ben  185    110\n## 2  Hanna  166     60\n## 4 Arthur  190     89\n\ndf5 &lt;- subset(df, !name %in% c(\"Ben\", \"Hanna\")) # delete row by attribute\ndf5\n##     name size weight\n## 3   Paul  175     76\n## 4 Arthur  190     89\nExcluding columns via the column name is possible via the subset() function. Here we can use the argument -select= with a leading minus to specify the name of the column to be deleted (or a vector with c() for several columns at the same time). The ! symbol is a logical operator and negates a condition (see ? \"!\").\nThe addition of observations and variables is of course also possible:\ndf$gender = c(\"m\", \"w\", \"m\", \"m\")         # add a column (variable)\ndf\n##     name size weight gender\n## 1    Ben  185    110      m\n## 2  Hanna  166     60      w\n## 3   Paul  175     76      m\n## 4 Arthur  190     89      m\n\nnewdata &lt;- data.frame(\"name\" = 'Lisa',    # add a row (observation)\n                      \"size\" = 180,\n                      \"weight\" = 70,\n                      \"gender\" = \"w\"\n                      )\n\ndf &lt;- rbind(df, newdata)\ndf\n##     name size weight gender\n## 1    Ben  185    110      m\n## 2  Hanna  166     60      w\n## 3   Paul  175     76      m\n## 4 Arthur  190     89      m\n## 5   Lisa  180     70      w\nIf a new line has to be added, the new data must have the same structure as the existing data frame.\nTime for training session number IV:",
    "crumbs": [
      "R Crash Course",
      "Part IV",
      "Lists"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/PartV.html#the-if-statement",
    "href": "R-Crash-Course/contents/PartV.html#the-if-statement",
    "title": "Part V",
    "section": "The IF statement",
    "text": "The IF statement\nUse the if / else command to perform simple queries on all data types. The Microsoft Excel equivalent would be the “IF” feature. In R, the syntax is as follows:\nx &lt;- 3                                # define x\n\nx &lt;= 4                                # logical condition\n## [1] TRUE\n\nif (x &lt;= 4) {\n  print(\"x is smaller than or equal to 4!\")    \n} else {\n  print(\"x is larger than 4!\")        \n}\n## [1] \"x is smaller than or equal to 4!\"\nExplanation: An IF-command needs three parts: the keyword if(), a condition that results in a single logical output x \\&lt;= 4 and a block of code in curly braces {}, which is executed if the expression is TRUE. So, if the condition is TRUE, the code will run in curly brackets after the IF-command. If the condition is FALSE, the code block after the elseis executed.\nThe print() function outputs the strings in the parentheses to the console window. Here we need the print() function so that the output can be written out of the IF-function, similar to return(see part I. A vectorized (and therefore more efficient) notation is the following:\nx &lt;- c(3, 4, 5, 6, 7)                                \n\nifelse(x &lt;=4, \"x is smaller than or equal to 4!\", \"x is larger than 4!\")\n## `1] \"x is smaller than or equal to 4!\" \"x is smaller than or equal to 4!\"\n## `3] \"x is larger than 4!\"              \"x is larger than 4!\"             \n## `5] \"x is larger than 4!\"\nUsing ifelse(), the condition for each individual element is determined as a vector. This is helpful, e.g., if we want to categorize data!",
    "crumbs": [
      "R Crash Course",
      "Part V",
      "Missing values and control structures"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/PartV.html#the-for-loop",
    "href": "R-Crash-Course/contents/PartV.html#the-for-loop",
    "title": "Part V",
    "section": "The FOR loop",
    "text": "The FOR loop\nLoops are incredibly useful when certain tasks need to be repeated very often in the script. A for loop is based on an iterable variable of defined length. But what does that mean? We define any variable, e.g., i, with a start integer value, e.g, 1. We then increment this integer value until a second integer value, e.g. 8, is reached. This can be done via the sequence operator ::\nfor (i in 1:8) {\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] 6\n## [1] 7\n## [1] 8\nThe practical thing: The code in curly brackets is automatically executed once each time (eight times in total)! And we can meanwhile pick up the expression of our variable i, in order to print it or index a vector with it and much more:\nv &lt;- c(23, 54, 12, 59, 67, 45)    # create integer vector\n\nlength(v)                         # check length of vector\n## [1] 6\n  \nfor (i in 1:length(v)) {          # iterate length(v) times\n  print(v[i])\n}\n## [1] 23\n## [1] 54\n## [1] 12\n## [1] 59\n## [1] 67\n## [1] 45\nIt is also possible to set the variable / iterator equal to the elements of the vector instead of an integer value for indexing. The information in which run the loop is, however, is initially lost:\nv &lt;- c(\"R\", \"is\", \"still\", \"fun\")    \n\nfor (i in v) {\n  print(i)\n}\n## [1] \"R\"\n## [1] \"is\"\n## [1] \"still\"\n## [1] \"fun\"\nNow, have a look at exercise V:",
    "crumbs": [
      "R Crash Course",
      "Part V",
      "Missing values and control structures"
    ]
  },
  {
    "objectID": "R-Crash-Course/contents/RExerciseII.html",
    "href": "R-Crash-Course/contents/RExerciseII.html",
    "title": "R – Exercise II",
    "section": "",
    "text": "Create an integer variable e holding the value 42! Check the object class of e with class()!\n\n\n\nClick here to see the answer\n\ne &lt;- 42L\n\nclass(e)\n## [1] \"integer\"\n\n\nConvert e to the character data type with as.character()! Check the object class again!\n\n\n\nClick here to see the answer\n\ne &lt;- as.character(e)\n\nclass(e)\n## [1] \"character\"\n\n\nCreate a character vector friends with four names from your circle of friends or acquaintances!\n\n\n\nClick here to see the answer\n\nfriends &lt;- c(\"Anna\", \"Otto\", \"Natan\", \"Ede\")\n\nfriends\n## [1] \"Anna\"  \"Otto\"  \"Natan\" \"Ede\"\n\n\nIndex the second element from the vector friends!\n\n\n\nClick here to see the answer\n\nfriends[2]\n## [1] \"Otto\"\n\n\nReplace the first element of the vector friends with “Isolde” and check the updated vector again!\n\n\n\nClick here to see the answer\n\nfreunde[1] &lt;- \"Isolde\"\n\nfreunde\n## [1] \"Isolde\" \"Otto\"   \"Natan\"  \"Ede\"\n\n\nCreate a vector v1 from the following elements  1, \"Hello\", 2, \"World\" ! Check the object class!\n\n\n\nClick here to see the answer\n\nv1 &lt;- c(1, \"Hello\", 2, \"World\")\n\nv1\n## [1] \"1\"     \"Hello\" \"2\"     \"World\"\n\nclass(v1)\n## [1] \"character\"\n\n\nCreate a vector v2 with numerical values ​​(only integers) ranging from 4 to 10!\n\n\n\nClick here to see the answer\n\nv2 &lt;- c(4, 5, 6, 7, 8, 9, 10)\n\nv2\n## [1]  4  5  6  7  8  9 10\n\n# or use the sequence operator \":\"\n\nv2 &lt;- c(4:10)\n\nv2\n## [1]  4  5  6  7  8  9 10\n\n\nIndex the first three elements from v2!\n\n\n\nClick here to see the answer\n\nv2[1:3]\n## [1] 4 5 6\n\n# or:\n\nv2[ c(1, 2, 3) ]\n## [1] 4 5 6\n\n\nIndex all elements of v2 except the second element and save the result as v2.subset!\n\n\n\nClick here to see the answer\n\nv2.subset &lt;- v2[-2]\n\nv2.subset\n## [1]  4  6  7  8  9 10\n\n\nUse the length () function to find out the length of v2.subset (= the number of elements in the vector)!\n\n\n\nClick here to see the answer\n\nlength(v2.subset)\n## [1] 6\n\n\nCalculate the arithmetic mean of vector v2! In addition, determine the standard deviation of v2.subset!\n\n\n\nClick here to see the answer\n\n  mean(v2)\n  ## [1] 7\n\n  sd(v2.subset)\n  ## [1] 2.160247",
    "crumbs": [
      "R Crash Course",
      "Part II",
      "Excersice II"
    ]
  },
  {
    "objectID": "R-Crash-Course/index.html",
    "href": "R-Crash-Course/index.html",
    "title": "R Crash Course",
    "section": "",
    "text": "This is a R crash course for anyone who previously had no or very little contact with script-based programming. It should establish the basic understanding needed for upcoming chapters.\nYou are an old hand in R programming and do not need this introduction? – Skip this section and go explore the RESEDA course!\nIn order to deepen your knowledge, in particular with a view to statistical analysis, we recommend the advanced e-learning course SOGA (Softwaregestützte Geodatenanalyse). Of course, there are lots of tutorials and relevant literature out there, which should not go unmentioned here.\n\nCrash Course in a Box\nWe subdivided this crash course into several sections. At the end of most sections you will get to test your knowledge with coding exercises (E)!\n\nPart I + E I:\n\nPackage Management\nCalculate With R\nVariables\nFunctions\n\nPart II + E II:\n\nVectors\nFactors\n\nPart III + E III:\n\nMatrices\n\nPart IV + E IV:\n\nLists\n\nPart V + E V:\n\nMissing values\nControl structures",
    "crumbs": [
      "R Crash Course",
      "Overview",
      "R Crash Course"
    ]
  },
  {
    "objectID": "quarto-website-template-main/cv/index.html",
    "href": "quarto-website-template-main/cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "About/index.html",
    "href": "About/index.html",
    "title": "About",
    "section": "",
    "text": "Freie Universität Berlin – represented by the President –\n\n\n\nFreie Universität Berlin\nOffice of the President\nKaiserswerther Str. 16/18\n14195 Berlin\n\n\n\nDE 811304768\n\n\n\nM.Sc. Johannes Rosentreter (Web content development)\nE-mail: rosentreter.johannes@gmail.de\nPhone:\nDipl.-Ing. Kristin Fenske (Web content development)\nE-mail: kristin.fenske@fu-berlin.de\nPhone: (+49 30) 838 65326\nProf. Dr. Hannes Feilhauer (Content-related support)\nE-mail: hfeilhauer@zedat.fu-berlin.de\nPhone: (+49 30) 838 66172\nDr. Marion Stellmes (Content-related support)\nE-mail: marion.stellmes@fu-berlin.de\nPhone: (+49 30) 838 61978\nDipl. Geog. Rolf Rissiek (Laboratory assistance)\nE-mail: rolf.rissiek@fu-berlin.de\nPhone: (+49 30) 838 70263\n\n\n\nSee our Privacy Policy\n\n\n\nFreie Universität Berlin is a statutory body under public law in accordance with §§1 and 2 of the Berlin Law Relating to Institutions of Higher Learning (Berliner Hochschulgesetz, BerlHG). The web site of Freie Universität Berlin is subject to current copyright law. Please also take notice of our disclaimer."
  },
  {
    "objectID": "About/index.html#institution",
    "href": "About/index.html#institution",
    "title": "About",
    "section": "",
    "text": "Freie Universität Berlin – represented by the President –"
  },
  {
    "objectID": "About/index.html#address",
    "href": "About/index.html#address",
    "title": "About",
    "section": "",
    "text": "Freie Universität Berlin\nOffice of the President\nKaiserswerther Str. 16/18\n14195 Berlin"
  },
  {
    "objectID": "About/index.html#turnover-tax-id",
    "href": "About/index.html#turnover-tax-id",
    "title": "About",
    "section": "",
    "text": "DE 811304768"
  },
  {
    "objectID": "About/index.html#web-content-editors",
    "href": "About/index.html#web-content-editors",
    "title": "About",
    "section": "",
    "text": "M.Sc. Johannes Rosentreter (Web content development)\nE-mail: rosentreter.johannes@gmail.de\nPhone:\nDipl.-Ing. Kristin Fenske (Web content development)\nE-mail: kristin.fenske@fu-berlin.de\nPhone: (+49 30) 838 65326\nProf. Dr. Hannes Feilhauer (Content-related support)\nE-mail: hfeilhauer@zedat.fu-berlin.de\nPhone: (+49 30) 838 66172\nDr. Marion Stellmes (Content-related support)\nE-mail: marion.stellmes@fu-berlin.de\nPhone: (+49 30) 838 61978\nDipl. Geog. Rolf Rissiek (Laboratory assistance)\nE-mail: rolf.rissiek@fu-berlin.de\nPhone: (+49 30) 838 70263"
  },
  {
    "objectID": "About/index.html#privacy-statement",
    "href": "About/index.html#privacy-statement",
    "title": "About",
    "section": "",
    "text": "See our Privacy Policy"
  },
  {
    "objectID": "About/index.html#legal-form",
    "href": "About/index.html#legal-form",
    "title": "About",
    "section": "",
    "text": "Freie Universität Berlin is a statutory body under public law in accordance with §§1 and 2 of the Berlin Law Relating to Institutions of Higher Learning (Berliner Hochschulgesetz, BerlHG). The web site of Freie Universität Berlin is subject to current copyright law. Please also take notice of our disclaimer."
  }
]