[
  {
    "objectID": "staff/index.html",
    "href": "staff/index.html",
    "title": "Staff",
    "section": "",
    "text": "Fabian Faßnach\n\n\n\n\n\n\nRemote sensing & geoinformatics group\n\n\nMar 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarion Stellmes\n\n\n\n\n\n\nRemote sensing & geoinformatics group\n\n\nMar 9, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "RESEDA/contents/SNAP.html",
    "href": "RESEDA/contents/SNAP.html",
    "title": "SNAP",
    "section": "",
    "text": "SNAP\nThe SeNtinel Application Platform (SNAP) is an open source architecture for European Space Agency (ESA) toolboxes designed for exploitation of earth observation data under the the Scientific Exploitation of Operational Missions (SEOM) programme. SNAP is the common architecture for the Sentinel 1, 2 and 3 Toolboxes, which support the scientific exploitation for the ERS-ENVISAT missions, the Sentinel 1/2/3 missions and a range of national and third party missions. Those toolboxes contain the functionalities of previous toolboxes such as BEAM, NEST and Orfeo Toolbox. SNAP not only enables simple functionalities, such as opening and exploring data products, but also creating and computing complex, user-defined processing chains.\n\n\n\n\n\n\nDefault layout in SNAP\n\n\n1 Toolbar: Main menu and standard tools for saving/opening data products , navigation over image data , as well as point, line and polygon feature drawing tools . The main functionality is listed in the main menu under the menu items Raster, Optical and Radar. Those are also accessible via the graph builder , which we will use to automate and chain operations later on\n2 Product Explorer: lists all loaded data products along with their metadata and band information. The Pixel Info tab allows you to get all the information about the coordinates and raster values of where the mouse pointer is pointing. Double click a band in order to visualize it in the Image View panel, or right click a file and choose “open RGB image window” for a RGB composite\n3 Navigation Tool Window: The Navigation and World View tabs allow you to spatially locate the current image view in the overall picture and on a 3d globe. Furthermore the Color Manipulation tab allows visual adjustments via histogram stretching\n4 Product Library: browse and view metadata of your locally stored Sentinel data products or search for new data sets on the ESA SciHUB servers. Have a look at chapter ESA SciHUB for more\n5 Image View: data you have loaded in your current session can be visualized here. If multiple data sets are open, you can switch back and forth between them by clicking on the tabs at the top of the Image View or tile them vertically or horizontal by clicking on  in the toolbar (1).\n\nThere is an official science toolbox expoitation platform, STEP for short. On this website you can access useful video tutorials showing most of the basic functionalities of SNAP. Furthermore there is an official STEP-forum, in which you can communicate with the SNAP developers or ask the sience community.\nSNAP not only supports the Sentinel missions, but also a wide range of third party products, including optical data (Sentinel 2 Toolbox), e.g., RapidEye, SPOT, MODIS (Aqua and Terra), Landsat (TM), and SAR data (Sentinel 1 Toolbox), e.g., ERS-1/2, ENVISAT, ALOS PALSAR, TerraSAR-X, COSMO-SkyMed and RADARSAT-2."
  },
  {
    "objectID": "RESEDA/contents/R-Studio.html",
    "href": "RESEDA/contents/R-Studio.html",
    "title": "R-Studio",
    "section": "",
    "text": "R is a programming language and software environment designed for statistical computing and graphic data output, which is very popular among data scientists. While R is operated purely by command line, there are several graphical user interfaces available, such as Rattle, R Commander, Deducer, RKWard, JGR, and R-Studio.\nIn this online course we will use R-Studio as an integrated development environments (IDE). The default window layout of R-Studio is divided into four panes:\n\n\n\n\n\n\nDefault layout in R-Studio\n\n\n1 Source Pane: for writing, saving and sending R code to the console. This pane does not exist by default when you start R-Studio. It appears only when you open at least one R-script via File &gt; New File &gt; R script\n2 Console Pane: Code you enter here is immediately processed by R. This pane is used for interactively testing code line-by-line before you copy your final code to the Source pane (1) above\n3 Environment Pane: presents a list of information about all variables/ objects in your current R-Studio session. This list contains their class, dimensions and names. There is a history of all processed lines accessible if you click the History-tab\n4 Files Pane: contains several tabs for a file browser (Files), an essential help function (Help), the package manager (Packages), and viewer for plots (Plots) and interactive R output (Viewer)\n\nAlthough initially confusing, you will get used to each pane over time and quickly learn to switch between them to optimize your workflow when coding!\nR-Studio provides a variety of features that make life easier for you, e.g., color highlighting of syntax, code completion, find and replace functionality, import functions for data sets, and many more. Especially the Source pane allows to save sequences of commands for later re-use, enhancing reproducibility.\n \n\n\nThere are multiple ways to execute code in R-Studio. The simplest and most straight-forward way is to type your code directly in the Console pane and press enter key (or return key).\nIn order to execute a line of code from the Source pane, place your cursor in this line and press Ctrl+Enter or use the Run toolbar button (top right corner of the Source pane). R-Studio automatically advances the cursor to the next line, enabling a execution line-by-line.\nYou can execute multiple lines at once by selecting requested lines and pressing Ctrl+Enter. It is even possible to select parts of a line and execute only this selection. Press Ctrl+Shift+Enter or the Source toolbar button (top right corner of the Source pane) to execute the whole code inside your Source pane.\nIf you want to make use of previously executed commands, you can use the keyboard’s up and down arrow keys when in Console pane to access the last entries, modify them if you want, and then press Enter again.\n \n\n\n\nR provides a comprehensive built-in help system. At the command prompt in the Console pane of R-Studio type any of the following in order to view descriptions of any operators, functions and objects:\nhelp.start()       # general help\nhelp(foo)          # help about function foo\n?foo               # help about function foo\napropos(\"foo\")     # list all functions having \"foo\" in their names\nexample(foo)       # show an example of function foo\n \n\n\n\nThere are some time saving shortcuts implemented to manage your R scripts:\n\nCtrl+Shift+n – create a new R document\nCtrl+o – open a existing file\nCtrl+s – save your currently active file in the Source Pane\nCtrl+f – activate search/ replace functionality within Source Pane\nCtrl+Shift+c – comment/ uncomment selected parts of your code (commented code with a # at the beginning of a line will not be executed)\n\nYou can find all of those commands in the top toolbar of R-Studio as well."
  },
  {
    "objectID": "RESEDA/contents/R-Studio.html#execute",
    "href": "RESEDA/contents/R-Studio.html#execute",
    "title": "R-Studio",
    "section": "",
    "text": "There are multiple ways to execute code in R-Studio. The simplest and most straight-forward way is to type your code directly in the Console pane and press enter key (or return key).\nIn order to execute a line of code from the Source pane, place your cursor in this line and press Ctrl+Enter or use the Run toolbar button (top right corner of the Source pane). R-Studio automatically advances the cursor to the next line, enabling a execution line-by-line.\nYou can execute multiple lines at once by selecting requested lines and pressing Ctrl+Enter. It is even possible to select parts of a line and execute only this selection. Press Ctrl+Shift+Enter or the Source toolbar button (top right corner of the Source pane) to execute the whole code inside your Source pane.\nIf you want to make use of previously executed commands, you can use the keyboard’s up and down arrow keys when in Console pane to access the last entries, modify them if you want, and then press Enter again."
  },
  {
    "objectID": "RESEDA/contents/R-Studio.html#get-some-help",
    "href": "RESEDA/contents/R-Studio.html#get-some-help",
    "title": "R-Studio",
    "section": "",
    "text": "R provides a comprehensive built-in help system. At the command prompt in the Console pane of R-Studio type any of the following in order to view descriptions of any operators, functions and objects:\nhelp.start()       # general help\nhelp(foo)          # help about function foo\n?foo               # help about function foo\napropos(\"foo\")     # list all functions having \"foo\" in their names\nexample(foo)       # show an example of function foo"
  },
  {
    "objectID": "RESEDA/contents/R-Studio.html#some-useful-shortcuts",
    "href": "RESEDA/contents/R-Studio.html#some-useful-shortcuts",
    "title": "R-Studio",
    "section": "",
    "text": "There are some time saving shortcuts implemented to manage your R scripts:\n\nCtrl+Shift+n – create a new R document\nCtrl+o – open a existing file\nCtrl+s – save your currently active file in the Source Pane\nCtrl+f – activate search/ replace functionality within Source Pane\nCtrl+Shift+c – comment/ uncomment selected parts of your code (commented code with a # at the beginning of a line will not be executed)\n\nYou can find all of those commands in the top toolbar of R-Studio as well."
  },
  {
    "objectID": "RESEDA/contents/PrepareYourself.html",
    "href": "RESEDA/contents/PrepareYourself.html",
    "title": "Prepare Yourself",
    "section": "",
    "text": "Prepare Yourself\nThis chapter guides you on how to get all the necessary software you will need for this online course and introduce you to the fundamental layout of several useful programs. Read on to find out what it is all about!\n\n\nChapter in a Box\nIn this chapter, the following content awaits you:\nQGIS\n– introduction to QGIS user interface\nR-Studio\n– introduction to R-Studio user interface\n– instructions how to execute R commands in R-Studio\nSNAP\n– introduction to SNAP user interface"
  },
  {
    "objectID": "RESEDA/contents/Acquireyourdata.html",
    "href": "RESEDA/contents/Acquireyourdata.html",
    "title": "Acquire your data",
    "section": "",
    "text": "First of all you need two things: On the one hand, obviously enough, a problem that you want to answer with remote sensing tools and, on the other hand, data with which you can accomplish this.\nIn this chapter, you will get to know two optical (Landsat 8 & Sentinel 2) and one SAR satellites (Sentinel 1) in more detail. At the end of the chapter, you will know where and how to acquire individual or immense amounts of data.\n\n\nIn this chapter, the following content awaits you:\nIntro\n- get an overview of remote sensing applications and various satellite missions\nSensor Basics\n- repeat basic concepts of remote sensing imaging\n- deepen your knowledge about the four resolutions in RS\nLandsat Earthexplorer & BIG DATA Download\n- use USGS EarthExplorer for browsing and downloading Landsat imagery\n- learn how to download an impressive amount of Landsat data\nSentinel SciHUB & BIG DATA Download\n- use ESA SciHUB for browsing and downloading Sentinel 1 and 2 imagery\n- learn how to download an impressive amount of Sentinel data\n\n\n\nIn general, satellite remote sensing has enabled major advances in understanding global climate systems and its changes. Thus, most of the research questions are aimed at better understanding and quantifying climate change to some extent. The results are regularly compiled in the IPCC reports.\nHowever, the focus of our chair at the FU-Berlin is on the analysis of land cover and land use changes (LULC), which includes all subsystems of the earth and allows a wide range of questions.\n\nAgriculture\ncrop type classification, crop condition assessment, crop yield estimation & forecasting, precision farming, irrigation management\n\nSoil & Geology\nmapping of soil types, soil moisture, mapping of soil management practices, groundwater discharge, permafrost carbon storage, extracting mineral deposits, lithological and structural mapping, detect oil reserves, sediment transport modelling, dinosaur tracks\n\nForest & Vegetation\nspecies classification, deforestation processes, tree crown delineation, biomass, stress monitoring (infestations), carbon storage assessment, estimating forest supplies\n\nOceans & Water Bodies\nsea surface temperature, sea current monitoring, wave height, mean sea level delineation, water salinity, algae growth/ red tides, coral reefs, surface wind speed & direction, oil slicks, fishing activities, watershed delineation, wetland preserving\n\nUrban\nurbanization/ population growth, urban heat island, local climate zones, urban structure types, automatic road network delineation, solar panel energy optimization, monitor traffic jams, night time activity, locate construction alteration, ancient archaeological sites\n\nFire & Disasters\nactive fires, burned areas, fire severity, coal mine fires, volcanic ash monitoring, post earthquake or floods damage mapping, assessment of droughts, landslides, dust storms, tsunamis\n\nIce & Glacier\nmonitoring ice sheets and glaciers, glacier melting, snow melt runoff, carbon content assessement, ship tracking & routing\n\n\n\nThere is a tremendous amount of free data products (FREE) provided by various remote sensing missions. We will take a closer look at three of them in the upcoming sections (Landsat 8, Sentinel 2 and Sentinel 1).\nThen again, companies, as Digital Globe and Planet Labs, offer very high quality commercial data products (COMMercial) for large amounts of money (four-digit range or more). Nevertheless, such data is sometimes offered free of charge during Announcement of Opportunity (AO)-events or can be used for scientific purposes when you submit a project proposal (PROPosal) at any time. Additionally, it is worth to visit the mission websites, as there might be some selected sample data sets for free – but pay attention to the user agreements!\nIn the following an overview of the most popular imaging remote sensing programs is given. The corresponding data providers are linked. Each of these satellites has unique specifications and different mission goals and may well complement each other for your research.\n\n\n\nMISSION\nFREE\nCOMM\nPROP\nOPERATIONAL\nCOMMENT\n\n\nOPTICAL\n\n\n\n\n\n\n\nLandsat 5/7/8/9\n✔\nX\nX\n03/1984 – today\nUSGS Earth Explorer\n\n\nSentinel 2\n✔\nX\nX\n06/2015 – today\nESA SciHUB\n\n\nSentinel 3\n✔\nX\nX\n02/2016 – today\nESA SciHUB\n\n\nRapidEye\nX\n✔\n✔\n02/2009 - today\nPlanet EyeFind\n\n\nRADAR\n\n\n\n\n\n\n\nSentinel 1\n✔\nX\nX\n04/2014 - today\nESA SciHUB\n\n\nTerraSAR-X\nX\n✔\n✔\n06/2007 - today\nAIRBUS TSX Archive"
  },
  {
    "objectID": "RESEDA/contents/Acquireyourdata.html#chapter-in-a-box",
    "href": "RESEDA/contents/Acquireyourdata.html#chapter-in-a-box",
    "title": "Acquire your data",
    "section": "",
    "text": "In this chapter, the following content awaits you:\nIntro\n- get an overview of remote sensing applications and various satellite missions\nSensor Basics\n- repeat basic concepts of remote sensing imaging\n- deepen your knowledge about the four resolutions in RS\nLandsat Earthexplorer & BIG DATA Download\n- use USGS EarthExplorer for browsing and downloading Landsat imagery\n- learn how to download an impressive amount of Landsat data\nSentinel SciHUB & BIG DATA Download\n- use ESA SciHUB for browsing and downloading Sentinel 1 and 2 imagery\n- learn how to download an impressive amount of Sentinel data"
  },
  {
    "objectID": "RESEDA/contents/Acquireyourdata.html#remotesensingapp",
    "href": "RESEDA/contents/Acquireyourdata.html#remotesensingapp",
    "title": "Acquire your data",
    "section": "",
    "text": "In general, satellite remote sensing has enabled major advances in understanding global climate systems and its changes. Thus, most of the research questions are aimed at better understanding and quantifying climate change to some extent. The results are regularly compiled in the IPCC reports.\nHowever, the focus of our chair at the FU-Berlin is on the analysis of land cover and land use changes (LULC), which includes all subsystems of the earth and allows a wide range of questions.\n\nAgriculture\ncrop type classification, crop condition assessment, crop yield estimation & forecasting, precision farming, irrigation management\n\nSoil & Geology\nmapping of soil types, soil moisture, mapping of soil management practices, groundwater discharge, permafrost carbon storage, extracting mineral deposits, lithological and structural mapping, detect oil reserves, sediment transport modelling, dinosaur tracks\n\nForest & Vegetation\nspecies classification, deforestation processes, tree crown delineation, biomass, stress monitoring (infestations), carbon storage assessment, estimating forest supplies\n\nOceans & Water Bodies\nsea surface temperature, sea current monitoring, wave height, mean sea level delineation, water salinity, algae growth/ red tides, coral reefs, surface wind speed & direction, oil slicks, fishing activities, watershed delineation, wetland preserving\n\nUrban\nurbanization/ population growth, urban heat island, local climate zones, urban structure types, automatic road network delineation, solar panel energy optimization, monitor traffic jams, night time activity, locate construction alteration, ancient archaeological sites\n\nFire & Disasters\nactive fires, burned areas, fire severity, coal mine fires, volcanic ash monitoring, post earthquake or floods damage mapping, assessment of droughts, landslides, dust storms, tsunamis\n\nIce & Glacier\nmonitoring ice sheets and glaciers, glacier melting, snow melt runoff, carbon content assessement, ship tracking & routing"
  },
  {
    "objectID": "RESEDA/contents/Acquireyourdata.html#earth-observation-missions",
    "href": "RESEDA/contents/Acquireyourdata.html#earth-observation-missions",
    "title": "Acquire your data",
    "section": "",
    "text": "There is a tremendous amount of free data products (FREE) provided by various remote sensing missions. We will take a closer look at three of them in the upcoming sections (Landsat 8, Sentinel 2 and Sentinel 1).\nThen again, companies, as Digital Globe and Planet Labs, offer very high quality commercial data products (COMMercial) for large amounts of money (four-digit range or more). Nevertheless, such data is sometimes offered free of charge during Announcement of Opportunity (AO)-events or can be used for scientific purposes when you submit a project proposal (PROPosal) at any time. Additionally, it is worth to visit the mission websites, as there might be some selected sample data sets for free – but pay attention to the user agreements!\nIn the following an overview of the most popular imaging remote sensing programs is given. The corresponding data providers are linked. Each of these satellites has unique specifications and different mission goals and may well complement each other for your research.\n\n\n\nMISSION\nFREE\nCOMM\nPROP\nOPERATIONAL\nCOMMENT\n\n\nOPTICAL\n\n\n\n\n\n\n\nLandsat 5/7/8/9\n✔\nX\nX\n03/1984 – today\nUSGS Earth Explorer\n\n\nSentinel 2\n✔\nX\nX\n06/2015 – today\nESA SciHUB\n\n\nSentinel 3\n✔\nX\nX\n02/2016 – today\nESA SciHUB\n\n\nRapidEye\nX\n✔\n✔\n02/2009 - today\nPlanet EyeFind\n\n\nRADAR\n\n\n\n\n\n\n\nSentinel 1\n✔\nX\nX\n04/2014 - today\nESA SciHUB\n\n\nTerraSAR-X\nX\n✔\n✔\n06/2007 - today\nAIRBUS TSX Archive"
  },
  {
    "objectID": "R-Crash-Course/contents/RExerciseV.html",
    "href": "R-Crash-Course/contents/RExerciseV.html",
    "title": "R – Exercise V",
    "section": "",
    "text": "R – Exercise V\n\n\n\nUse ifelse() and your data frame df from exercise IV: If the person is less than or equal to 175 cm, it should have the attribute “small”, otherwise “tall”. Save the result in your df as the new column size.category.\n\nx &lt;- ifelse(df$size &lt;= 175, \"small\", \"tall\")\n\nx\n## [1] \"small\" \"small\" \"tall\" \"tall\" \"small\"\n\ndf$size.categorie &lt;- x\n\ndf\n##    name age size    city weight size.categorie\n## 1  Anna  66  170 Hamburg  115.0          small\n## 2  Otto  53  174  Berlin  110.2          small\n## 3 Natan  22  182  Berlin   95.0          tall\n## 4   Ede  36  180 Cologne   87.0          tall\n## 5  Anna  32  174 Hamburg   63.0          small\n\nWrite a loop that outputs all integers from 5 to 15!\n\nvektor &lt;- 5:15\nvektor\n##  [1]  5  6  7  8  9 10 11 12 13 14 15\n\nfor (i in vektor) {\n  print(i)\n}\n## [1] 5\n## [1] 6\n## [1] 7\n## [1] 8\n## [1] 9\n## [1] 10\n## [1] 11\n## [1] 12\n## [1] 13\n## [1] 14\n## [1] 15\n\nAdvanced: Create a for loop that outputs the arithmetic mean for each variable (column) of your data frame df – provided that the variable is numeric!\n\nfor (i in 1:ncol(df)) {\n  \n  if (class(df[[i]]) == \"numeric\") {\n    print(names(df)[i])\n    result &lt;- mean(df[[i]], na.rm=TRUE)\n    print(result)\n  }\n  \n}\n## [1] \"age\"\n## [1] 41.8\n## [1] \"size\"\n## [1] 176\n## [1] \"weight\"\n## [1] 94.04\n\n# Even if it looks complicated, take your time and go through it line by line. Everything should be known by now!"
  },
  {
    "objectID": "R-Crash-Course/contents/RExerciseIII.html",
    "href": "R-Crash-Course/contents/RExerciseIII.html",
    "title": "R – Exercise III",
    "section": "",
    "text": "R – Exercise III\n\n\n\nCreate a matrix named m1 with three rows and five columns and all the numeric (integer) values from 6 to 20!\n\nm1 &lt;- matrix(6:20, nrow = 3, ncol = 5)\n\nm1\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    6    9   12   15   18\n## [2,]    7   10   13   16   19\n## [3,]    8   11   14   17   20\n\nMultiply all elements in m1 by 0.5! Overwrite the matrix m1 with the result!\n\nm1 &lt;- m1 * 0.5\n\nCreate another matrix m2 with one row and five columns and all the numeric (integer) values from 1 to 5!\n\nm2 &lt;- matrix(1:5, nrow = 1, ncol = 5)\n\nm2\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n\nCalculate the sum of all elements in m2!\n\nsum(m2)\n## [1] 15\n\nCombine m1 and m2 with rbing(). Save the result as m3 and check the dimension of the new matrix!\n\nm3 &lt;- rbind(m1, m2)\n\nm3\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]  3.0  4.5  6.0  7.5  9.0\n## [2,]  3.5  5.0  6.5  8.0  9.5\n## [3,]  4.0  5.5  7.0  8.5 10.0\n## [4,]  1.0  2.0  3.0  4.0  5.0\n\ndim(m3)\n## [1] 4 5\n\nIndex the 5th column of m3!\n\nm3[ , 5]\n## [1]  9.0  9.5 10.0  5.0\n\nIndex the 2nd and 4th lines of m3!\n\nm3[ c(2, 4), ]\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]  3.5    5  6.5    8  9.5\n## [2,]  1.0    2  3.0    4  5.0\n\nCalculate the sums for all columns in m3!\n\ncolSums(m3)\n## [1] 11.5 17.0 22.5 28.0 33.5\n\nCalculate the standard deviation for the 3rd column in m3!\n\nsd( m3[ , 3] )\n## [1] 1.796988\n\nFrom m3, index the element in the 2nd column and 2nd line and all eight adjacent elements! Save the result as m4 and examine its object class!\n\nm4 &lt;- m3[2:4, 2:4]\n\nm4\n##      [,1] [,2] [,3]\n## [1,]  5.0  6.5  8.0\n## [2,]  5.5  7.0  8.5\n## [3,]  2.0  3.0  4.0\n\nclass(m4)\n## [1] \"matrix\""
  },
  {
    "objectID": "R-Crash-Course/contents/RExerciseI.html",
    "href": "R-Crash-Course/contents/RExerciseI.html",
    "title": "R – Exercise I",
    "section": "",
    "text": "R – Exercise I\n\n\nWelcome to your first training session!\nNo need to be nervous: this page contains not only tasks, but also their solutions as folded code elements. You can unfold these code blocks by simply clicking on them. Give it a try:\n# Well done!\n\n# Spoiler! -  you will find your answer here\n\nCreate a variable called a and assign the number 2017 to it!\n\n# use the \"&lt;-\" operator for variable assignments:\na &lt;- 2017\n\nCalculate the square root of 1089 and save the result in variable b!\n\n# use the built- in function \"sqrt()\" and the number 1089 as an argument:\nb &lt;- sqrt(1089)\n\nCalculate the sum of a and b!\n\n# done via standard operators:\na + b\n## [1] 2050\n\nOverwrite variable a by assigning the value 2018 to it!\n\n# simply assign a new value to an existing variable in order to overwrite it\na &lt;- 2018\n\nMake a copy of variable b and name it c!\n\n# variable assignment works from right (existing variable) to left (new variable):\nc &lt;- b\n\nCreate your own function called my.fun(), which requires three variables as input. The function should generate the square root of the product of all three variables and return one numeric value!\n\nmy.fun &lt;- function( var1, var2, var3 ) {\n  result &lt;- sqrt( var1 * var2 * var3 )\n  return(result)\n}\n\nUse a, b and c (from the previous tasks) as input into my.fun() and save the output to variable d! Check the resulting value!\n\nd &lt;- my.fun(a, b, c)\nd\n## [1] 1482.431"
  },
  {
    "objectID": "R-Crash-Course/contents/PartIV.html#indexing-in-lists",
    "href": "R-Crash-Course/contents/PartIV.html#indexing-in-lists",
    "title": "Part IV",
    "section": "Indexing in lists",
    "text": "Indexing in lists\nUsing the respective index number or the assigned element name (if available), we can use a double square bracket [[]] to access the contents of the list. Using a simple square bracket, we would only get a part of the list here, which would still belong to class list:\nl[1]                        # first part of the list\n## $my.integer\n## [1] 13\nclass(l[1])\n## [1] \"list\"\n\nl[[1]]                      # extract first element (integer value)\n## [1] 13\nclass(l[[1]])\n## [1] \"integer\"\n\nl[[\"my.string\"]]           # extract element by its name\n## [1] \"Hello\"\n\nl[[3]]                      # extract third element (matrix)\n##      [,1] [,2] [,3]\n## [1,]    1    3    5\n## [2,]    2    4    6"
  },
  {
    "objectID": "R-Crash-Course/contents/PartIV.html#modify-lists",
    "href": "R-Crash-Course/contents/PartIV.html#modify-lists",
    "title": "Part IV",
    "section": "Modify lists",
    "text": "Modify lists\nLists can be expanded (assign a new index number or new element name to a value), and elements can be deleted (assign NULL) or overwrite individual list elements (reassign existing index or name):\nl[\"my.numeric\"] &lt;- 45.7325          # add new element to list\n\nl[1] &lt;- NULL                         # delete first element in list\n\nl[\"meinString\"] &lt;- \"World\"           # overwrite existing element"
  },
  {
    "objectID": "R-Crash-Course/contents/PartIV.html#indexing-in-data-frames",
    "href": "R-Crash-Course/contents/PartIV.html#indexing-in-data-frames",
    "title": "Part IV",
    "section": "Indexing in data frames",
    "text": "Indexing in data frames\nIn a data frame columns can be addressed either by the double square brackets [[]] by means of index numbers or directly by the name of the column (if available) by means of the dollar sign \\$. In addition, the rows or columns can be addressed adequately to a matrix by means of simple square brackets[]:\ndf\n##     name size weight\n## 1    Ben  185    110\n## 2  Hanna  166     60\n## 3   Paul  175     76\n## 4 Arthur  190     89\n\ndf[2]                                  # output column 2 as data frame\n##   size\n## 1  185\n## 2  166\n## 3  175\n## 4  190\n\ndf[[2]]                                # output as numeric\n## [1] 185 166 175 190\n\ndf$size                                # output as numeric\n## [1] 185 166 175 190\n\ndf[ , 2]                               # column output as numeric\n## [1] 185 166 175 190\n\ndf[1,  ]                               # row output as data frame\n##   name size weight\n## 1  Ben  185    110\n\ndf[1, 2]                               # element in row 1, col 2 as numeric\n## [1] 185\nVarious queries are also possible, for which we use the boolean operators:\ndf\n##     name size weight\n## 1    Ben  185    110\n## 2  Hanna  166     60\n## 3   Paul  175     76\n## 4 Arthur  190     89\n\ndf$size &gt; 170\n## [1]  TRUE FALSE  TRUE  TRUE\n\ndf[df$size &gt; 170, ]                     \n##     name size weight\n## 1    Ben  185    110\n## 3   Paul  175     76\n## 4 Arthur  190     89\n\ndf[df$size &gt; 180 & df$weight &lt; 100, ]        # AND condition\n##     name size weight\n## 4 Arthur  190     89\n\ndf[df$size &gt; 188 | df$weight &lt; 70, ]         # OR condition\n##     name size weight\n## 2  Hanna  166     60\n## 4 Arthur  190     89\n\ndf[df$name == \"Ben\" | df$name == \"Hanna\", ]  # OR condition\n##    name size weight\n## 1   Ben  185    110\n## 2 Hanna  166     60\nExplanation: For queries we use boolean operators. By the query in line 8 we get a boolean Vector, which contains a TRUE if the respective value of the Observation is greater than 170. We use this vector in line 11 to index the corresponding entries in the data frame (outputs all observations with a TRUE). When chaining conditions, either both conditions must be fulfilled at the same time by using AND &, or only one of both by using OR |."
  },
  {
    "objectID": "R-Crash-Course/contents/PartIV.html#modify-data-frames",
    "href": "R-Crash-Course/contents/PartIV.html#modify-data-frames",
    "title": "Part IV",
    "section": "Modify data frames",
    "text": "Modify data frames\nOften it is necessary to delete data from a data frame or to implement additional entries later. For both tasks there are several possibilities in R. In the following two simple solutions:\ndf2 &lt;- df[ , -2]                                # delete column by index\ndf2\n##     name weight\n## 1    Ben    110\n## 2  Hanna     60\n## 3   Paul     76\n## 4 Arthur     89\n\ndf3 &lt;- subset(df, select = -c(weight, size))    # delete column by name\ndf3\n##     name\n## 1    Ben\n## 2  Hanna\n## 3   Paul\n## 4 Arthur\n\ndf4 &lt;- df[-3, ]                                 # delet row by index\ndf4\n##     name size weight\n## 1    Ben  185    110\n## 2  Hanna  166     60\n## 4 Arthur  190     89\n\ndf5 &lt;- subset(df, !name %in% c(\"Ben\", \"Hanna\")) # delete row by attribute\ndf5\n##     name size weight\n## 3   Paul  175     76\n## 4 Arthur  190     89\nExcluding columns via the column name is possible via the subset() function. Here we can use the argument -select= with a leading minus to specify the name of the column to be deleted (or a vector with c() for several columns at the same time). The ! symbol is a logical operator and negates a condition (see ? \"!\").\nThe addition of observations and variables is of course also possible:\ndf$gender = c(\"m\", \"w\", \"m\", \"m\")         # add a column (variable)\ndf\n##     name size weight gender\n## 1    Ben  185    110      m\n## 2  Hanna  166     60      w\n## 3   Paul  175     76      m\n## 4 Arthur  190     89      m\n\nnewdata &lt;- data.frame(\"name\" = 'Lisa',    # add a row (observation)\n                      \"size\" = 180,\n                      \"weight\" = 70,\n                      \"gender\" = \"w\"\n                      )\n\ndf &lt;- rbind(df, newdata)\ndf\n##     name size weight gender\n## 1    Ben  185    110      m\n## 2  Hanna  166     60      w\n## 3   Paul  175     76      m\n## 4 Arthur  190     89      m\n## 5   Lisa  180     70      w\nIf a new line has to be added, the new data must have the same structure as the existing data frame.\nTime for training session number IV:"
  },
  {
    "objectID": "R-Crash-Course/contents/PartII.html#indexing-in-vectors",
    "href": "R-Crash-Course/contents/PartII.html#indexing-in-vectors",
    "title": "Part II",
    "section": "Indexing in vectors",
    "text": "Indexing in vectors\nTo address individual elements in our vector, we must “index” it. We index something in R by using square brackets [] . Counting starts at 1 – so we get the first element of the vector with [1]. If the index number is greater than the length of the vector, NA values are obtained.\nSince a vector is one-dimensional, we only need to write one number as an index between the square brackets in order to retrieve the respective entry. A minus sign in front of the index number leads to an exclusion of the respective entry:\nx &lt;- c(4, 2, 7, 9, 3) \nx\n## [1] 4 2 7 9 3\n\nx[1]           # first element\n## [1] 4\n\nx[-3]          # all but the third element\n## [1] 4 2 9 3\n\nx[2:4]         # all elements from second to fourth entry\n## [1] 2 7 9\n\nx[c(2, 5)]     # only the second and fifth element\n## [1] 2 3"
  },
  {
    "objectID": "R-Crash-Course/contents/PartII.html#calculate-with-vectors",
    "href": "R-Crash-Course/contents/PartII.html#calculate-with-vectors",
    "title": "Part II",
    "section": "Calculate with vectors",
    "text": "Calculate with vectors\nNumerical vectors are calculated element by element. By multiplying a vector with the value 5, we again get a vector of the same length in which each element has been multiplied by 5:\nv1 &lt;- c( 1,  3,  5,  7)\n\nv1 * 5\n## [1]  5 15 25 35\n\nv1 + 100\n## [1] 101 103 105 107\nIf we have two vectors of the same length, we can compute them both element by element, so the first element of one vector is calculated with the first element of the other, and so on.\nv1 &lt;- c( 1,  3,  5,  7)\nv2 &lt;- c(20, 40, 60, 80)\n  \nv1 + v2\n## [1] 21 43 65 87\n\nv2 * v1\n## [1]  20 120 300 560\nHowever, if the two vectors are of different lengths, the shorter will be “recycled” until the length of the other vector is reached (“recycling rule”):\nv3 &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nv4 &lt;- c(10, 1)  \n\nv3 + v4\n##  [1] 11  3 13  5 15  7 17  9 19 11\nThere are many useful functions in R that speed up data analysis. Here are some functions that are already implemented in the base package and thus in every basic installation of R:\nv5 &lt;- c(2, 4, 6, 8, 1, 3)\n\nmean(v5)                    # arithmetisches Mittel des Vektors\n## [1] 4\n\nmedian(v5)                  # median\n## [1] 3.5\n\nmax(v5)                     # maximum value\n## [1] 8\n\nmin(v5)                     # minimum value\n## [1] 1\n\nsum(v5)                     # sum of all elements\n## [1] 24\n\nquantile(v5)                # quantiles\n##   0%  25%  50%  75% 100% \n## 1.00 2.25 3.50 5.50 8.00\n\nvar(v5)                     # variance\n## [1] 6.8\n\nsd(v5)                      # standard deviation\n## [1] 2.607681\n\nsort(v5, decreasing=TRUE)   # sort elements\n## [1] 8 6 4 3 2 1"
  },
  {
    "objectID": "quarto-website-template-main/cv/index.html",
    "href": "quarto-website-template-main/cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "projects/posts/welcome/index.html",
    "href": "projects/posts/welcome/index.html",
    "title": "MOFA",
    "section": "",
    "text": "Monitoring Farmland Abandonment by multitemporal and multisensor remote sensing imagery (MOFA)\n\nThe research project studies an area in the border region of Poland and Ukraine. With the fall of the Iron Curtain the region experienced drastic changes in political and socio- economic structures. Large farmland areas become abandoned and gradual processes of forest succession take place on the abandoned land. The aim of the project is the development of adequate strategies to monitor farmland abandonment, using multitemporal SAR and multispectral remote sensing data.\n\n\n\nFinally enhanced maps should be provided, which enable more detailed analysis of the gradual process of land cover transitions. (PI: B. Waske, funding: German Research Foundation (DFG) WA 2728/2-1)\nThus, remote sensing based mapping offers great opportunities to map these phenomena and ultimately to better understand patterns, processes and underlying causes. Existing studies are based on multispectral imagery, abandonment maps, however, are difficult to obtain due to spectral ambiguities, phenological variability and limited data availability. SAR data can overcome these problems and different remote sensing studies demonstrated the potential of multisensor imagery. The aim of the project is the development of adequate strategies to monitor farmland abandonment, using multitemporal SAR and multispectral remote sensing data. Finally enhanced maps should be provided, which enable more detailed analysis of the gradual process of land cover transitions.\nThe study site within the border region of Poland and Ukraine (© Google, 2012)\n\n\nProject Duration: 01/2012 - 12/2014\nPrincipal Investigator: Björn Waske\nProjects staff: Jan Stefanski (University Bonn)\nProject partners:\nProf. Dr. Tobias Kümmerle, Biogeography and Conservation Biology Group, Humboldt-University of Berlin\nDr. Oleh Chaskovskyy, Ukrainian National Forestry University, Lviv (Ukraine)\nDr. hab. Jacek Kozak, Department of GIS, Cartography and Remote Sensing, Jagiellonian University, Kraków (Poland)\nFunding: DFG - German Research Foundation (WA 2728/2-1)"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Research projects",
    "section": "",
    "text": "Future Forest\n\n\n\n\n\n\n\nOngoing\n\n\nsentinel-2\n\n\nforest damage\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2024\n\n\nRemote sensing & geoinformatics group\n\n\n\n\n\n\n  \n\n\n\n\nMOFA\n\n\n\n\n\n\n\nCompleted\n\n\nmultitemporal\n\n\nmultisensor\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2014\n\n\nBjörn Waske\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "About/index.html",
    "href": "About/index.html",
    "title": "About",
    "section": "",
    "text": "Freie Universität Berlin – represented by the President –\n\n\n\nFreie Universität Berlin\nOffice of the President\nKaiserswerther Str. 16/18\n14195 Berlin\n\n\n\nDE 811304768\n\n\n\nM.Sc. Johannes Rosentreter (Web content development)\nE-mail: rosentreter.johannes@gmail.de\nPhone:\nDipl.-Ing. Kristin Fenske (Web content development)\nE-mail: kristin.fenske@fu-berlin.de\nPhone: (+49 30) 838 65326\nProf. Dr. Hannes Feilhauer (Content-related support)\nE-mail: hfeilhauer@zedat.fu-berlin.de\nPhone: (+49 30) 838 66172\nDr. Marion Stellmes (Content-related support)\nE-mail: marion.stellmes@fu-berlin.de\nPhone: (+49 30) 838 61978\nDipl. Geog. Rolf Rissiek (Laboratory assistance)\nE-mail: rolf.rissiek@fu-berlin.de\nPhone: (+49 30) 838 70263\n\n\n\nSee our Privacy Policy\n\n\n\nFreie Universität Berlin is a statutory body under public law in accordance with §§1 and 2 of the Berlin Law Relating to Institutions of Higher Learning (Berliner Hochschulgesetz, BerlHG). The web site of Freie Universität Berlin is subject to current copyright law. Please also take notice of our disclaimer."
  },
  {
    "objectID": "About/index.html#institution",
    "href": "About/index.html#institution",
    "title": "About",
    "section": "",
    "text": "Freie Universität Berlin – represented by the President –"
  },
  {
    "objectID": "About/index.html#address",
    "href": "About/index.html#address",
    "title": "About",
    "section": "",
    "text": "Freie Universität Berlin\nOffice of the President\nKaiserswerther Str. 16/18\n14195 Berlin"
  },
  {
    "objectID": "About/index.html#turnover-tax-id",
    "href": "About/index.html#turnover-tax-id",
    "title": "About",
    "section": "",
    "text": "DE 811304768"
  },
  {
    "objectID": "About/index.html#web-content-editors",
    "href": "About/index.html#web-content-editors",
    "title": "About",
    "section": "",
    "text": "M.Sc. Johannes Rosentreter (Web content development)\nE-mail: rosentreter.johannes@gmail.de\nPhone:\nDipl.-Ing. Kristin Fenske (Web content development)\nE-mail: kristin.fenske@fu-berlin.de\nPhone: (+49 30) 838 65326\nProf. Dr. Hannes Feilhauer (Content-related support)\nE-mail: hfeilhauer@zedat.fu-berlin.de\nPhone: (+49 30) 838 66172\nDr. Marion Stellmes (Content-related support)\nE-mail: marion.stellmes@fu-berlin.de\nPhone: (+49 30) 838 61978\nDipl. Geog. Rolf Rissiek (Laboratory assistance)\nE-mail: rolf.rissiek@fu-berlin.de\nPhone: (+49 30) 838 70263"
  },
  {
    "objectID": "About/index.html#privacy-statement",
    "href": "About/index.html#privacy-statement",
    "title": "About",
    "section": "",
    "text": "See our Privacy Policy"
  },
  {
    "objectID": "About/index.html#legal-form",
    "href": "About/index.html#legal-form",
    "title": "About",
    "section": "",
    "text": "Freie Universität Berlin is a statutory body under public law in accordance with §§1 and 2 of the Berlin Law Relating to Institutions of Higher Learning (Berliner Hochschulgesetz, BerlHG). The web site of Freie Universität Berlin is subject to current copyright law. Please also take notice of our disclaimer."
  },
  {
    "objectID": "About/cv.html",
    "href": "About/cv.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Github\n  \n  \n    \n     Email\n  \n  \n    \n     \n  \n\n  \n  \n\nWelcome\nto your Remote Sensing Data Analysis online course, or RESEDA for short!\nThis course helps you improve your analysis of remote sensing image data for the next data science project or thesis. We will have a look at basic and advanced concepts needed for a complete project implementation using remote sensing imagery – with a special focus on automatisation of individual operations and big data processing. This course will provide a great deal of knowledge and valuable expertise for all related fields of environmental earth sciences!\nFor that purpose we will mainly use the statistical programming language R in a Linux environment – but please don’t panic, it’s not as bad as it sounds: You will be led through a complete analysis process, from data acquisition, to import, exploration and finally the export of your results, guided by a lot of reproduceable examples, exercises and pretty pictures!\nKeep in mind that this course content complements and extends the material covered in the classes Fernerkundung und Digitale Bildverarbeitung and Geographische Informationssysteme, both being taught at the Freie Universität Berlin.\nPlease feel free to contact us if you have any questions or would like to discuss potential projects.\n\n\n\n\n\nBest regards, your FU Berlin Remote Sensing and Geoinformatics staff"
  },
  {
    "objectID": "projects/posts/post-with-code/index.html",
    "href": "projects/posts/post-with-code/index.html",
    "title": "Future Forest",
    "section": "",
    "text": "The aim of FutureForest is to use artificial intelligence (AI) methods to provide information that can support climate-adapted forest conversion.\nOngoing Projects\n\nFutureForest (future-forest.eu): The aim of FutureForest is to use artificial intelligence (AI) methods to provide information that can support climate-adapted forest conversion. The effects of different climate forecasts and forest conversion scenarios are to be simulated and the possible effects of different management methods are to be assessed. Main tasks of FU Berlin include:\n\nSentinel-2 time series based detection of forest damaged areas in Germany\nOperational \"near real-time\" monitoring using Deep Learning\n\n\n\nEarly detection of damaged areas\nProvision of a Germany-wide map of current forest damage/warnings\n\nUAV based analysis of the deciduous forests under drought stress in the historical garden districts of Berlin and Potsdam in 2020 (in cooperation with „Stiftung Preußische Schlösser & Gärten Berlin-Brandenburg” (SPSG) and „Grünflächenamt Berlin Steglitz-Zehlendorf”\n\nYou can visit the live dashboard of the Installation site here."
  },
  {
    "objectID": "quarto-website-template-main/404.html",
    "href": "quarto-website-template-main/404.html",
    "title": "404",
    "section": "",
    "text": "Page not found, sorry! Try the search or navigate back to the base website."
  },
  {
    "objectID": "R-Crash-Course/contents/PartI.html",
    "href": "R-Crash-Course/contents/PartI.html",
    "title": "Part I",
    "section": "",
    "text": "Part I\n\n\nPackage Management\nPackages are collections of functions and compiled code written by the R community or the R Development Core Team. R comes with a standard set of packages, e.g., base, stats, and graphics. However, R is enormously expandable in its (geo-)statistical functionality via the official network CRAN (The Comprehensive R Archive Network). There are over 11,000 different extensions, or packages, available free of charge. The directory, where installed packages are stored, is called library.\nYour can install new packages to your library by clicking on the Packages-tab and then Install in the Files Pane of R-Studio. Anyway, download and installation of packages can also be done script-based via the function install.packages() (illustrated using the ggplot2 package, which is useful for creating elegant data visualizations):\ninstall.packages(\"ggplot2\")\n\nlibrary()\nThe function library() in line 3 lists all currently installed packages on your system. This list is also visible in the Files Pane of R-Studio (Packages tab). Keep in mind that packages must be installed only once and remain permanently installed, even after a restart of R-Studio.\nOnce installed, you have to load the package into your current R-session before you can use its functionalities by using library() together with the package name:\nlibrary(ggplot2)    \nYou can list all packages, which are loaded in your current R session by using search(). Activated packages also have a tick symbol next to their names in the Files pane (Packages tab) of R-Studio.\n\n\nCalculate With R\nOf course, R can be used as a simple calculator. Required operators can be entered directly into the Console pane of R-Studio or as a whole script in the Source Pane, from which you can send the commands to the Console pane with Ctrl+Enter. Results are then immediately printed to the console. In this online course, corresponding outputs are also shown with two hash tags at the beginning of the line for better transparency and readability:\n# hash tags allow you to make valuable notes and reminders \n\n19 + 23      \n## [1] 42\n\n34 - 22     \n## [1] 12\n\n27 / 9       \n## [1] 3\n\n6 * 8        \n## [1] 48\n\n(2 + 3) * 4  \n## [1] 20\nYou will see a number in square brackets [1] at the beginning of your output prompts. This number refers to the length of your output, i.e., the number of elements, which is 1 for all examples above. More on that in chapter Vectors. In addition to these standard operators, there were plenty of other operators commonly encountered in R:\n\n\n\n\n\n\n\n  ?  \nhelp function\n\n\n  +   –   /   *   ^  \naddition, subtraction, division, multiplication, potentiation\n\n\n  !  \nnegation sign\n\n\n  &lt;   &gt;   &lt;=   &gt;=   ==   !=  \nlesser, greater, lesser or equal, greater or equal, equal, not equal\n\n\n  &   |  \nboolean AND, boolean OR\n\n\n  &lt;-  \nvariable assignment\n\n\n  ~  \nseparate left- and right-hand sides in a model formula\n\n\n  :  \ngenerate regular sequences\n\n\n  %%  \nmodulo\n\n\n[  [[]]  $  @\nindexing in vectors, matrices, and data frames\n\n\n\nThe help operator in R provides access to the documentation pages for R functions, data sets, and other objects, both for packages in the standard R distribution and for contributed packages. In order to access documentation for the sequence operator, for example, enter the command ?\":\" or help(\":\").\n\n\n\n\nVariables\nIn most cases, however, you will want to cache results of commands in order re-access them later on. Then, variables come into play. In R variables are defined using the  \\&lt;-  operator. Although the output will not be printed to the Console pane directly, we store the variable in our temporary workspace. Thus, the variable should be visible under Values ​​in the Environment pane in R-Studio.\nx &lt;- 8 + 7         # assignment to variable x\n\ny &lt;- 4 * 2         # assignment to variable y\nWe only get the value of the variable when we call its name as a command or look into the Environment pane. Further calculations with the variables are also possible:\nx            \n## [1] 15\n\ny  \n## [1] 8\n\nx + y        \n## [1] 23\n\nmy.variable &lt;- x + y\nmy.variable \n## [1] 23\nA convention in R is to include points in variable names, e.g., my.variable. This is for the sake of clarity only, especially when many variables exist, and has no deeper meaning beyond that. Strictly avoid any other special symbols in variable names.\n\n\n\n\nFunctions\nA function is a piece of code written to carry out a specific task. It can accept arguments and returns one or more values.\nR standard packages offer several arithmetic built-in functions and constants, which make statistical analysis quite efficient. A function generally consists of a function name and two parentheses  () , in which arguments are given as input. Of course, previously created variables can serve as arguments for functions, too:\nsqrt(64)           # square root \n## [1] 8\n\nexp(3)             # exponential\n## [1] 20.08554\n\ncos(13)            # cosinus\n## [1] 0.9074468\n\npi                 # constant number pi\n## [1] 3.141593\n\nround(pi)          # round values\n## [1] 3\n\na &lt;- 6\nb &lt;- 9.2\nlog10(a + b)       # logarithm (base 10)\n## [1] 1.181844\nSo, the best way to learn about the internal workings of a function is to write your own one. R allows to create user defined functions, whereby the basic construct of each function is the following:\nname.of.fun( arguments ) { body }\nThe code in between the curly braces is the body of the function. This is where you define all the commands your functions will perform. Let us write a function that calculates a normalized ratio of two numeric values! It is best to copy the following code into the script window, select everything and then execute the code. The function should then appear in the Environment window and can be called hereinafter.\nmy.fun &lt;- function(var1, var2) {\n  x &lt;- (var1 - var2) / (var1 + var2)\n  return(x)\n}\nExplanation: Use function() to create a new function and assign it to any variable, e.g.,my.fun. The two arguments var1 and var2 are placeholders for variables that are assigned when this function is called. Operations of the function are defined between the curly braces {}. Intermediate results, i.e., the x in our example, exist locally within the function – they do not appear in the Environment window. Only variables given to the [return` function can be saved as a variable. The function call is done via:\nresult &lt;- my.fun(42, 13)\nresult \n## [1] 0.5272727\nIf you feel ready click the button below and check your understanding up to here!"
  },
  {
    "objectID": "R-Crash-Course/contents/PartIII.html#indexing-in-matrices",
    "href": "R-Crash-Course/contents/PartIII.html#indexing-in-matrices",
    "title": "Part III",
    "section": "Indexing in matrices",
    "text": "Indexing in matrices\nIndexing in matrices behaves adequately to indexing in vectors, except that we now put two index numbers in the square brackets [] to address rows and columns. Both numbers must always be separated by a comma [line, column]. If we want all the entries from one dimension, we simply leave the corresponding slot for the index numbers empty:\nm &lt;- matrix(1:15, nrow = 5, ncol = 3) \nm\n##      [,1] [,2] [,3]\n## [1,]    1    6   11\n## [2,]    2    7   12\n## [3,]    3    8   13\n## [4,]    4    9   14\n## [5,]    5   10   15\n\nm[ , 2]                     # extract second column\n## [1]  6  7  8  9 10\n\nm[3,  ]                     # extract third row\n## [1]  3  8 13\n\nm[1, c(2, 3)]               # elements of first row in 2nd and 3rd column\n## [1]  6 11"
  },
  {
    "objectID": "R-Crash-Course/contents/PartIII.html#calculate-with-matrices",
    "href": "R-Crash-Course/contents/PartIII.html#calculate-with-matrices",
    "title": "Part III",
    "section": "Calculate with matrices",
    "text": "Calculate with matrices\nR is an equally powerful tool in terms of linear algebra. Appropriate to the vectors, whole matrices can be multiplied by a single value (scalar multiplication) or element by element. For the latter, however, the matrices necessarily need the same dimensionality dim().\nm1 &lt;- matrix(1:8, nrow = 2)\nm1\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    3    5    7\n## [2,]    2    4    6    8\n\nm1 * 5                         # scalar multiplication\n##      [,1] [,2] [,3] [,4]\n## [1,]    5   15   25   35\n## [2,]   10   20   30   40\n\nm1 * m1                        # multiplication element-wise\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    9   25   49\n## [2,]    4   16   36   64\nSome useful and commonly used functions:\nm2 &lt;- matrix(1:6, nrow = 2)\nm2\n##      [,1] [,2] [,3]\n## [1,]    1    3    5\n## [2,]    2    4    6\n\ncolMeans(m2)               # mean of all columns\n## [1] 1.5 3.5 5.5\n\ncolSums(m2)                # sum of all columns\n## [1]  3  7 11\n\nrowMeans(m2)               # mean of all rows\n## [1] 3 4\n\nrowSums(m2)                # sum of all rows\n## [1]  9 12\n\nt(m2)                      # transpose a matrix\n##      [,1] [,2]\n## [1,]    1    2\n## [2,]    3    4\n## [3,]    5    6\n\nm3 &lt;- matrix(1:6, ncol = 2)\nm3 %*% m2                  # matrix multiplication\n##      [,1] [,2] [,3]\n## [1,]    9   19   29\n## [2,]   12   26   40\n## [3,]   15   33   51\nMatrix multiplications assume that the inner dimensions of the two matrices are the same length (here you will find further information).\nNext training session incoming:"
  },
  {
    "objectID": "R-Crash-Course/contents/PartV.html#the-if-statement",
    "href": "R-Crash-Course/contents/PartV.html#the-if-statement",
    "title": "Part V",
    "section": "The IF statement",
    "text": "The IF statement\nUse the if / else command to perform simple queries on all data types. The Microsoft Excel equivalent would be the “IF” feature. In R, the syntax is as follows:\nx &lt;- 3                                # define x\n\nx &lt;= 4                                # logical condition\n## [1] TRUE\n\nif (x &lt;= 4) {\n  print(\"x is smaller than or equal to 4!\")    \n} else {\n  print(\"x is larger than 4!\")        \n}\n## [1] \"x is smaller than or equal to 4!\"\nExplanation: An IF-command needs three parts: the keyword if(), a condition that results in a single logical output x \\&lt;= 4 and a block of code in curly braces {}, which is executed if the expression is TRUE. So, if the condition is TRUE, the code will run in curly brackets after the IF-command. If the condition is FALSE, the code block after the elseis executed.\nThe print() function outputs the strings in the parentheses to the console window. Here we need the print() function so that the output can be written out of the IF-function, similar to return(see part I. A vectorized (and therefore more efficient) notation is the following:\nx &lt;- c(3, 4, 5, 6, 7)                                \n\nifelse(x &lt;=4, \"x is smaller than or equal to 4!\", \"x is larger than 4!\")\n## `1] \"x is smaller than or equal to 4!\" \"x is smaller than or equal to 4!\"\n## `3] \"x is larger than 4!\"              \"x is larger than 4!\"             \n## `5] \"x is larger than 4!\"\nUsing ifelse(), the condition for each individual element is determined as a vector. This is helpful, e.g., if we want to categorize data!"
  },
  {
    "objectID": "R-Crash-Course/contents/PartV.html#the-for-loop",
    "href": "R-Crash-Course/contents/PartV.html#the-for-loop",
    "title": "Part V",
    "section": "The FOR loop",
    "text": "The FOR loop\nLoops are incredibly useful when certain tasks need to be repeated very often in the script. A for loop is based on an iterable variable of defined length. But what does that mean? We define any variable, e.g., i, with a start integer value, e.g, 1. We then increment this integer value until a second integer value, e.g. 8, is reached. This can be done via the sequence operator ::\nfor (i in 1:8) {\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] 6\n## [1] 7\n## [1] 8\nThe practical thing: The code in curly brackets is automatically executed once each time (eight times in total)! And we can meanwhile pick up the expression of our variable i, in order to print it or index a vector with it and much more:\nv &lt;- c(23, 54, 12, 59, 67, 45)    # create integer vector\n\nlength(v)                         # check length of vector\n## [1] 6\n  \nfor (i in 1:length(v)) {          # iterate length(v) times\n  print(v[i])\n}\n## [1] 23\n## [1] 54\n## [1] 12\n## [1] 59\n## [1] 67\n## [1] 45\nIt is also possible to set the variable / iterator equal to the elements of the vector instead of an integer value for indexing. The information in which run the loop is, however, is initially lost:\nv &lt;- c(\"R\", \"is\", \"still\", \"fun\")    \n\nfor (i in v) {\n  print(i)\n}\n## [1] \"R\"\n## [1] \"is\"\n## [1] \"still\"\n## [1] \"fun\"\nNow, have a look at exercise V:"
  },
  {
    "objectID": "R-Crash-Course/contents/RExerciseII.html",
    "href": "R-Crash-Course/contents/RExerciseII.html",
    "title": "R – Exercise II",
    "section": "",
    "text": "R – Exercise II\n\n\n\nCreate an integer variable e holding the value 42! Check the object class of e with class()!\n\ne &lt;- 42L\n\nclass(e)\n## [1] \"integer\"\n\nConvert e to the character data type with as.character()! Check the object class again!\n\ne &lt;- as.character(e)\n\nclass(e)\n## [1] \"character\"\n\nCreate a character vector friends with four names from your circle of friends or acquaintances!\n\nfriends &lt;- c(\"Anna\", \"Otto\", \"Natan\", \"Ede\")\n\nfriends\n## [1] \"Anna\"  \"Otto\"  \"Natan\" \"Ede\"\n\nIndex the second element from the vector friends!\n\nfriends[2]\n## [1] \"Otto\"\n\nReplace the first element of the vector friends with “Isolde” and check the updated vector again!\n\nfreunde[1] &lt;- \"Isolde\"\n\nfreunde\n## [1] \"Isolde\" \"Otto\"   \"Natan\"  \"Ede\"\n\nCreate a vector v1 from the following elements  1, \"Hello\", 2, \"World\" ! Check the object class!\n\nv1 &lt;- c(1, \"Hello\", 2, \"World\")\n\nv1\n## [1] \"1\"     \"Hello\" \"2\"     \"World\"\n\nclass(v1)\n## [1] \"character\"\n\nCreate a vector v2 with numerical values ​​(only integers) ranging from 4 to 10!\n\nv2 &lt;- c(4, 5, 6, 7, 8, 9, 10)\n\nv2\n## [1]  4  5  6  7  8  9 10\n\n# or use the sequence operator \":\"\n\nv2 &lt;- c(4:10)\n\nv2\n## [1]  4  5  6  7  8  9 10\n\nIndex the first three elements from v2!\n\nv2[1:3]\n## [1] 4 5 6\n\n# or:\n\nv2[ c(1, 2, 3) ]\n## [1] 4 5 6\n\nIndex all elements of v2 except the second element and save the result as v2.subset!\n\nv2.subset &lt;- v2[-2]\n\nv2.subset\n## [1]  4  6  7  8  9 10\n\nUse the length () function to find out the length of v2.subset (= the number of elements in the vector)!\n\nlength(v2.subset)\n## [1] 6\n\nCalculate the arithmetic mean of vector v2! In addition, determine the standard deviation of v2.subset!\n\nmean(v2)\n## [1] 7\n\nsd(v2.subset)\n## [1] 2.160247"
  },
  {
    "objectID": "R-Crash-Course/contents/RExerciseIV.html",
    "href": "R-Crash-Course/contents/RExerciseIV.html",
    "title": "R – Exercise IV",
    "section": "",
    "text": "R – Exercise IV\n\n\n\nCreate a data frame df that contains the following variables for at least four observations:\n\n\nname: name of at least four friends or acquaintances\nage: the age of those persons\nsize: the height of those persons in cm\ncity: Place of residence of those persons (city)\n\ndf &lt;- data.frame(\n  name = c(\"Anna\", \"Otto\", \"Natan\", \"Ede\"), \n  age  = c(66, 53, 22, 36),\n  size = c(170, 174, 182, 180),\n  city = c(\"Hamburg\", \"Berlin\", \"Berlin\", \"Cologne\")\n  )\n\ndf\n##    name age size    city\n## 1  Anna  66  170 Hamburg\n## 2  Otto  53  174  Berlin\n## 3 Natan  22  182  Berlin\n## 4   Ede  36  180 Cologne\n\nExamine the dimensionality, structure and statistical summary of your data frame df!\n\ndim(df)\n## [1] 4 4\n\nstr(df)\n## 'data.frame':    4 obs. of  4 variables:\n##  $ name: Factor w/ 4 levels \"Anna\",\"Ede\",\"Natan\",..: 1 4 3 2\n##  $ age : num  66 53 22 36\n##  $ size: num  170 174 182 180\n##  $ city: Factor w/ 3 levels \"Berlin\",\"Cologne\",..: 3 1 1 2\n\nsummary(df)\n##     name        age             size            city  \n##  Anna :1   Min.   :22.00   Min.   :170.0   Berlin :2  \n##  Ede  :1   1st Qu.:32.50   1st Qu.:173.0   Cologne:1  \n##  Natan:1   Median :44.50   Median :177.0   Hamburg:1  \n##  Otto :1   Mean   :44.25   Mean   :176.5              \n##            3rd Qu.:56.25   3rd Qu.:180.5              \n##            Max.   :66.00   Max.   :182.0\n\nIndex the second column with simple square brackets [] and save the output as df.subset! Which class does the output belong to?\n\ndf.subset &lt;- df[2]\n\ndf.subset\n##   age\n## 1  66\n## 2  53\n## 3  22\n## 4  36\n\nclass(df.subset)\n## [1] \"data.frame\"\n\nIndex the variable age with double square brackets [] and save the output as age.persons. Which class does the output belong to?\n\nage.persons &lt;- df[[\"alter\"]]\n\n# oder\n\nage.persons &lt;- df[[2]]\n\nage.persons\n## [1] 66 53 22 36\n\nclass(age.persons)\n## [1] \"numeric\"\n\nAdd the variable weight in kg to the data frame df!\n\ndf$weight &lt;- c(115, 110.2, 95, 87)\n\ndf\n##    name age size    city weight\n## 1  Anna  66  170 Hamburg  115.0\n## 2  Otto  53  174  Berlin  110.2\n## 3 Natan  22  182  Berlin   95.0\n## 4   Ede  36  180 Cologne   87.0\n\nAdd another observation (person) to your df!\n\nnew.person &lt;- data.frame(\n  \"name\"   = \"Anna\",\n  \"age\"    = 32,\n  \"size\"   = 174,\n  \"weight\" = 63,\n  \"city\"   = \"Hamburg\"\n)\n\ndf &lt;- rbind(df, new.person)\n\ndf\n##    name age size    city weight\n## 1  Anna  66  170 Hamburg  115.0\n## 2  Otto  53  174  Berlin  110.2\n## 3 Natan  22  182  Berlin   95.0\n## 4   Ede  36  180 Cologne   87.0\n## 5  Anna  32  174 Hamburg   63.0\n\nCalculate the mean value of the variable age and save the result as ages.mean!\n\nages.mean &lt;- mean(df$age)\n\nIndex all observations (persons) that are older than the average ages.mean!\n\ndf[df$age &gt; ages.mean, ]\n\nIndex all persons, which are lighter than 100 kg AND at least 180 cm tall!\n\ndf[df$weight = 180, ]\n##    name age size    city weight\n## 3 Natan  22  182  Berlin     95\n## 4   Ede  36  180 Cologne     87\n\n# or\n\nsubset(df, df$weight = 180)\n##    name age size    city weight\n## 3 Natan  22  182  Berlin     95\n## 4   Ede  36  180 Cologne     87"
  },
  {
    "objectID": "R-Crash-Course/index.html",
    "href": "R-Crash-Course/index.html",
    "title": "R Crash Course",
    "section": "",
    "text": "R Crash Course\n\n\nThis is a R crash course for anyone who previously had no or very little contact with script-based programming. It should establish the basic understanding needed for upcoming chapters.\nYou are an old hand in R programming and do not need this introduction? – Skip this section and go explore the RESEDA course!\nIn order to deepen your knowledge, in particular with a view to statistical analysis, we recommend the advanced e-learning course SOGA (Softwaregestützte Geodatenanalyse). User name and password for SOGA are “ilovestats” in a single word. Of course, there are lots of advanced tutorials and relevant literature out there, which should not go unmentioned here.\n\nCrash Course in a Box\nWe subdivided this crash course into several sections. At the end of most sections you will get to test your knowledge with coding exercises (E)!\n\nPart I + E I:\n\nPackage Management\nCalculate With R\nVariables\nFunctions\n\nPart II + E II:\n\nVectors\nFactors\n\nPart III + E III:\n\nMatrices\n\nPart IV + E IV:\n\nLists\n\nPart V + E V:\n\nMissing values\nControl structures"
  },
  {
    "objectID": "RESEDA/contents/Download.html",
    "href": "RESEDA/contents/Download.html",
    "title": "Download",
    "section": "",
    "text": "We show you two platforms which allow users to view and download remote sensing data such as satellite images, aerial photographs, and cartographic products. You will learn how to use them in order to query and download Landsat 8, Sentinel 2 and Sentinel 1 data from the archives.\n\n\nLandsat Data via USGS EarthExplorer\n\n\n\n\n\n\n\nSentinel Data via ESA SciHUB"
  },
  {
    "objectID": "RESEDA/contents/Download.html#section-in-a-box",
    "href": "RESEDA/contents/Download.html#section-in-a-box",
    "title": "Download",
    "section": "Section in a Box",
    "text": "Section in a Box\nIn this section, the following content awaits you:\n1 USGS EarthExplorer User Interface\n– visiting USGS EarthExplorer for the first time\n2 Registration\n– do a registration in order to be allowed to download products\n3 Perform a Search\n– create a search query to get products from the database\n4 Download a Dataset\n– download Landsat Level 1 and Level 2 products\nEXERCISE"
  },
  {
    "objectID": "RESEDA/contents/Download.html#usgs-earthexplorer-user-interface",
    "href": "RESEDA/contents/Download.html#usgs-earthexplorer-user-interface",
    "title": "Download",
    "section": "1 USGS EarthExplorer User Interface",
    "text": "1 USGS EarthExplorer User Interface\nThe EarthExplorer supports online search in comprehensive databases, quicklook visualizations, metadata export, and data download for earth science data from the archives of the U.S.Geological Survey (USGS). You get to the USGS EarthExplorer via the following URL using any web browser (Firefox in our RESEDA VM):\nhttps://earthexplorer.usgs.gov/\nOnce clicked, the main EarthExplorer graphical user interface (GUI) should be loaded, which is composed of three key elements:\n\n\n\n\n\n\nUSGS EarthExplorer user interface\n\n\n1 Header Menu Bar: Buttons for login and registration services, as well as help, RSS and feedback functionalities. After login you can save and load queries here\n2 Data Search Side Bar: Search components are divided among four tabs and allow you to enter search criteria, select datasets to query, enter additional criteria, and review results in a tabular window\n3 Image View with Navigation Elements: embedded Google Maps components to visualize search results, with standard Google Maps navigation tools, i.e., zoom in/ zoom out, street view (lower right corner), and coordinate information of current cursor position (upper right corner). You can toggle between satellite imagery view and GIS data view by selecting the adequate button in the top left corner"
  },
  {
    "objectID": "RESEDA/contents/Download.html#registration",
    "href": "RESEDA/contents/Download.html#registration",
    "title": "Download",
    "section": "2 Registration",
    "text": "2 Registration\nFirst of all, to fully use the services of the EarthExplorer, you need to register by clicking on the register button in the header menu bar and proceed through the user registration. Only registered users can download data. You definitely need a working email address for this. The information gathered from the registration process is not distributed to other organizations and is only used to determine trends in data usage. You have to work through a user affiliation/data usage and address page:\n\n\n\nOnce you have completed the registration, you should get an email on your given email address to confirm your account. After confirmation you will be redirected to the login page where you have to fill in your username and password (or click on “login” in the header menu bar).\n\n\n\n\n\n\nUSGS EarthExplorer login formular\n\nAfter login you will notice your username and your “shopping”/ item basked in the header menu bar:\n\n\n\n\n\n\nSuccessful login"
  },
  {
    "objectID": "RESEDA/contents/Download.html#perform-a-search",
    "href": "RESEDA/contents/Download.html#perform-a-search",
    "title": "Download",
    "section": "3 Perform a Search",
    "text": "3 Perform a Search\nWe want to use an example to explore the search function of the EarthExplorer: We are looking for all Landsat 8 scenes that depict Berlin in summer 2017 and have a cloud cover of less than 10 %!\nEarthExplorer provides four tabs in the search procedure to guide you through your search request:\n\n\n\n\n\n\nEarthExplorer search tabs\n\n1. Enter Search Criteria: This one helps defining an area of interest (AOI) and a time span in which data will be found. The most straightforward way to define the AOI is to use the integrated Google Maps by typing in an address or place name, e.g., “Berlin, Alexanderplatz”, click  and choose one of the prompted suggestions. Designations for geographical longitude and latitude of the desired position are also possible, e.g., “52.5194, 13.4067”.\n\n\n\n\n\n\nDefining area of interest (AOI)\n\nFurthermore there is a global notation used for cataloging Landsat data, called Worldwide Reference System (WRS), whereby Landsat 8 follows the WRS-2. This system divides the Earth’s surface into the recording geometries of the Landsat acquisitions. There are a WRS-2 overview map and a WRS-2 Path/Row to Latitude/Longitude converter provided by the USGS. Especially the converter helps to find all possible Path-Row combinations for your AOI, which is 192/024 and 193/23 in our case for Berlin. Entering one of those Path/Row pairs and click  to add the adequate center of the Landsat acquisition as a coordinate to your Google Map interface:\n\n\n\n\n\n\nUse Path and Row specifications for queries\n\nIn order to delete any given coordinate, press the red cross  next to it.\nAnother easy way to define your AOI is to just left click within the map, which automatically adds a coordinate for a single coordinate search. By defining two points on the map, you will do a line search, which results in all data products which intersects the line. By defining three or more coordinates, a polygon is automatically displayed, forming your AOI:\n\n\n\n\n\n\nPolygon defined by four coordinates via left clicking in the Google Map interface\n\nThere are several other ways to define the subject area more precisely, e.g., by shapefiles, features, predefined areas, or kmls, but the methods described are effective and usually sufficient. For more detailed descriptions please have a look at the online help.\nTo make your query more concrete, you can define the time span within which you want to get data at the bottom of the Search Criteria tab. Simply set the start date and end date as well as all desired months, in our case june, july and august:\n\n\n\n\n\n\nDefining the time span and months of interest\n\n2. Select Your Data Set(s): The Data Set tab categorizes datasets into similar data collections. There is a dynamic tree structure, which allows you to expand/ collapse products by pressing on the plus and minus signs next to it. As you can see, there is a huge amount of data to choose from. Landsat 8 Level-1 data can be found at Landsat &gt; Landsat Collection 1 Level-1 &gt; Landsat 8 OLI/TIRS C1 Level-1:\n\n\n\n\n\n\nLandsat Collection 1 Data Level-1 products\n\nLandsat 8 Level-2 data is also available on demand under Landsat &gt; Landsat Collection 1 Level-2 (On-Demand) &gt; Landsat 8 OLI/TIRS C1 Level-2 (see chapter Landsat 8 for more information).\n3. Additional Criteria (Optional): This tab helps to further narrow the results of your search query by defining additional search criteria, e.g., the allowed proportion of cloud cover over land, absolute cloud cover, day or night as well as Path/Row restrictions. In addition, you can use the unique product ID (e.g., LC08_L1TP_192023_20170830_20170914_01_T1) to find specific individual scenes. Restrict the Land Cloud Cover to “Less than 10%” for our query:\n\n\n\n\n\n\nAdditional Criteria tab: cloud cover restrictions\n\n4. Search Results: When set, click on  on the bottom of the Data Search Side Bar or the Results tab on the top to execute your research. You will most likely get four data products as a result (depending on the shape of your AOI):\n\n\n\n\n\n\nSearch results\n\nFour scenes are not much, are they? A cloud cover under 10% is already pretty strict, when you allow more cloud coverage, the number of scenes will increases. Each product is given a unique ID, as well as a acquisition time and the WGS-2 Path and Row. Furthermore there is a number of overlay and download controls you can choose from for each scene:\n\n\n\n\n\n\nOverlay and download controls\n\n\n1 Show Footprint: display the contour of a scene on Google Map in true color\n2 Show Browse Overlay: display a preview image of the scene\n3 Compare Browse: activate this button on multiple scenes, then overlay those here\n4 Show Metadata and Browse: display the browse image and full metadata for the selected scene\n5 Download Options: allows registered users to download the selected data\n6 Add to Bulk Download: allows registered users to bulk download the selected data\n7 Order Scene: allows registered users to order or request specialized processing of products\n8 Exclude Scene from Results: delete the particular scene from current result window"
  },
  {
    "objectID": "RESEDA/contents/Download.html#download-a-dataset",
    "href": "RESEDA/contents/Download.html#download-a-dataset",
    "title": "Download",
    "section": "4 Download a Dataset",
    "text": "4 Download a Dataset\nIn order to download a single Level-1 scene (see chapter Landsat 8), click on Download Options  and choose the last item, which should be the largest file (approximately 700-900 MB for Landsat 8):\n\n\n\n\n\n\nDownload options for a Landsat 8 Level-1 scene\n\nYou will recieve a zipped file, which contains all spectral bands as georeferenced geotiff-files.\nAnyway, Level-2 products need a preprocessing and are NOT available for immediate download. In order to acquire Level-2 data, pick Landsat Collection 1 Level-2 in the Data Sets tab (as shown in 2. Select Your Data Set(s)). In the Result tab you have to use Order Scene  to put all the wanted scenes in your Item Basket. A number next to your Item Basket shows how many scenes you have chosen:\n\n\n\n\n\n\nAdd scenes to Item Basket\n\nWhen you are done, click on Item Basket in the Header Menu Bar to submit your order. You will see a list of all selected scenes. Confirm your selection by pressing Proceed To Checkout:\n\n\n\n\n\n\nOrder overview when clicked on Item Basked\n\nOn the next screen, press Submit Order:\n\n\n\n\n\n\nSubmit your order\n\nDONE! You will be given a unqiue order ID and a confirmation email will be sent to your email address – check in your email inbox!\n\n\n\n\n\n\nOrder confirmation\n\nAll Level-2 orders submitted through ESPA are processed within 2-5 days, depending on the size of the order and the backlog already in the system.\nBe patient.\nA second email confirmation will be send when the products are ready for download. From this moment on, all scenes will remain available for 10 days.\nClick on the Order status url in your second confirmation email, which redirects you to the following website:\n\n\n\n\n\n\nOrder overview given by the link of the second email\n\nClick on your Order ID, which brings you to the download site. Simply click on the download link in order to get your data:\n\n\n\n\n\n\nFinally get your Level-2 data!\n\nFollow the search queries in the upcoming exercise to familiarize yourself with how to use the EarthExplorer!"
  },
  {
    "objectID": "RESEDA/contents/Download.html#section-in-a-box-1",
    "href": "RESEDA/contents/Download.html#section-in-a-box-1",
    "title": "Download",
    "section": "Section in a Box",
    "text": "Section in a Box\nIn this section, the following content awaits you:\nBulk Download L8 Level-1 data\n– use this when you need about 5-20 Level-1 scenes\nBulk Download L8 Level-2 data\n– use this when you need about 5-20 Level-2 scenes\nBIG DATA Download\n– use this when you need a lot more scenes…"
  },
  {
    "objectID": "RESEDA/contents/Download.html#bulk-download-l8-level-1-data",
    "href": "RESEDA/contents/Download.html#bulk-download-l8-level-1-data",
    "title": "Download",
    "section": "Bulk Download L8 Level-1 data",
    "text": "Bulk Download L8 Level-1 data\nUSGS provides an application for bulk-downloading Level-1 Landsat data with a very appropriate name: the Bulk Download Application. The Bulk Download Application is an easy-to-use tool for downloading large quantities of satellite imagery and geospatial data. In order to use it, you have to order your Landsat 8 scenes via the Add to Bulk Download option  as mentioned in the Download section:\n\n\n\n\n\n\nOrder imagery by choosing bulk download option\n\nProceed to checkout and confirm your order as shown in the Download section. After that, a specific order number is given to you by email and on the USGS website:\n\n\n\n\n\n\nOrder number provided after submission\n\nNow open the application by navigating to Other &gt; Bulk Download Application in the start menu of our VM:\n\n\n\n\n\n\nLocation of Bulk Download Application in our VM\n\nThe user interface opens, in which you have to fill in your username and password used for the USGS Earth Explorer orders:\n\n\n\n\n\n\nLog in for bulk downloading\n\nChoose your order by its ID and click Select Order:\n\n\n\n\n\n\nChoose the order ID for your download\n\nChange the target directory to your liking by clicking on the right top icon to your. We recommend to set this path to “/media/sf_exchange” to not unnecessarily enlarge the VirtualBox storage. Begin your download by pressing Begin Download:\n\n\n\n\n\n\nBegin your bulk download"
  },
  {
    "objectID": "RESEDA/contents/Download.html#bulk-download-l8-level-2-data",
    "href": "RESEDA/contents/Download.html#bulk-download-l8-level-2-data",
    "title": "Download",
    "section": "Bulk Download L8 Level-2 data",
    "text": "Bulk Download L8 Level-2 data\nUnfortunately, Level-2 data is not supported by the USGS bulk download application as shown in the previous section. Anyway, there is a nice python based script provided by USGS-EROS, which downloads automatically all scenes for you! The source code is open source and can be reviewed on GitHub via this link and can also be downloaded here. If you are using our VM, the script is locally stored in your document folder under /home/student/Documents/download_espa_order.py.\nFirst of all, you should follow the guide for ordering Level-2 scenes (Download section) and receive the second email with the download confirmation.\nThen open the linux terminal – it is linked in the taskbar of our VM:\n\n\n\n\n\n\nLinux terminal shortcut in taskbar\n\nTo initiate the bulk download, you just need one command. Here is the blueprint of this command, in which you need to replace your email address, your username and password before executing:\n/usr/bin/python /home/student/Documents/download_espa_order.py -r 10 -c -o ALL -d /media/sf_exchange/ -e john.doe@gmail.com -u user123 -p password123\nExplanation: The command needs a python installation to work, so we define the location of python via \n/usr/bin/python. The download script itself is named “download_espa_order.py” and is stored in the documents folder of our VM, so we define its location with \n/home/student/Documents/download_espa_order.py. All individual letters preceded by a minus sign are arguments, which are assigned with the subsequent input. You have to change the email-address \n-e john.doe@gmail.com, the username -u user123, and the adequate password -p password123 to your own settings (USGS login data). For more information on this, have a look at the readme on GitHub.\nType in the whole command without any linebreaks into the terminal:\n\n\n\n\n\n\nTerminal command for bulk downloading Landsat Level-2 data\n\nPress the enter-key and watch the script downloading and saving all ordered scenes to your exchange folder -d /media/sf_exchange/ (you can change this destination folder to your liking):\n\n\n\n\n\n\nTerminal command for bulk downloading Landsat Level-2 data"
  },
  {
    "objectID": "RESEDA/contents/Download.html#big-data-download",
    "href": "RESEDA/contents/Download.html#big-data-download",
    "title": "Download",
    "section": "BIG DATA Download",
    "text": "BIG DATA Download\nYou can not get enough of data and you have some Terrabyte of disk space available? So, if you just want to download ALL the scenes of a search query (up to 20,000 scenes at once), there is an easy way to do it! When you have completed a search query, as shown in the Perform a Search section, there is an button on top of the scene list which can be used to export your results. This button is only clickable when you are logged in:\n\n\n\n\n\n\nMetadata Export button\n\nA small window opens asking what format the metadata should be exported to. Choose “Non-Limited Results” and “CSV” in order to export the metadata of every single file found to a csv-file (which is a text file):\n\n\n\n\n\n\nMetadata Export options\n\nClick Export and you will receive an email with a link to the metadata export file. The processing time and thus the waiting time for the email will vary depending on number of search results requested.\nClick the link in the mail and you will be redirected to a USGS page. Download the zipped csv-file to your harddrive (the exchange folder in our VM is recommended). Unzip the file by right-clicking on it and chose “Extract Here”. Next, open the following USGS website:\nhttps://earthexplorer.usgs.gov/filelist\nSelect Single Data Set and choose the type of dataset you want to download via the drop-down menu Data Set:\n\nData Set for Level-1: Landsat Collection 1 Level-1: Landsat 8 OLI/TIRS C1 Level-1\nData Set for Level-2: Landsat Collection 1 Level-2 (On-Demand): Landsat 8 OLI/TIRS C1 Level-2\n\nClick the button next to File and upload the unzipped .csv-file containing the metadata information. Confirm by pressing Submit File List:\n\n\n\n\n\n\nMetadata Export options\n\nIf you have selected the Level-1 product, select Bulk Download on the next page and click on Submit Order. Please follow the instructions given in the bulk download section above to complete your download with the bulk download application.\n\n\n\n\n\n\nLevel-1 product Bulk Download order\n\nIf you have selected the Level-2 product, select Order on the next page and click on Submit Order, wait for the ESPA confirmation email (Download section) and follow the instructions given in the Bulk Download section above:\n\n\n\n\n\n\nLevel-2 product Bulk Download order"
  },
  {
    "objectID": "RESEDA/contents/QGIS.html",
    "href": "RESEDA/contents/QGIS.html",
    "title": "QGIS",
    "section": "",
    "text": "QGIS\nQGIS is a open-source geographic information system (GIS) for viewing, manipulating, and gathering spatial data and is licensed under the GNU General Public License. Essential features of the application are the broad support of common vector data and raster data, e.g., shapefiles and GeoTIFFs. There is an integration of sophisticated digitizing tools for creating vector data, as well as tools for a easy creation of cartographic maps, which you can use in publications and final papers.\nThe layout and appearance of QGIS is highly modifiable. The panels are arranged as follows:\n\n\n\n\n\n\nDefault layout in QGIS\n\n\n1 Main Menu and Toolbars: various toolbars available, e.g., project toolbar for QGIS project management, map navigation toolbar, attributes toolbar for feature selection and identification, layer management toolbar for adding new layers, and many more. Right click in the toolbar area in order to select/ deselect the individual toolbars and panels\n2 Layers Panel: shows all data layers in your current QGIS session. The data layers listed at the top are those that are visible in the data view on top. Rearange the order by left-clicking one layer and then dragging and dropping it to the desired location. You can hide individual layers by removing the checkmark next to it\n3 Browser Panel: lets you navigate in your filesystem and manage geodata. You can have access to common vector files, databases and WMS/WFS connections\n4 Map View: visualize all your vector and raster data loaded into your current session (layers panel)\n5 Status Bar: get all the information about the coordinates of where the mouse pointer is pointing, the scale, magnifier, rotation and the current coordinate system of your QGIS session\n\nSo why do we use QGIS over other geographic information systems? At first it does not cost a penny and supports all types of operating systems. This is why the community is big and there are lots of help and tutorials available online, e.g., the official User Manual or internet forums, such as Stack Exchange. Furthermore it is actively developed by the QGIS Development Team and volunteer developers who regularly release updates and bug fixes.\nWe will mainly use QGIS in this online course to prepare classification maps accordingly in sections Visualization and Visualize in QGIS."
  },
  {
    "objectID": "RESEDA/contents/SensorBasics.html",
    "href": "RESEDA/contents/SensorBasics.html",
    "title": "Sensor Basics",
    "section": "",
    "text": "In this section you will repeat the meaning of basic remote sensing terminology using the example of three satellites which we will continue to use in RESEDA, i.e., Landsat 8, Sentinel 2 and Sentinel 1. However, the shown methods in this online course are also adaptable to other satellite sensors, airborne imagery or close-ups by considering the respective sensor characteristics.\n\n\n\n\nNon-imaging sensors, such as laser profilers or RADAR altimeter, usually provide point data, which do not offer spatially continuous information about how the input varies within the sensor’s field of view.\nBy contrast, imaging sensors are instruments that build up a spatially continuous digital image within their field of view, whereby they include not only information about the intensity of a given target signal, but also information about its spatial distribution. Examples include aerial photography, visible or near infrared scanner as well as synthetic aperture radars (SAR). All three satellite missions we will work with (Landsat 8, Sentinel 2 and Sentinel 1) are imaging sensors.\n\n\n\n\n\nAnyway, there is a fundamental differentiation of remote sensing systems that we need to be aware of: passive and active sensors. This classification is based on the underlying recording principles, which are contrasted in the following:\n\n\n\n\n\n\nPassive sensor operating mode\n\nPassive sensors should be the more common of those two. Passive sensors measure solar light reflected or emitted from the Earth surfaces and objects.\nThese instruments primarily rely on short waved electromagnetic solar energy of the sun as the only source of radiation. Objects on the Earth’s surface react to this electromagnetic energy either with reflection, transmission, or absorption, depending on the composition of the object’s atoms. Passive sensors mainly capture the reflected proportion of the solar energy. Thus, they can only do their observation job when the sun is present as a radiation source – that is, only during the day. Anyway, since objects also partially absorb incoming solar light, there is a inherent radiation of these objects, which can be measured, for example, as thermal radiation.\nPassive remote sensing imagery can be very similar to how our human eyes perceive land cover, which makes it easier for us to interpret the image data.\nUnfortunately, there is one big limitation of passive systems: Due to the fact that the reflected electromagnetic radiation has to pass through the Earth’s atmosphere, the signal is strongly influenced by the weather and cloud conditions: Fog, haze and clouds render affected image information partially or completely useless, as electromagnetic energy is scattered by the large particles of dense clouds. This is where active sensors come into play.\n\n\n\n\n\n\nActive sensor operating mode\n\nActive sensors emit their own electromagnetic radiation to illuminate the object they observe.\nActive sensors send a pulse of energy at the speed of light to the Earth’s surface, that is reflected, refracted or scattered by the objects on the surface and the atmosphere. The recieved backscatter then gives information on land surface characteristics. There are many types of active sensors out there: RADAR (Radio Detection and Ranging), Scatterometer, LiDAR (Light Detection and Ranging), and Laser Altimeter. We will take a closer look at Sentintel 1, a SAR system (Synthetic Aperture RADAR), which is an imaging RADAR type working with microwaves, in the upcoming SAR chapters.\nImages of active sensors are comparatively difficult to interpret: A SAR signal contains amplitude and phase information. Amplitude is the strength of the RADAR response and phase is the fraction of a full sine curve. In order to generate beautiful images out of these information, a more extensive preprocessing is often necessary, which we will do in SNAP.\nSince no natural light source is required, active sensors are capable to emit and capture their signals regardless of daytime. In addition, many systems operate in the electromagnetic domain of microwaves, so their wavelength is large enough to be unaffected by clouds and other atmospheric distortions.\n\n\n\n\n\nWhen working with imaging remote sensing systems, a distinction is made between four different resolution terms: geometric, spectral, radiometric and temporal. It is essential to know those resolutions in order to be able to assess whether the sensor system is suitable for your research question!\n\n\nA digital image consists of at least one matrix of integers values – the picture elements, or pixels. Each pixel contains information about a signal response from a small area on the land surface, e.g., reflectance or backscatter. The geometric Resolution describes the edge length of this area (usually in meters) and is determined by the sensor’s instantaneous field of view (IFOV). It determines which object sizes can still be identified in the image – the degree of detail, so to speak. The effects of geometric resolutions becomes evident when comparing different images, for example an airborne orthophoto (0.1 m), Sentinel 2 (10 m) and Landsat 8 (30 m) images:\n\n\n\n\n\n\nResidential area in Friedenau, orthophoto (l), Sentinel 2 data (m), and Landsat 8 data (r)\n\nThere are several synonyms commonly used for the term geometric resolution, e.g., spatial resolution, pixel size, pixel edge length, or ground sampling distance.\n\n\n\nWe humans can only perceive the visible light around us, which is just a very small part of the available electromagnetic spectrum. Satellites, on the other hand, sense a much wider range of the electromagnetic spectrum and can provide us with information about processes that would otherwise be neglected.\nSpectral satellite sensors measure the reflection from the earth’s surface in different wavelength areas of the electromagnetic spectrum, via so-called channels or bands. Each band can have a different bandwidth, i.e., the area scanned within the electromagnetic spectrum. The sensors concentrate the signals gathered within a band to one (pixel-) value via a sensor specific filter function.\nHowever, spectral resolution describes the number of bands that the sensor senses. The purpose of multiple bands is to capture the differences in the reflection characteristics of different surfaces and materials within the electromagnetic spectrum.\nPanchromatic systems have only one spectral channel with a large bandwidth usually from 0.4 to 0.7 μm. Due to this large bandwidth there is enough energy available to achieve high geometric resolutions. Various satellites additionally provide such a panchromatic channel, e.g., Landsat program, Quickbird-2 pan, and IKONOS pan.\nMultispectral systems generally refers to 3 to 15 bands, which are usually located within the visible range (VIS, 0.4-0.7 μm), near-infrared (NIR, 0.75–1.4 μm), short-wavelength infrared (SWIR, 1.4–3 μm), mid-wavelength infrared (MWIR, 3–8 μm) and long wavelength/ thermal infrared (LWIR/TIR, 8–15 μm). Examples: Landsat program, Sentinel 2/3, SPOT, Quickbird, and RapidEye.\n\n\n\n\nSentinel 2 image, airport Schönefeld, Berlin; left: true color composite (RGB 4,3,2), right: pseudo-color composite (RGB 8,4,3)\n\nHyperspectral systems (spectrometer) offer hundreds or thousands of bands with narrow bandwidths. A spectral resolution this high gives ability to distinguish minor differences of land cover characteristics, which in turn provides ability to address issues that could not be solved with multispectral data, e.g., mineral or building material classifications. There are some airborne spectrometers (AVIRIS, HySPEX, HyMAP) and to date only one operational satellite: Hyperion. Anyway, more spaceborne systems are already in the starting blocks, e.g., EnMAP and HyspIRI.\n\n\n\nA digital sensor generally recognizes objects as intensity values, whereby it can only distinguish between dark and bright. The radiometric resolution refers to the ability to tell apart objects based on differences in those intensities. Thus, a sensor with a high radiometric resolution records more intensity levels or grey-scale levels. This property is expressed by the number of bits. Most satellite products have a bit depth of 8 to 16 bits, which means that they support 28(=256) or 216(=65536) different gray scale levels, respectively. A 1 bit image would be subject to a huge loss of information in comparison!\nThose effects are easier to understand when looking at the image comparison below: An image with a bit depth of 1 contains only two gray scale levels (21=2), i.e., black and white. Using 2 bits double the number of available colors (22=4), which allows a few more details (as shown in the middle image). When using 8 bits, the image can draw from a whole color ramp ranging from black to white (dark to bright) comprising 256 grey scale values (28=256).\n\n\n\n\n\n\nBlue band of an orthophoto in 1 bit (l), 2 bit (m), and 8 bit (r) representation\n\nHuman perception is barely sufficient to detect gray-scale differences beyond 8 bits in digital images. Nevertheless, machine learning algorithms often benefit from a finer differentiation of contrasts.\n\n\n\nThe temporal resolution of a sensor is simply the distance of time (usually in days) between two image acquisitions of the same area. A high temporal resolution thus indicates a smaller time window between two images, which allows a better observation of temporally highly dynamic processes on the Earth’s surface, e.g., weather or active fire monitoring.\nMost satellite sensors have a temporal resolution of about 14 days. Anyway, by using a satellite constellation of multiple sensors identical in construction the time between two acquisitions can be shortened. For example Planet Labs operates five RapidEye satellites, which are synchronized so that they overlap in coverage.\nHowever, weather satellites are capable of acquiring images of the same area every 15 minutes. This can be explained by the different orbits of the satellites: geostationary and polar orbiting satellite systems.\n\n\n\n\n\n\nGeostationary orbiting satellite\n\nGeostationary sensors follow a circular geosynchronous orbit directly above the Earth’s equator. A geosynchronous system provides the same orbital period as the Earth’s rotation period (24 h), so it always looks at the same area on Earth, which it can observate at very high frequencies of several minutes. This rotation pattern is only possible at an altitude very close to 35.786 km, which generally results in a comparatively lower geometric resolution. Geostationary orbits are used by weather, communication and television satellites.\n\n\n\n\n\n\nPolar orbiting satellite\n\nPolar orbiting satellites pass above or nearly above the poles on each orbit, so the inclination to the Earth’s equator is very close to 90 degrees. They fly at an altitude of approximately 800-900 km. The lower a satellite flies, the faster it is. That’s why an orbit takes only ~90 minutes. While flying from the north to south pole in 45 minutes, sun-synchronous sensors look at the sunlit side of the Earth (descending images). Moving from south to nord pole results in nighttime imagery (ascending images). On an descending flight, all polar orbiting satellites cross the equator between 10:00 am and 10:15 am (local time) to provide maximum illumination and minimum water vapor to prevent haze and cloud build-up. Due to their inclination, they map the entire surface of the earth within several days (~14 d) as the earth continues to rotate beneath them. The temporal resolution of polar orbiting satellites usually describes the repetition rate at the Equator. The coverage gets better at higher latitudes due to the poleward convergence of the satellite orbits."
  },
  {
    "objectID": "RESEDA/contents/SensorBasics.html#non-imaging-vs-imaging-sensors",
    "href": "RESEDA/contents/SensorBasics.html#non-imaging-vs-imaging-sensors",
    "title": "Sensor Basics",
    "section": "",
    "text": "Non-imaging sensors, such as laser profilers or RADAR altimeter, usually provide point data, which do not offer spatially continuous information about how the input varies within the sensor’s field of view.\nBy contrast, imaging sensors are instruments that build up a spatially continuous digital image within their field of view, whereby they include not only information about the intensity of a given target signal, but also information about its spatial distribution. Examples include aerial photography, visible or near infrared scanner as well as synthetic aperture radars (SAR). All three satellite missions we will work with (Landsat 8, Sentinel 2 and Sentinel 1) are imaging sensors."
  },
  {
    "objectID": "RESEDA/contents/SensorBasics.html#passive-vs-active-sensors",
    "href": "RESEDA/contents/SensorBasics.html#passive-vs-active-sensors",
    "title": "Sensor Basics",
    "section": "",
    "text": "Anyway, there is a fundamental differentiation of remote sensing systems that we need to be aware of: passive and active sensors. This classification is based on the underlying recording principles, which are contrasted in the following:\n\n\n\n\n\n\nPassive sensor operating mode\n\nPassive sensors should be the more common of those two. Passive sensors measure solar light reflected or emitted from the Earth surfaces and objects.\nThese instruments primarily rely on short waved electromagnetic solar energy of the sun as the only source of radiation. Objects on the Earth’s surface react to this electromagnetic energy either with reflection, transmission, or absorption, depending on the composition of the object’s atoms. Passive sensors mainly capture the reflected proportion of the solar energy. Thus, they can only do their observation job when the sun is present as a radiation source – that is, only during the day. Anyway, since objects also partially absorb incoming solar light, there is a inherent radiation of these objects, which can be measured, for example, as thermal radiation.\nPassive remote sensing imagery can be very similar to how our human eyes perceive land cover, which makes it easier for us to interpret the image data.\nUnfortunately, there is one big limitation of passive systems: Due to the fact that the reflected electromagnetic radiation has to pass through the Earth’s atmosphere, the signal is strongly influenced by the weather and cloud conditions: Fog, haze and clouds render affected image information partially or completely useless, as electromagnetic energy is scattered by the large particles of dense clouds. This is where active sensors come into play.\n\n\n\n\n\n\nActive sensor operating mode\n\nActive sensors emit their own electromagnetic radiation to illuminate the object they observe.\nActive sensors send a pulse of energy at the speed of light to the Earth’s surface, that is reflected, refracted or scattered by the objects on the surface and the atmosphere. The recieved backscatter then gives information on land surface characteristics. There are many types of active sensors out there: RADAR (Radio Detection and Ranging), Scatterometer, LiDAR (Light Detection and Ranging), and Laser Altimeter. We will take a closer look at Sentintel 1, a SAR system (Synthetic Aperture RADAR), which is an imaging RADAR type working with microwaves, in the upcoming SAR chapters.\nImages of active sensors are comparatively difficult to interpret: A SAR signal contains amplitude and phase information. Amplitude is the strength of the RADAR response and phase is the fraction of a full sine curve. In order to generate beautiful images out of these information, a more extensive preprocessing is often necessary, which we will do in SNAP.\nSince no natural light source is required, active sensors are capable to emit and capture their signals regardless of daytime. In addition, many systems operate in the electromagnetic domain of microwaves, so their wavelength is large enough to be unaffected by clouds and other atmospheric distortions."
  },
  {
    "objectID": "RESEDA/contents/SensorBasics.html#imaging-sensor-resolutions",
    "href": "RESEDA/contents/SensorBasics.html#imaging-sensor-resolutions",
    "title": "Sensor Basics",
    "section": "",
    "text": "When working with imaging remote sensing systems, a distinction is made between four different resolution terms: geometric, spectral, radiometric and temporal. It is essential to know those resolutions in order to be able to assess whether the sensor system is suitable for your research question!\n\n\nA digital image consists of at least one matrix of integers values – the picture elements, or pixels. Each pixel contains information about a signal response from a small area on the land surface, e.g., reflectance or backscatter. The geometric Resolution describes the edge length of this area (usually in meters) and is determined by the sensor’s instantaneous field of view (IFOV). It determines which object sizes can still be identified in the image – the degree of detail, so to speak. The effects of geometric resolutions becomes evident when comparing different images, for example an airborne orthophoto (0.1 m), Sentinel 2 (10 m) and Landsat 8 (30 m) images:\n\n\n\n\n\n\nResidential area in Friedenau, orthophoto (l), Sentinel 2 data (m), and Landsat 8 data (r)\n\nThere are several synonyms commonly used for the term geometric resolution, e.g., spatial resolution, pixel size, pixel edge length, or ground sampling distance.\n\n\n\nWe humans can only perceive the visible light around us, which is just a very small part of the available electromagnetic spectrum. Satellites, on the other hand, sense a much wider range of the electromagnetic spectrum and can provide us with information about processes that would otherwise be neglected.\nSpectral satellite sensors measure the reflection from the earth’s surface in different wavelength areas of the electromagnetic spectrum, via so-called channels or bands. Each band can have a different bandwidth, i.e., the area scanned within the electromagnetic spectrum. The sensors concentrate the signals gathered within a band to one (pixel-) value via a sensor specific filter function.\nHowever, spectral resolution describes the number of bands that the sensor senses. The purpose of multiple bands is to capture the differences in the reflection characteristics of different surfaces and materials within the electromagnetic spectrum.\nPanchromatic systems have only one spectral channel with a large bandwidth usually from 0.4 to 0.7 μm. Due to this large bandwidth there is enough energy available to achieve high geometric resolutions. Various satellites additionally provide such a panchromatic channel, e.g., Landsat program, Quickbird-2 pan, and IKONOS pan.\nMultispectral systems generally refers to 3 to 15 bands, which are usually located within the visible range (VIS, 0.4-0.7 μm), near-infrared (NIR, 0.75–1.4 μm), short-wavelength infrared (SWIR, 1.4–3 μm), mid-wavelength infrared (MWIR, 3–8 μm) and long wavelength/ thermal infrared (LWIR/TIR, 8–15 μm). Examples: Landsat program, Sentinel 2/3, SPOT, Quickbird, and RapidEye.\n\n\n\n\nSentinel 2 image, airport Schönefeld, Berlin; left: true color composite (RGB 4,3,2), right: pseudo-color composite (RGB 8,4,3)\n\nHyperspectral systems (spectrometer) offer hundreds or thousands of bands with narrow bandwidths. A spectral resolution this high gives ability to distinguish minor differences of land cover characteristics, which in turn provides ability to address issues that could not be solved with multispectral data, e.g., mineral or building material classifications. There are some airborne spectrometers (AVIRIS, HySPEX, HyMAP) and to date only one operational satellite: Hyperion. Anyway, more spaceborne systems are already in the starting blocks, e.g., EnMAP and HyspIRI.\n\n\n\nA digital sensor generally recognizes objects as intensity values, whereby it can only distinguish between dark and bright. The radiometric resolution refers to the ability to tell apart objects based on differences in those intensities. Thus, a sensor with a high radiometric resolution records more intensity levels or grey-scale levels. This property is expressed by the number of bits. Most satellite products have a bit depth of 8 to 16 bits, which means that they support 28(=256) or 216(=65536) different gray scale levels, respectively. A 1 bit image would be subject to a huge loss of information in comparison!\nThose effects are easier to understand when looking at the image comparison below: An image with a bit depth of 1 contains only two gray scale levels (21=2), i.e., black and white. Using 2 bits double the number of available colors (22=4), which allows a few more details (as shown in the middle image). When using 8 bits, the image can draw from a whole color ramp ranging from black to white (dark to bright) comprising 256 grey scale values (28=256).\n\n\n\n\n\n\nBlue band of an orthophoto in 1 bit (l), 2 bit (m), and 8 bit (r) representation\n\nHuman perception is barely sufficient to detect gray-scale differences beyond 8 bits in digital images. Nevertheless, machine learning algorithms often benefit from a finer differentiation of contrasts.\n\n\n\nThe temporal resolution of a sensor is simply the distance of time (usually in days) between two image acquisitions of the same area. A high temporal resolution thus indicates a smaller time window between two images, which allows a better observation of temporally highly dynamic processes on the Earth’s surface, e.g., weather or active fire monitoring.\nMost satellite sensors have a temporal resolution of about 14 days. Anyway, by using a satellite constellation of multiple sensors identical in construction the time between two acquisitions can be shortened. For example Planet Labs operates five RapidEye satellites, which are synchronized so that they overlap in coverage.\nHowever, weather satellites are capable of acquiring images of the same area every 15 minutes. This can be explained by the different orbits of the satellites: geostationary and polar orbiting satellite systems.\n\n\n\n\n\n\nGeostationary orbiting satellite\n\nGeostationary sensors follow a circular geosynchronous orbit directly above the Earth’s equator. A geosynchronous system provides the same orbital period as the Earth’s rotation period (24 h), so it always looks at the same area on Earth, which it can observate at very high frequencies of several minutes. This rotation pattern is only possible at an altitude very close to 35.786 km, which generally results in a comparatively lower geometric resolution. Geostationary orbits are used by weather, communication and television satellites.\n\n\n\n\n\n\nPolar orbiting satellite\n\nPolar orbiting satellites pass above or nearly above the poles on each orbit, so the inclination to the Earth’s equator is very close to 90 degrees. They fly at an altitude of approximately 800-900 km. The lower a satellite flies, the faster it is. That’s why an orbit takes only ~90 minutes. While flying from the north to south pole in 45 minutes, sun-synchronous sensors look at the sunlit side of the Earth (descending images). Moving from south to nord pole results in nighttime imagery (ascending images). On an descending flight, all polar orbiting satellites cross the equator between 10:00 am and 10:15 am (local time) to provide maximum illumination and minimum water vapor to prevent haze and cloud build-up. Due to their inclination, they map the entire surface of the earth within several days (~14 d) as the earth continues to rotate beneath them. The temporal resolution of polar orbiting satellites usually describes the repetition rate at the Equator. The coverage gets better at higher latitudes due to the poleward convergence of the satellite orbits."
  },
  {
    "objectID": "RESEDA/contents/SensorBasics.html#data-products",
    "href": "RESEDA/contents/SensorBasics.html#data-products",
    "title": "Sensor Basics",
    "section": "Data Products",
    "text": "Data Products\nLandsat 8 acquires over 500 scenes each day. Most acquired scenes are downlinked to the Landsat Ground Network and made available for download within 24 hours after acquisition. Those scenes are usually uploaded to and stored in the USGS global archive. All Landsat standard data products are processed using the Landsat Product Generation System (LPGS) and come as compressed “.tgz”-files, which can be uncommpressed by using file archiver, such as 7-Zip or WinRAR. Once uncompressed, the image data is in GeoTIFF output format and projected to the Universal Transverse Mercator (UTM) map projection with the World Geodetic System 84 (WGS84) datum.\nThere are two main data products, which differ in the previous level of preprocessing: one without an atmospheric correction (Level-1) and one with (Level-2). Information on the processing level designations can be found in the metadata file (“.MTL.txt”) that is delivered with the Landsat 8 product.\n \n\nLevel-1 products\nStandard Landsat 8 data products consist of quantized and calibrated scaled Digital Numbers (DN) representing the multispectral image acquired by OLI and TIRS. They can be converted to Top Of Atmosphere (TOA) reflectance and radiance values by using radiometric rescaling coefficients as described in this USGS guide.\nLevel-1 Landsat scenes with the highest available data quality are declared as Tier 1 (L1TP) and are considered suitable for time-series analysis. Tier 1 includes data that have well-characterized radiometry and consistent georegistration and that is inter-calibrated across the different Landsat instruments.\n\nL1TP (Tier 1): This product offers radiometrically calibrated and orthorectified pixels by using auxiliary digital elevation models (DEM) and ground control points (GCP) to correct for relief displacement\nL1GT (Tier 2): worse, since GCP were not available\nL1GS (Tier 2): worst, since neither GCPs nor DEMs were available\n\nLevel-1 products contain the following 14 files once uncompressed:\n\nLevel-1 bands (1, 2, 3, 4, 5, 6, 7, 8, 9, 10 , and 11)\nQuality Assessment (QA) Band\nAngle Band Coefficients file\nMetadata text file (MTL.txt)\n\n \n\n\nLevel-2 products\nUSGS offers on-demand Surface Reflectance data products (Level-2). Surface Reflectance products provide an estimate of the surface spectral reflectance as it would be measured at ground level in the absence of atmospheric scattering or absorption. Surface reflectance values are scaled between 0 % and 100 %. This atmospheric correction is done via the Landsat Surface Reflectance Code (LaSRC), which utilizes the Landsat band 1, auxiliary MODIS data and radiative transfer models.\nKeep in mind: This product contain neither Level-1 TOA layers nor thermal bands!\nAlthough these data are also free, it may take several days for the processing to be completed by the servers of the Earth Resources Observation and Science (EROS) Center, before you are able to download them (read on: USGS Earth Explorer).\nLevel-2 products contain the following 13 files once uncompressed:\n\nLevel-2 bands (1, 2, 3, 4, 5, 6, and 7)\nRadiometric Saturation QA band (radsat_qa.tif)\nSurface Reflectance Aerosol QA band (sr_aerosol.tif)\nLevel-2 Pixel Quality Assessment band (pixel_qa.tif)\nSurface Reflectance metadata file (.xml)\nLevel-1 metadata file (MTL.txt)\nLevel-1 Angle Coefficient file (ANG.txt)\n\nIn general, we recommend you to use the Level 2 products, especially if you work multitemporally, since the radiometry of surface reflectances is more comparable between scenes.\nThe official product guide offers more information on the Level-1 and Level-2 Landsat products."
  },
  {
    "objectID": "RESEDA/contents/SensorBasics.html#data-products-1",
    "href": "RESEDA/contents/SensorBasics.html#data-products-1",
    "title": "Sensor Basics",
    "section": "Data Products",
    "text": "Data Products\nA single Sentinel 2 scene is huge. The swath width of Sentinel 2 is 290 km. So that the data can be handled at all, imagery is geometrically subdivided into rectangular tiles, or so-called granules. The Payload Data Ground Segment (PDGS) is responsible for the processing and archiving those granules. All products are innately projected to the Universal Transverse Mercator (UTM) coordinate system with the World Geodetic System 84 (WGS84) datum.\nAnd again, Sentinel 2 data goes through multiples processing levels:\n\nLevel-0: unprocessed instrument data at full resolution; telemetry analysis, preliminary cloud mask generation, and coarse coregistration\nLevel-1A: radiometric corrections and geometric viewing model refinement\nLevel-1B: resampling, conversion to reflectances and cloud/water/land mask generation\n\nMore information on pre-processing is given in the Sentinel 2 User Handbook.\nAnyway, Level-0, Level-1A and Level-1B products are PDGS-internal products not made available to users! When browsing the ESA SciHUB archive, you will be able to choose from Level-1C and Level-2A products:\nLevel-1C products are radiometric and geometric corrected top of atmosphere (TOA) data. This corrections include orthorectification and spatial registration on the UTM/WGS84 system with sub-pixel accuracy. Level-1C imagary is delivered in granules of 100×100 km of approximately 600-800 MB each. The individual spectral bands are present in their respective resolution (10, 20 and 60m), which is why resampling to a uniform geometric resolution is often necessary for further processing. A product consists of image data, available as JPEG2000 files, and the associated metadata, all capsuled within a “SAFE” file container. You will need the SNAP-software in order to read those SAFE-container (see chapter Visualize for a guide). The Sentinel-SAFE format wraps image data and product metadata in a specific folder structure. Do not alter this folder structure or any file names to ensure that image data and auxiliary information can be imported correctly.\nLevel-2A is the surface reflectance, or Bottom-Of-Atmosphere (BOA), product derived from a associated Level-1C product. This product is corrected by any distortion of atmosphere, terrain and cirrus clouds. Level-2A datasets come with some additional data layers: aerosol optical thickness-, water vapour, and scene classification maps and quality indicators, including cloud and snow probabilities. You can choose a resolution (10 m, 20 m, or 60 m) in which all channels are resampled uniformly.\nAnyway, if you already own a Level-1C scene, the conversion to Level-2A can also be done by yourself via the processor Sen2Cor. A guide on how to do this is given in chapter Classification in R.\nIn general, we recommend you to use the Level-2A products, especially if you work multitemporally, since the radiometry of surface reflectances is more comparable between scenes."
  },
  {
    "objectID": "RESEDA/contents/SensorBasics.html#data-products-2",
    "href": "RESEDA/contents/SensorBasics.html#data-products-2",
    "title": "Sensor Basics",
    "section": "Data Products",
    "text": "Data Products\nJust like Sentinel 2 data, Sentinel 1 ships its data in a zipped “SAFE” container format wrapping image data and product metadata in a specific folder structure. Do not alter this folder structure or any file names to ensure that image data and auxiliary information can be imported correctly. You do not need to unzip the file! The SNAP-software is able to read the file zipped.\nSentinel-1 data products which are generated by the Payload Data Ground Segment (PDGS) operationally are distributed at three levels of processing:\n\nLevel-0: compressed and unfocused SAR raw data, basis for all other high level products\nLevel-1: baseline product intended for most data users\nLevel-2: geolocated wind, wave and currents products derived from Level-1\n\nFor most applications, you should focus on Level-1 data. Additionally, Level-1 products can be one of two product types:\n\nSingle Look Complex (SLC) products are represented by a complex (I and Q) magnitude value and therefore contains both amplitude and phase information\nGround Range Detected (GRD) products consist of focused SAR data that has been detected, multi-looked and projected to ground range using an Earth ellipsoid model\n\nA more comprehensive explanation of data product is given in the Sentinel 1 user guide."
  },
  {
    "objectID": "RESEDA/index.html",
    "href": "RESEDA/index.html",
    "title": "RESEDA",
    "section": "",
    "text": "to your Remote Sensing Data Analysis online course, or RESEDA for short!\nThis course helps you improve your analysis of remote sensing image data for the next data science project or thesis. We will have a look at basic and advanced concepts needed for a complete project implementation using remote sensing imagery – with a special focus on automatisation of individual operations and big data processing. This course will provide a great deal of knowledge and valuable expertise for all related fields of environmental earth sciences!\nFor that purpose we will mainly use the statistical programming language R in a Linux environment – but please don’t panic, it’s not as bad as it sounds: You will be led through a complete analysis process, from data acquisition, to import, exploration and finally the export of your results, guided by a lot of reproduceable examples, exercises and pretty pictures!\nKeep in mind that this course content complements and extends the material covered in the classes Fernerkundung und Digitale Bildverarbeitung and Geographische Informationssysteme, both being taught at the Freie Universität Berlin.\nPlease feel free to contact me if you have any questions or would like to discuss potential projects."
  },
  {
    "objectID": "RESEDA/index.html#learning-objectives",
    "href": "RESEDA/index.html#learning-objectives",
    "title": "RESEDA",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThis online course is divided into separate sections covering particular topics, which together provide a whole workflow commonly used for remote sensing imagery. Although the sections are built on one another and we recommend to handle them in given order, feel free to skip parts you are already comfortable with and focus on the chapters you are interested in. In almost every section you will come across exercises, e.g., multiple choice questionnaires or coding exercises to proof your comprehension of previous sections.\nLet us have a look at the learning objectives and checkpoints of each individual section:\nPrepare Yourself\ninstall our VirtualBox containing all required software get used to the GUIs of R-Studio, QGIS and SNAP refresh basics of the programming language R (if needed)\nAcquire Data\n\nrepeat basics of optical and radar imagery\nbecome familiar with online data provider and HUBs\nautomatically download many images using bulk downloads\n\nAnalyse Your Data\n\nrepeat basics of classification and regression tasks\nclassify image data in R with Random Forest and SVM\nvisualize results in R and QGIS\n\nValidate Results\n\nrepeat validation basics\nvalidate results in R with state of the art methods\n\nSAR Processing\n\nget deeper insights in SAR processing\nprocess Sentinel imagery in SNAP\nlearn to do InSAR and texture analysis\n\nWe wish you a lot of creative ideas, much findings and great results!\nBest regards, your FU Berlin Remote Sensing and Geoinformatics staff"
  }
]